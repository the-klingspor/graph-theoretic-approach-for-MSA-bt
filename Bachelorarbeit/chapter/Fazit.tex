\chapter{Fazit}
\label{ch:fazit}

\section{Zusammenfassung}

Nachdem wir mit dem Algorithmus von Needleman-Wunsch einen ersten Einblick ins Thema bekommen haben, wurden im Laufe dieser Arbeit zwei algorithmische Ansätze für das \emph{multiple sequence Alignment}-Problem vorgestellt: DIALIGN 2.2 und der \emph{min-cut}-Ansatz von \cite{cpm10}.

DIALIGN 2.2 mit seinen Erweiterungen berechnet solide globale Alignments und ist bei denen von lokalen Sequenzfamilien überragend. Mit Hilfe von dynamischer Programmierung berechnet das speicherplatzeffiziente Verfahren aus DIALIGN 2.2 die quadratisch vielen paarweisen Alignments. Dabei wird jedem möglichen Fragment mit Hilfe der Gewichtsfunktion $w^{*}$ aus DIALIGN 2.0 ein Gewicht zugewiesen und das Gesamtgewicht aller miteinander konsistenten Fragmente maximiert. Mit Hilfe eines kleinen Tricks welche Fragmente wir dabei speichern mussten konnte der benötigte Speicherplatz weiter gesenkt werden. Um für unser multiples Alignment Fragmente zu bevorzugen, die Übereinstimmungen mit ähnlichen Abschnitten in weiteren Sequenzen haben, berechnen wir im nächsten Schritt \emph{Überlappgewichte}. Anstelle der naiven Berechnung, die alle Fragmente miteinander vergleicht, habe ich einen effizienteren Algorithmus entwickelt, der auf dem parallelen Traversieren der Listen von Fragmenten aller paarweisen Alignments basiert.

Nachdem die \emph{Überlappgewichte} berechnet wurden, nutzen wir diese für die Sortierung aller Fragmente. Eine gierige Heuristik wählt jetzt solange das Fragment mit dem höchsten Gewicht, das zu allen zuvor gewählten konsistent ist, bis keins mehr übrig ist. Für die rechenintensive Berechnung der Konsistenzgrenzen haben wir den mit DIALIGN 2.1 eingeführten Ansatz von Abdedda\"im betrachtet. Dieser berechnet mit Hilfe eines \emph{Spanning Set of disjoint Paths} (SSDP) die transitive Hülle des sogenannten Alignmentgraphen mit dessen Hilfe sich die Konsistenzgrenzen berechnen lassen.
Solange sich das Alignment durch Fragmente mit positiven Gewichten erweitern lässt, führen wir auf den Teilsequenzen zwischen den Konsistenzgrenzen weitere Durchläufe von DIALIGN durch. Schlussendlich fügen wir für unsere Ausgabe Lücken in die Menge von alignierten Sequenzen ein, sodass alle Zuweisungsspalten genau untereinander stehen.
	
Der im Kern auf DIALIGN basierende Ansatz von \cite{cpm10} erweitert dieses um graphentheoretische Ansätze. Der Hauptunterschied zum ursprünglichen Verfahren ist, dass man eine Menge von multiplen Zuweisungen auf eine konsistente Menge zu verringern, statt nach und nach mit paarweisen Zuweisungen ein Alignment aufzubauen. Zunächst werden dafür die Verbindungen aller paarweisen Alignments in einem Inzidenzgraphen gesammelt. In diesem fassen wir Zusammenhangskomponenten als Flussnetzwerke auf und trennen mit Hilfe eines Algorithmus zur Berechnung von minimalen Schnitten Knoten, die direkt oder indirekt miteinander verbunden sind und gleichzeitig aus der selben Sequenz kommen. Wurden alle dieser Mehrdeutigkeiten aufgelöst, ergibt sich als Zwischenstand eine Menge von partiellen Zuweisungsspalten. In diesen gibt es keine transitiven Mehrfachzuweisungen mehr, aber Überkreuzungen können weiterhin vorkommen. 

Zur Entfernung von Stellen aus Zuweisungen, die für Inkonsistenzen sorgen, haben wir den Algorithmus von Pitschi kennengelernt. In einem Sukzessionsgraphen fügen wir die partiellen Zuweisungsspalten als Knoten ein, die miteinander verbunden sind, wenn es Sequenzen gibt, in denen Stellen der Knoten Nachfolger voneinander sind. Für jede Sequenz $S_i$ konstruieren wir basierend darauf einen neuen Graphen in dem Ketten von Knoten auf einem Pfad vom Start- zum Endknoten einer Menge von konsistenten Zuweisungen bezüglich $S_i$ entsprechen. Weil wir diese Menge maximieren möchten, wählen wir für jede Sequenz den Pfad maximaler Länge durch den entsprechenden Graphen.

Die resultierende Menge von konsistenten Zuweisungsspalten können wir wie zuvor bei DIALIGN in unseren Alignmentgraphen einfügen und danach auf den Teilsequenzen weitere Fragmente berechnen. Auch die Vorbereitung der Ausgabe erfolgt analog, nachdem sich das Alignment nicht mehr vergrößern lässt.

Aufgrund der begrenzten Zeit der Arbeit und dem großen Umfang konnte nicht das ganze Verfahren implementiert werden. Stattdessen habe ich exemplarisch einen Algorithmus zur Bestimmung des längsten Pfades in der Programmiersprache C++ und mit Hilfe der Bibliothek \enquote{Boost Graph Library} umgesetzt. Dieser Algorithmus stellt einen wichtigen Schritt im Algorithmus von Pitschi dar, den wir zuvor auf einer höheren Abstraktionsebene betrachtet haben. 

\section{Future Works} \label{sec:fut_work}

Im Folgenden möchte ich kurz auf mögliche Weiterentwicklungen des vorgestellten Verfahrens eingehen. Teilweise wurden diese bereits im Lauf der Bachelorarbeit vorgestellt, es fehlt aber noch eine Evaluierung der Ergebnisse mit Hilfe einer vollständigen Implementierung des Algorithmus. Andere wurden hingegen noch nicht erwähnt.

\cite{cpm10} haben festgestellt, dass die numerischen Scores ihres Verfahrens in vielen Fällen schlechter waren als die von DIALIGN 2.2, obwohl die Ergebnisse biologisch relevanter waren. Sie schlussfolgern daher, dass es auf Basis der aktuellen Gütefunktion keine substantielle Verbesserung des DIALIGN-Ansatzes mit Hilfe neuartiger Heuristiken mehr geben wird. Eine erste mögliche Verbesserung des Verfahrens könnten stochastische Ansätze für die Gütefunktion sein, wie beispielsweise \emph{Conditional Random Fields} oder \emph{Hidden Markov Models}. Außerdem könnte man statt paarweisen Alignments auch kleinere multiple Zuweisungen als Grundbausteine für die Heuristiken verwenden. Ein Ansatz sind zum Beispiel ternäre Alignments bei denen sich Fragmente gleich über alle drei der betrachteten Sequenzen ziehen können. Leider sorgten sowohl die größere Anzahl dieser Alignments ($\oh(n^3)$ statt $\oh(n^2)$), als auch die höhere Laufzeit pro Tripel von Sequenzen (vermutlich etwa $\oh(L^3)$ statt $\oh(L^2)$) für eine insgesamt erheblich höhere Komplexität. Erst aufwändige Implementierungen eines darauf basierenden Verfahrens dürften zeigen, ob die zu erwartenden besseren Ergebnisse den Aufwand rechtfertigen.

\cite{snkm04} haben mit DIALIGN P eine parallelisierte Implementierung von DIALIGN entwickelt. Aber auch beim \emph{min-cut}-Ansatz lassen sich viele Schritte des Algorithmus leicht parallelisieren:

\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
	\item Wie bei DIALIGN P kann man die die voneinander unabhängigen paarweisen Alignments gleichzeitig berechnen.
	\item Die \emph{Überlappgewichte} lassen sich parallel bestimmen, wenn man für die Fragmente jedes paarweisen Alignments nur lesend auf die der anderen Alignments zugreift, bei denen eine der zwei selben Sequenzen beteiligt war.
	\item Alle chnitte sind voneinander unabhängig. Sie können also in jedem Durchlauf der while-Schleife für jede Zusammenhangskomponente im Inzidenzgraphen gleichzeitig berechnet werden.
	\item Für alle Sequenzen $S_i \in S$ können beim Algorithmus von Pitschi simultan die Graphen $G_{S_i}$ konstruiert und die längsten Pfade bestimmt werden.
\end{enumerate}

\noindent Auf diese Art und Weise ließe sich die mitunter sehr lange Laufzeit des graphtheoretischen Ansatzes vermutlich beträchtlich verringern.

Beim Konstruieren des Alignmentgraphen könnte man anstatt zwei antiparallele Kanten zwischen den neu alignierten Stellen einzufügen auch eine Verschmelzung von ihnen in einem Knoten durchführen. Das entspricht dann zwar nicht dem der transitiven Hülle auf einem Graph mit SSDP, weil die Pfade entlang jeder Sequenz durch den Graphen nicht mehr disjunkt sind. In der Praxis wäre das aber kein Problem, weil die Doppelkanten ohnehin für Verbindungen in beide Richtungen sorgen und die Knoten insofern äquivalent im Sinne der Funktion \textrm{EdgeAddition} sind. Das ist auch wenig überraschend, weil ein Alignment eine Äquivalenzrelation ist, in der alignierte Stellen Teil der selben Äquivalenzklasse sind. Man müsste in der Praxis testen wie sich der verringerte Speicherverbrauch durch die kleinere Anzahl an Kanten im Vergleich zu den zusätzlichen Kosten für die Vereinigung der ausgehenden Kanten auswirkt.

Wie im entsprechenden Abschnitt schon angedeutet könnte man die Felder nextClass[] und prevClass[] auch durch effizientere Datenstrukturen ersetzen. Es ließe sich viel Speicherplatz sparen, wenn man nicht für alle Waisen Vorgänger und Nachfolger speichert, sondern stattdessen für die Äquivalenzklassen an Waisen zwischen zwei alignierten Stellen. Denkbar wäre zum Beispiel eine Waldstruktur mit einem Suchbaum für jede Sequenz, der als Knoten Angaben über Start- und Endpunkte solcher Klassen enthält.

Im Abschnitt über den Algorithmus von Pitschi haben wir gezeigt, dass es Situationen geben kann, in denen die einfache Heuristik zum Löschen von Kanten im Graphen nicht das erwünschte Ergebnis liefert. Bei dem Versuch die Zyklen im Graphen aufzulösen kann es dazu kommen, dass auch eine Vielzahl an unbeteiligten Kanten gelöscht werden, im schlimmsten Fall sogar alle. Ein Beispiel für eine solche Situation sind Sequenzfamilien, wo es im Lauf der Evolution zu einer Permutation eines größeren Abschnitts gekommen ist. Um solchen Fällen vorzubeugen kann man beispielsweise überprüfen, ob die ursprüngliche Heuristik mehr als sagen wir 25\% aller Kanten löscht und falls das der Fall ist, auf einen anderen Algorithmus wie den von \cite{els93} zurückgreifen. Das wäre zwar zusätzlicher Aufwand, aber besser als die teuer berechneten partiellen Zuweisungsspalten nicht zu benutzen.

In Abschnitt \ref{sec:ueberlapp} habe ich einen effizienten Algorithmus zum Berechnen der \emph{Überlappgewichte} vorgestellt. Statt alle $\oh(n^2\cdot L)$ Fragmente miteinander auf Überschneidungen zu überprüfen, vergleiche ich nur Fragmente an denen genau eine gemeinsame Sequenz beteiligt ist. Weil die Listen von Fragmenten nach dem Berechnen der paarweisen Alignments sortiert sind, lässt sich der Aufwand dafür weiter verringern, indem ich parallel über sortierte Listen traversiere.

Anstatt einheitliche Kapazitäten in den Flussnetzwerken über den Inzidenzgraphen zu benutzen, verwende ich Kapazitäten, die von der Ähnlichkeit der verbundenen Stellen abhängen. Das Kalkül dahinter ist, so dass im Zweifelsfall eher unähnliche Symbole voneinander getrennt werden, anstelle von ähnlichen. Ob dieses Verfahren eine merkliche Verbesserung der Ergebnisse bewirkt muss man in der Praxis testen. Es ist aber auf jeden Fall eine vielversprechende Idee.













