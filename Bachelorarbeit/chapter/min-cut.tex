\chapter{Ein Min-Cut-Ansatz für das Konsistenzproblem}
\label{ch:min-cut}
Da es, wie im letzten Kapitel gezeigt, auf manchen Sequenzfamilien durch die gierige Heuristik von DIALIGN zu suboptimalen \emph{Scores} und \emph{Alignments} kommt, werden wir jetzt einen verbesserten graphtheoretischen Ansatz von \cite{cpm10} betrachten.

Dazu benötigen wir zwei verschiedene Graphen: zum einen den \emph{Inzidenzgraphen}, bei dem alle \emph{Stellen} Knoten sind und ihre \emph{Anker} Zusammenhangskomponenten. Der zweite ist der \emph{Sukzessionsgraph}, der die Zusammenhangskomponenten unseres \emph{Inzidenzgraphen} als Knoten und die natürliche Ordnung auf den Sequenzen als Kanten benutzt. Man kann sich vorstellen, dass genau dann eine Kante zwischen zwei  Diese beiden Datenstrukturen werden wir benutzen, um \emph{Inkonsistenzen} aufzulösen. Wenn wir uns an die Definition von \emph{Konsistenz} aus dem letzten Kapitel erinnern, dann stellen wir fest, dass es zwei Arten von ihr gibt: zum einen implizite, transitive Mehrfachzuweisungen bei denen einer \emph{Stelle} einer Sequenze mehrere \emph{Stellen} einer anderen Sequenz zugeordnet sind und zum anderen überkreuzte Zuweisungen.

Als Ausgangspunkt starten wir wieder mit unseren paarweisen \emph{Alignments} aus DIALIGN. \emph{Überlappgewichte} brauchen wir in unserem Fall nicht. Dann konstruieren wir mit Hilfe dieser Zuweisungen unseren \emph{Inzidenzgraphen} und benutzen einen Algorithmus zur Berechnung des minimalen Schnitts (\enquote{min-cut}) auf den Zusammenhangskomponenten, um alle \emph{Inkonsistenzen} aufgrund von transitiven Mehrfachzuweisungen aufzulösen. Die so entstehenden Zusammenhangskomponenten benutzen wir, um einen \emph{Sukzessionsgraphen} aufzubauen. Dank eines Algorithmus von \cite{pdc10} können wir mit diesem Überkreuzungen aus unserer Relation löschen. Alle dieser Konzepte werden wir im Laufe dieses Kapitel formal definieren, genauer analysieren und die Korrektheit der Aussagen beweisen.
 
\section{Flussnetzwerke}

\subsection{Einführung}

Stell dir eine Produktionsstätte vor, wo ein bestimmtes Produkt hergestellt wird und eine Verwendungsstätte bei der das Produkt benötigt wird. Zwischen dem Ort der Produktion und dem Ort der Verwendung gibt es ein Schienennetz über das die Produkte geliefert werden können. Über jeden Abschnitt der Schienen kann aber nur eine bestimmte Anzahl an Waggons geleitet werden. So mag es weniger stark befestigte Strecken geben, wo nur kleinere oder leichtere Züge fahren können und andere gut ausgebaute mit mehreren Gleisen nebeneinander. Die Anzahl an Einheiten, die über einen Abschnitt geleitet werden können, nennt man Kapazität, während Produktionsstätte und Zielort Quelle und Senke heißen. Ziel ist es zu bestimmen wie viele Produktionseinheiten über dieses Netz von der Fabrik zum Verbraucher geliefert werden können.

Neben diesem Problem kann man mit Flussnetzwerken noch viele andere Anwendungen modellieren, zum Beispiel eine chemische Produktionsstraße mit vielen Rohren, die jeweils nur den Durchfluss einer bestimmten Menge erlauben. Oder Stromnetze mit Kraftwerken und Endverbrauchern zwischen denen es Stromleitungen gibt, die jeweils höchstens einen bestimmten Stromfluss erlauben. Für eine Hochspannungsleitung mag dieser sehr hoch und für die Leitungen, die direkt zu den Häusern gehen, sehr klein sein. Stromnetze bieten uns auch eine direkte Analogie, wie mit den Knotenpunkten zwischen den Verbindungen umzugehen ist. An jedem Knoten, außer der Quelle und Senke, wird genauso viel Fluss hinein- wie wieder hinausgeleitet. Es wird nichts gespeichert oder geht verloren. Dieses Konzept, das man \emph{Flusserhaltung} nennt, funktioniert genau wie das erste \emph{Kirchhoffsche Gesetz} bei Strömen \citep{clrs09}. 

Ursprünglich stammt das Konzept des Flussnetzwerkes aus dem Kalten Krieg und der militärischen Forschung. Es wurde das Schienennetzwerk des Sowjetunion in Osteuropa untersucht, um herauszufinden wie viel Material die UdSSR im Kriegsfall aus dem russischen Kerngebiet nach Mitteleuropa transportieren könnte und welche Strecken man zerstören müsste, um den Nachschub am schnellsten Abzuschneiden. Die Untersuchung bietet uns sogleich einen intuitiven Begriff des \emph{maximalen Flusses} und des \emph{minimalen Schnitts}. Der \emph{maximale Fluss} ist die maximale Anzahl an Einheiten, die von der Quelle zur Senke transportiert werden kann, während der \emph{minimale Schnitt} den \enquote{Bottleneck} des Netzwerkes darstellt, der den Fluss von der Quelle zur Senke minimiert. Wie wir später formal zeigen werden sind diese beiden Werte in einem Flussnetzwerk äquivalent.

Zunächst betrachten wir Flussnetzwerke auf einer formalen Ebene, dann widmen wir uns kurz und skizziert einigen der wichtigsten Algorithmen zum berechnen von \emph{maximalen Flüssen} und danach beweisen wir die Äquivalenz von diesen mit \emph{minimalen Schnitten}. Diese \emph{minimalen Schnitte} brauchen wir später zum Auflösen von \emph{Inkonsistenzen} im \emph{Inzidenzgraphen}.

\begin{definition}[Flussnetzwerk]
	Ein \emph{Flussnetzwerk} ist ein gerichteter Graph $G = (V,E)$ bei dem jeder Kante $(u,v) \in E$ eine nicht-negative Kapazität $c(u,v) \geq 0$ zugeordnet ist und bei dem es zwei ausgezeichnete Knoten $s, t \in V$ gibt, die wir \emph{Quelle} und \emph{Senke} nennen.  
\end{definition}

Der Einfachheit halber nehmen wir im Folgenden an, dass jeder Knoten von $G$ auf einem Pfad von $s$ nach $t$ liegt. Sollte es doch solche Knoten geben, dann wären sie ohnehin nicht relevant, weil kein \emph{Fluss} durch sie von der \emph{Quelle} zur \emph{Senke} geschickt werden kann.

\begin{definition}[Fluss]
	Sei $G = (V,E)$ mit Kapazitätsfunktion $c$ ein \emph{Flussnetzwerk}. Dann ist definieren wir einen \emph{Fluss} als eine Funktion $f: V \times V \rightarrow \mathbb{R}$, die die folgenden beiden Eigenschaften erfüllt:
	\begin{itemize}[leftmargin=12em]
		\item[\textbf{Kapazitätsbeschränkung:}] Für alle Knoten $u,v \in V$ gelte $0 \leq f(u,v) \leq c(u,v)$.
		\item[\textbf{Flusserhaltung:}] Für alle Knoten $u \in V\setminus\{s,t\}$ gelte \\ 
			$\sum_{v \in V}{f(v,u)} = \sum_{v \in V}{f(u,v)}$.
	\end{itemize} 
	Für Knoten $u,v\in V$, die nicht durch eine Kante verbunden sind ($(u,v)\notin E$), setzen wir den Fluss auf 0: $f(u,v) = 0$.
\end{definition}

Ein \emph{Fluss} ist also eine Zuordnung von Werten an die Kanten unseres \emph{Flussnetzwerkes}, sodass keine Kapazität verletzt wird und in jeden Knoten soviel hinein wie hinaus fließt. Wir nennen auch den Wert $f(u,v)$ \emph{Fluss} zwischen den beiden Knoten $u$ und $v$.


Hier ein Beispiel für ein einfaches \emph{Flussnetzwerk} mit sechs Knoten:
\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill=gray,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=20pt,
		font=\footnotesize},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 1.2cm
	]
	\node[mycircle] (c1) {$s$};
	\node[mycircle,below right=of c1] (c2) {$v_2$};
	\node[mycircle,right=of c2] (c3) {$v_4$};
	\node[mycircle,above right=of c1] (c4) {$v_1$};
	\node[mycircle,right=of c4] (c5) {$v_3$};
	\node[mycircle,below right=of c5] (c6) {$t$};
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c2/8/below,
		c1/c4/11/above,
		c2/c3/11/below,
		c3/c6/4/below,
		c4/c5/12/above,
		c5/c6/15/above,
		c5/c2/4/below,
		c3/c5/7/below,
		c2.70/c4.290/1/below}
	\draw [myarrow] (\i) -- node[sloped,font=\footnotesize,\p] {\txt} (\j);
	
	
	% draw this outside loop to get proper orientation of 10
	\draw [myarrow] (c4.250) -- node[sloped,font=\small,above,rotate=180] {10} (c2.110);
	\end{tikzpicture}
\end{center}

Viele Autoren setzen voraus, dass es keine Kanten in die \emph{Senke} und keine aus der \emph{Quelle} gibt. Diese Beschränkung ist zwar hilfreich fürs Verständnis, wird aber grundsätzlich nicht benötigt. Man kann sich leicht überlegen, dass selbst wenn es solche Kanten gibt, ihr \emph{Fluss} 0 sein muss. Bei den \emph{Flussnetzwerken}, die wir für unseren Algorithmus benutzen, wird es diese Kanten geben, weshalb wir auf die Beschränkung verzichten. 

\begin{definition}[Wert eines Flusses]
	Der \emph{Wert} eines \emph{Flusses} ist definiert als
	\begin{equation}
		|f| = \sum_{v \in V}{f(s,v)} - \sum_{v \in V}{f(v,s)}.
	\end{equation} 
\end{definition}

Eine andere Einschränkung, die man in vielen Definitionen sieht, ist der Verzicht auf antiparallele Kanten. Das heißt, dass es gleichzeitig Kanten $(u,v), (v,u) \in E$ gibt. Manche Implementierungen für Algorithmen, die den maximalen Fluss berechnen, sind so programmiert, dass sie vom Benutzer eine Menge von Gegenkanten mit \emph{Fluss} 0 erwarten, die intern benötigt werden. Grundsätzlich sind antiparallele Kanten aber unproblematisch, weil man jeden \emph{Fluss} mit positiven Werten auf antiparallelen Kanten in einen \emph{Fluss} umwandeln kann, bei dem zwischen zwei Knoten höchstens eine Kante einen positiven \emph{Fluss} hat. Sei beispielsweise $f(u,v) = x, f(v,u) = y$ mit $x \not 0 \not y$. Dann kann man einen \emph{Fluss} mit dem selben \emph{Wert} definieren, wenn man den \emph{Fluss} auf den antiparallelen Kanten wie folgt definiert: $f(u,v) = x - \min\{x,y\}, f(v,u) = y - \min\{x,y\}$. Alternativ lässt sich jedes \emph{Flussnetzwerk} mit antiparallelen Kanten auch in ein äquivalentes übertragen, das keine solchen Kanten enthält.

\begin{center}
\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill=gray,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=20pt,
		font=\footnotesize},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 1.2cm
	]
	\node[mycircle] at (0,0) (c1) {$u$};
	\node[mycircle] at (0,-2) (c2) {$v$};
	
	\node[mycircle] at (5,0) (c3) {$u$};
	\node[mycircle] at (3,-1) (c4) {$c$};
	\node[mycircle] at (7,-1) (c5) {$c'$};
	\node[mycircle] at (5,-2) (c6) {$v$};

	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1.255/c2.105/2/below,
		c2.75/c1.285/5/below,
		c3/c4/2/below,
		c4/c6/2/below,
		c6/c5/5/below,
		c5/c3/5/below}
	\draw [myarrow] (\i) -- node[sloped,font=\footnotesize,\p] {\txt} (\j);
\end{tikzpicture}
\end{center}

\subsection{Wichtige Algorithmen}

Es gibt eine Vielzahl von Algorithmen zur Berechnung eines maximalen Flusses auf einem Flussnetzwerk. Die wichtigsten und bekanntesten Vertreter möchte ich kurz vorstellen. 

Der erste Algorithmus, der speziell zum berechnen des \emph{maximalen Flusses} entwickelt wurde war der Algorithmus von Ford-Fulkerson \citep{gt14}. Dieser verwendete einen sogenannten \emph{Restwertgraphen}, der für jede Kante im ursprünglichen \emph{Flussnetzwerk} eine Vorwärts- und eine Rückwärtskante enthält. Die Vorwärtskante hat als Kapazität die der Kante im ursprünglichen Graph minus den \emph{Fluss} im \emph{Flussnetzwerk}. Die der Rückwärtskante im \emph{Restwertgraphen} ist genau die Kapazität des \emph{Flusses} der zugehörigen Kante im \emph{Flussnetzwerk}. Für einen Pfad von der \emph{Quelle} zur \emph{Senke} im \emph{Restwertgraph} kann man den \emph{Fluss} im \emph{Flussnetzwerk} um die kleinste Kapazität auf diesem Pfad erhöhen, ohne die \emph{Kapazitätsbeschränkung} zu verletzen. Diese Pfade nennt man \emph{augmentierend} und nach jeder Aktualisierung des Flusses, müssen auch die Kapazitäten im \emph{Restwertgraphen} angepasst werden. Der Algorithmus von Ford-Fulkerson besucht solange \emph{augmentierende} Pfade, bis es keine solchen mehr gibt. Ist das der Fall, dann kann der \emph{Fluss} nicht mehr vergrößert werden und der \emph{maximale Fluss} wurde berechnet. Der Algorithmus von Ford-Fulkerson hat für einen zusammenhängenden Graphen mit ganzzahligen Kapazitäten eine pseudopolynomielle Laufzeit von $\oh(|E|\cdot U)$, wobei $U$ die Summe der Kapazitäten der ausgehenden Kanten von der \emph{Quelle} ist.

Oft wird Ford-Fulkerson eher als eine \enquote{Methode} statt als \emph{Algorithmus} bezeichnet, weil die Reihenfolge nach der \emph{augmentierende} Pfade gewählt werden nicht spezifiziert ist. In der Folge wurde der Algorithmus von Ford-Fulkerson an mehreren Stellen verbessert. Edmond-Karps benutzt eine Breitensuche, um den Pfad mit den wenigsten Knoten von der \emph{Quelle} zur \emph{Senke} zu finden. Dadurch lässt sich die Laufzeit auf $\oh(|V|\cdot |E|^2)$ verbessern \citep{clrs09}. Auch der Algorithmus von Dinic \footnote{Eigentlich entwickelt vom sowjetischen Forscher Yefim A. Dinitz. Die verbreitete Schreibweise beruht auf einem Übersetzungsfehler. Später wanderte Dinitz nach Israel aus \citep{d06}.} sucht nach kürzesten Pfaden im \emph{Restwertgraphen}. Zusätzlich wird das Konzept von sogenannten \emph{blockierenden Flüssen} benutzt, bei denen jeder $s-t$-Pfad mindestens eine Kante enthält, deren Kapazität komplett ausgereizt ist. Der Algorithmus von Dinic benötigt nur $\oh(|V|^2\cdot |E|)$ Rechenschritte \citep{d06}.

Die nächste Klasse von Algorithmen waren die sogenannten \emph{Push-Relabel-Algorithmen}. Diese benutzen bei ihren Schritten statt \emph{Flüssen} nur sogenannten \emph{Präflüsse} bei denen zwar die \emph{Kapazitätsbeschränkung} gilt, die \emph{Flusserhaltung} aber nicht. Das heißt, dass jeder Knoten einen positiven \emph{Überfluss} haben kann, wenn mehr in ihn hinein als hinaus fließt. Jeder Knoten hat einen Index, der seine Höhe im Netz angibt, wobei \emph{Fluss} immer nur von oben nach unten geleitet werden kann. Wie der Name schon sagt, sind die zwei Grundschritte bei allen \emph{Push-Relabel-Algorithmen} die Methoden \emph{Push} und \emph{Relabel}. Bei \emph{Push} verschieben wir den \emph{Überfluss} eines Knotens zu einem tiefer gelegenen Nachbarn. Bei \emph{Relabel} wird hingegen die Höhe eines Knotens vergrößert, wenn er noch \emph{Überfluss} hat, aber alle benachbarten Knoten einen größeren Index haben. Die \emph{Push-} und \emph{Relabelmethoden} werden solange angewendet, bis es keine Knoten mehr gibt auf die sie anwendbar sind \citep{clrs09}. Je nachdem wie der nächste zu bearbeitende Knoten ausgewählt wird, kann die Laufzeit stark variieren. Ein generischer Algorithmus, der die Knoten mehr oder minder zufällig auswählt, läuft in $\oh(|V|^2\cdot |E|)$. Mit einer FIFO-Warteschlange in dem jeder bearbeitete Knoten, der noch \emph{Überfluss} hat, wieder ans Ende eingereiht wird, lässt sich die Laufzeit auf $\oh(|V|^3)$ verbessern \citep{gt88}. Der in der Praxis verbreitetste Ansatz benutzt die \enquote{Highest-Label}-Regel, bei der alle Knoten in Töpfen nach ihrem Index eingeordnet sind und jeweils ein Knoten mit maximalem Index ausgewählt wird. Hier verbessert sich die Laufzeit auf $\oh(|V|^2\cdot \sqrt{|E|})$ Rechenschritte im schlimmsten Fall \citep{akmo97}.

Weiter verbessern ließ sich die asymptotische Laufzeit durch die Verwendung von \emph{dynamischen Bäumen}. \emph{Dynamische Bäume} sind eine Datenstruktur, die die nicht-saturierten Teile von \emph{augmentierenden} Pfaden speichern. Mit Operationen auf dieser Datenstruktur ist es möglich die Laufzeit zu senken. Beschränkt man beispielsweise die maximale Baumgröße und benutzt eine weitere Datenstruktur, dann lässt sich der maximale Fluss mit Hilfe von \emph{blockierenden Flüssen} in $\oh(|V|\cdot |E|\cdot \log(|V|^2/|E|))$ berechnen \citep{gt14}. In der Praxis sind diese Algorithmen aber nicht von Relevanz, weil die verwendete Baumstruktur einen großen konstanten Vorfaktor benötigt. Einer der schnellsten bekannten Algorithmen ist der von Orlin. Er verwendet eine Kombination verschiedener Techniken in unterschiedlichen Situationen und kommt insgesamt auf eine Laufzeit von $\oh(|V|\cdot |E|)$ \citep{gt14}.
 
\subsection{Schnitte und der \emph{Max-Flow-Min-Cut-Satz}}

\begin{definition}[Schnitt]
	Sei $G = (V,E)$ mit \emph{Senke} und \emph{Quelle} $s, t \in V$ und einer Kapazitätsfunktion $c$. Dann ist ein \emph{Schnitt} von $G$ eine Partitionierung von $V$ in zwei Mengen $A$ und $B$, sodass $s \in A$ und $t \in B$ gelten.
	
	Die Kapazität $c_{A,B}$ dieses \emph{Schnitts} ist definiert als die Summe der Kapazitäten aller Kanten von $A$ nach $B$:
	\begin{equation}
		c_{A,B} \coloneqq \sum_{(v,w) \in E \cap (A \times B)}{c(v,w)}
	\end{equation}
\end{definition}

Betrachten wir ein Beispiel für einen \emph{Schnitt} auf einem \emph{Flussnetzwerk}. Alle Knoten in $A$ wurden grün und alle in $B$ rot markiert. Kanten zwischen den beiden Mengen wurden gepunktet dargestellt.

\vspace{-8pt}
\begin{center}
	\begin{tikzpicture}[
	mygreennode/.style={
		circle,
		draw=black,
		fill=green,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=20pt,
		font=\footnotesize},
	myrednode/.style={
		circle,
		draw=black,
		fill=red,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=20pt,
		font=\footnotesize},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 1.2cm
	]
	\node[mygreennode] (c1) {$s$};
	\node[myrednode,below right=of c1] (c2) {$v_2$};
	\node[mygreennode,right=of c2] (c3) {$v_4$};
	\node[mygreennode,above right=of c1] (c4) {$v_1$};
	\node[mygreennode,right=of c4] (c5) {$v_3$};
	\node[myrednode,below right=of c5] (c6) {$t$};
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c4/11/above,
		c4/c5/12/above,
		c5/c6/15/above,
		c3/c5/7/below}
	\draw [myarrow] (\i) -- node[sloped,font=\footnotesize,\p] {\txt} (\j);
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c2/8/below,
		c2/c3/11/below,
		c3/c6/4/below,
		c5/c2/4/below,
		c2.70/c4.290/1/below}
	\draw [myarrow, dotted] (\i) -- node[sloped,font=\footnotesize,\p] {\txt} (\j);
	
	% draw this outside loop to get proper orientation of 10
	\draw [myarrow, dotted] (c4.250) -- node[sloped,font=\small,above,rotate=180] {10} (c2.110);
	\end{tikzpicture}
\end{center}
\vspace{-8pt}

Es gilt $A = \{s, v_1, v_3, v_4\}$ und $B = \{v_2, t\}$. Zur Kapazität des \emph{Schnitts} tragen alle Kanten bei, die von Knoten aus $A$ zu solchen aus $B$ verlaufen. Die Kante $(v_3,v_2)$ also schon, die Kante $(v_2, v_4)$ aber nicht. Insgesamt beträgt die Kapazität des \emph{Schnitts} $c(s,v_2) + c(v_1, v_2) + c(v_3,v_2) + c(v_3,t) + c(v_4,t) = 8 + 10 + 4 + 15 + 4 = 41$.  

\begin{definition}[Minimaler Schnitt]
	Als \emph{minimalen Schnitt} bezeichnet man den oder einen Schnitt mit minimaler Kapazität. Es kann mehrere solcher \emph{minimaler Schnitte} geben. 
\end{definition}

Dem geneigten Leser mag die Asymmetrie zwischen \emph{Flüssen}, die maximal sein können und \emph{Schnitten}, die minimal sein können, aufgefallen sein. Den Zusammenhang der beiden Konzepte möchte ich im Folgenden kurz beschreiben. Dazu werden wir zwei formale Aussagen betrachten, die hier aber nicht bewiesen werden. Diese sind zwar für unseren Algorithmus von Bedeutung, aber für einen formalen Beweis bräuchten wir einen größeren theoretischen Hintergrund, der obgleich furchtbar spannend, hier den Rahmen sprengte und wenig zielführend wäre.

\begin{lemma}
	Der \emph{Wert} eines beliebigen \emph{Flusses} über einem \emph{Flussnetzwerk} ist beschränkt durch einen beliebigen Schnitt.
\end{lemma}

\begin{beweis}
	Siehe \cite{clrs09}.
\end{beweis}

\begin{satz}[Max-Flow-Min-Cut-Satz]
	Für ein \emph{Flussnetzwerk} $G = (V,E)$ mit \emph{Senke} und \emph{Quelle} $s, t \in V$ entspricht die minimale Kapazität aller \emph{Schnitte} auf $G$ dem maximalen \emph{Wert} aller Flüsse von $s$ nach $t$. 
\end{satz} 

\begin{beweis}
	Wenn wir an den Algorithmus von Ford-Fulkerson denken, dann erinnern wir uns, dass dieser solange \emph{augmentierende} Pfade mit positiver \emph{Restwertkapazität} von der \emph{Quelle} zur \emph{Senke} gesucht hat, bis es keinen solchen mehr gab. Sobald das der Fall war, hatte man den \emph{maximalen Fluss} berechnet. Das bedeutet gleichzeitig, dass man den \emph{Bottleneck} zwischen $s$ und $t$ gefunden hat. Genau dieser trennt die Knoten von $G$ in zwei Hälften und ist unser \emph{minimaler Schnitt}. Für einen formalen Beweis neben dieser Skizze siehe auch hier \cite{clrs09}.
\end{beweis} 

Die Erkenntnis, die man aus diesem Satz zieht, ist, dass man Algorithmen zur Berechnung des \emph{maximalen Flusses} benutzen kann, um den \emph{minimalen Schnitt} eines \emph{Flussnetzwerkes} zu bestimmen. Das werden wir im nächsten Abschnitt ausnutzen, um \emph{Inkonsistenzen} zwischen unseren Sequenzen aufzulösen.

\section{Inzidenzgraphen und das Auflösen von Inkonsistenzen mit Hilfe von Flussnetzwerken}

\subsection{Konstruieren des Inzidenzgraphen}

Wir starten wie bei DIALIGN	zunächst mit paarweisen \emph{Alignments}. Unser Ziel ist es Übereinstimmungen zu finden, die in möglichst vielen Sequenzen gleichzeitig vorkommen. Das ist ein vielversprechender Ansatz, weil die gesuchten Motive oft in vielen sich überlappenden \emph{Fragmente} vorkommen, während Überlappungen bei zufälligen Übereinstimmungen unwahrscheinlich sind. Dazu konstruieren wir einen sogenannten \emph{Inzidenzgraphen}, der alle \emph{Stellen} als Knoten enthält, die durch Kanten verbunden sind, falls es ein sie verbindendes \emph{Fragment} gibt. Mit Hilfe eines Algorithmus zur Berechnung des \emph{maximalen Flusses} bestimmen wir \emph{minimale Schnitte}, bis nur noch dichte Zusammenhangskomponenten übrig bleiben, die jeweils höchstens einen Knoten aus jeder Sequenz enthalten \citep{cpm10}.

\begin{definition}[Mehrdeutigkeit und partielle Zuweisungsspalten]
	Es sei eine Menge an Sequenzen $S$ mit \emph{Stellenraum} $\mathcal{S}$ gegeben. Eine Teilmenge $C \subset \mathcal{S}$ nennen wir \emph{mehrdeutig}, wenn es mindestens eine Sequenz $S_i$ gibt, sodass $C \cap S_i$ zwei oder mehr \emph{Stellen} $(i,p), (i,p') \in \mathcal{S}$ enthält. In diesem Fall nennen wir auch $(i,p)$ und $(i,p')$ \emph{mehrdeutig}. Analog nennen wir eine Äquivalenzrelation $\mathcal{R}$ \emph{mehrdeutig}, wenn $\mathcal{R}$ eine \emph{mehrdeutige} Äquivalenzklasse enthält.
	
	Eine \emph{nicht-mehrdeutige} Teilmenge $C \subset \mathcal{S}$ bezeichnen wir als \emph{partielle Zuweisungsspalte}.
\end{definition}

Es lässt sich folgern, dass eine \emph{konsistente} Äquivalenzrelation auch nicht \emph{mehrdeutig} ist, während das Gegenteil im Allgemeinen nicht gilt. Das liegt daran, dass es überkreuzte Zuweisungen geben kann, die aber \emph{partielle Zuweisungsspalten} sind. \emph{Nicht-mehrdeutige} Äquivalenzrelationen bestehen nur aus \emph{partiellen Zuweisungsspalten}.

Im Folgenden sei eine Menge von \emph{Fragmenten} $\mathcal{F} = \{f_1, \dots, f_k\}$ gegeben. Normalerweise werden dies die \emph{Fragmente} unserer paarweisen \emph{Alignments} sein, aber theoretisch kann man auch anders bestimmte benutzen. Wir wollen möglichst wenige Verbindungen aus der durch die \emph{Fragmente} induzierte Relation $\mathcal{R}$ löschen, bis eine \emph{nicht-mehrdeutige} Äquivalenzrelation $\mathcal{R'}$ bleibt.

\begin{definition}[Inzidenzgraph]
	Es sei ein \emph{Stellenraum} $\mathcal{S}$ mit einer Menge von \emph{Fragmenten} $\mathcal{F}$ auf diesem gegeben. Dann bezeichnen wir den ungerichteten Graphen $G_{\mathcal{F}} = (\mathcal{S},E_{\mathcal{F}})$ als \emph{Inzidenzgraphen}. In diesem Graph existiert genau dann eine Kante $(u,v) \in E_{\mathcal{F}}$, wenn die \emph{Stellen} $u$ und $v$ in einem gemeinsamen \emph{Fragment} $f_i \in \mathcal{F}$ vorkommen.
\end{definition} 

Wie man sieht, sind die Zusammenhangskomponenten unseres \emph{Inzidenzgraphen} genau die Äquivalenzklassen der durch $\mathcal{F}$ induzierten Kanten. Weil diese nicht weiter nützlich sind, kann man \emph{Stellen}, die nicht mit anderen verbunden sind, von vornherein ignorieren beziehungsweise sie löschen, wenn sie nach dem Entfernen von Kanten Grad 0 haben. So lässt sich etwas Speicherplatz sparen und die Ergebnisse dieses Algorithmus können direkt für den nächsten Schritt weiterverwendet werden. Um die Positionierung der Zusammenhangskomponenten im \emph{Inzidenzgraph} zu verdeutlichen, lassen wir sie in unseren graphischen Beispiele aber stehen.

\subsection{Beispiel Inzidenzgraph}

Auch hier werden wir wieder unser Beispiel aus dem letzten Kapitel benutzen. Erinnern wir uns zunächst an die \emph{Fragmente} der paarweisen \emph{Alignments}. Die \emph{Überlappgewichte} brauchen wir für diesen Schritt des Algorithmus nicht, weil wir versuchen mit Hilfe von \emph{minimalen Schnitten} ein ähnliches aber besseres Ergebnis zu erreichen.

\begin{tabular}{r|c|c||r|c|c||r|c|c}
	Seq. & \emph{Frag.} & \emph{Ü-Gew.} & Seq. & \emph{Frag.} & \emph{Ü-Gew.} & Seq. & \emph{Frag.} & \emph{Ü-Gew.}\\
	\hline
	2 & \texttt{GTCADCTC} & \multirow{2}{*}{16} & 1 & \texttt{TCTCA} & \multirow{2}{*}{7} & 1 & \texttt{GT} &\multirow{2}{*}{2} \\
	4 & \texttt{GTCADATC} &                     & 3 & \texttt{TATCA} &                     & 2 & \texttt{GT} & \\
	3 & \texttt{TCAD} & \multirow{2}{*}{8} & 1 & \texttt{CTCA} & \multirow{2}{*}{8} & 1 & \texttt{TC} & \multirow{2}{*}{2} \\
	4 & \texttt{TCAD} &                     & 2 & \texttt{CTCA} &                          & 4 & \texttt{TC} & \\
	2 & \texttt{TCAD} & \multirow{2}{*}{8} & 1 & \texttt{DGTC} & \multirow{2}{*}{8} &    &   & \\
	3 & \texttt{TCAD} &                     & 4 & \texttt{DGTC} &                     &    &   & \\
\end{tabular}

Diese \emph{Fragmente} überführen wir direkt in einen \emph{Inzidenzgraphen} bei denen die \emph{Stellen} als Knoten und die Verbindungen in gemeinsamen \emph{Fragmenten} als Kanten übertragen werden. Um die Übersichtlichkeit halbwegs zu wahren, wurden die Kanten jeder Zusammenhangskomponente jeweils in einer Farbe markiert.

\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill=gray,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 1.1cm
	]
	% erste Sequenz
	\node[mycircle] (c11) {A};
	\node[mycircle,right=of c11] (c12) {D};
	\node[mycircle,right=of c12] (c13) {G};
	\node[mycircle,right=of c13] (c14) {T};
	\node[mycircle,right=of c14] (c15) {C};
	\node[mycircle,right=of c15] (c16) {T};
	\node[mycircle,right=of c16] (c17) {C};
	\node[mycircle,right=of c17] (c18) {A};
	
	% zweite Sequenz
	\node[mycircle,below=of c11] (c21) {G};
	\node[mycircle,right=of c21] (c22) {T};
	\node[mycircle,right=of c22] (c23) {C};
	\node[mycircle,right=of c23] (c24) {A};
	\node[mycircle,right=of c24] (c25) {D};
	\node[mycircle,right=of c25] (c26) {C};
	\node[mycircle,right=of c26] (c27) {T};
	\node[mycircle,right=of c27] (c28) {C};
	\node[mycircle,right=of c28] (c29) {A};
	
	% dritte Sequenz
	\node[mycircle,below=of c21] (c31) {T};
	\node[mycircle,right=of c31] (c32) {A};
	\node[mycircle,right=of c32] (c33) {T};
	\node[mycircle,right=of c33] (c34) {C};
	\node[mycircle,right=of c34] (c35) {A};
	\node[mycircle,right=of c35] (c36) {D};
	\node[mycircle,right=of c36] (c37) {G};
	\node[mycircle,right=of c37] (c38) {G};
	
	% vierte Sequenz
	\node[mycircle,below=of c31] (c41) {D};
	\node[mycircle,right=of c41] (c42) {G};
	\node[mycircle,right=of c42] (c43) {T};
	\node[mycircle,right=of c43] (c44) {C};
	\node[mycircle,right=of c44] (c45) {A};
	\node[mycircle,right=of c45] (c46) {D};
	\node[mycircle,right=of c46] (c47) {A};
	\node[mycircle,right=of c47] (c48) {T};
	\node[mycircle,right=of c48] (c49) {C};
		
	
	% Erste Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c16/c33//above,
		c14/c22//above,
		c33/c22//above,
		c33/c43//above,
		c22/c43//above,
		c27/c48//above,
		c14/c31//above,
		c14/c43//above,
		c16/c48//above,
		c16/c27//above}
	\draw [draw=blue, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	% Zweite Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c17/c34//above,
		c34/c44//above,
		c23/c34//above,
		c17/c28//above,
		c23/c44//above,
		c28/c49//above,
		c15/c32//above,
		c15/c26//above,
		c15/c44//above,
		c17/c49//above,
		c26/c47//above}
	\draw [draw=green, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	% Dritte Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c18/c35//above,
		c35/c24//above,
		c35/c45//above,
		c24/c45//above,
		c18/c29//above}
	\draw [draw=red, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	% Vierte Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c25/c36//above,
		c25/c46//above,
		c36/c46//above}
	\draw [draw=cyan, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	% Fünfte Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c21/c42//above,
		c13/c21//above,
		c13/c42//above}
	\draw [draw=lime, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);
	
	% Sechste Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c12/c41//above}
	\draw [draw=magenta, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);
	\end{tikzpicture}
\end{center}

\subsection{Mehrdeutigkeiten Auflösen}

Die Zusammenhangskomponenten unseres \emph{Inzidenzgraphen} werden wir jetzt als \emph{Flussnetzwerke} interpretieren, um mit Hilfe eines Algorithmus zur Berechnung eines \emph{minimalen Schnitts} solange Kanten zu entfernen, bis alle \emph{Mehrdeutigkeiten} aufgelöst sind.

Sei $C$ eine Zusammenhangskomponente von $G_{\mathcal{F}}$, die \emph{mehrdeutig} ist, also zwei Knoten $x,y$ aus der selben Sequenz enthält. Wir wählen die \emph{mehrdeutigen} Knoten $x$ und $y$ als \emph{Quelle} und \emph{Senke} unseres \emph{Flussnetzwerks}. Die ungerichteten Kanten des \emph{Inzidenzgraphen} werden durch zwei antiparallele gerichtete Kanten ersetzt. \cite{cpm10} benutzen als Kapazitäten jeweils 1. Meiner Meinung nach wäre es hingegen sinnvoller die Kapazitäten so zu wählen, dass ähnliche Symbole seltener voneinander durch den \emph{minimalen Schnitt} und das damit einhergehende Löschen von Kanten getrennt werden. Das können wir beispielsweise erreichen, indem wir uns an den Ähnlichkeitswerten unserer Substitutionsmatrix orientieren. Weil \emph{Flussnetzwerke} nicht-negative Kapazitäten erwarten, müssen die Werte gegebenenfalls modifiziert werden, indem wir den betragsmäßig größten Eintrag der Matrix zu allen Einträgen addieren. Das bedeutet, dass bei unserer $(+3/-1)$-Substitutionsmatrix aus dem Beispiel zwischen übereinstimmenden Symbolen eine Kapazität von 6 und zwischen nicht übereinstimmendes eine von 2 benutzt wird. Leider war es aufgrund der begrenzten Zeit und des hohen Aufwands nicht möglich das ganze Verfahren zu implementieren. Eine Evaluierung mit echten \emph{Alignments} fehlt also leider. Weil ich den Ansatz mit den nicht-unitären Kapazitäten als sehr sinnvoll erachte, habe ich ihn bei unseren Beispielen verwendet.

Als nächstes benutzen wir einen Algorithmus zur Bestimmung des \emph{maximalen Flusses}. Wie mit dem \emph{Max-Flow-Min-Cut-Satz} gezeigt, bestimmen wir durch die Bestimmung des \emph{maximalen Flusses} auch gleichzeitig einen \emph{minimalen Schnitt}. Das ist die \enquote{schmalste} Verbindungsstelle zwischen der \emph{Quelle} und der \emph{Senke} und wir hoffen durch Löschen der dazugehörigen Kanten die \emph{Mehrdeutigkeit} aufzulösen und die dichtbesetzten Untergraphen zu erhalten. Im Original wird der Algorithmus von Edmonds-Karp benutzt, während wir stattdessen einen \emph{Push-Relabel-Algorithmus} mit Laufzeit \emph{$\oh(|V|^2\cdot \sqrt{|E|})$} für die Komplexität betrachten und in unserer Implementierung verwenden beziehungsweise ihn verwendet hätten, wenn die Zeit dafür gereicht hätte. Nachdem der \emph{minimale Schnitt} bestimmt wurde, löschen wir alle Kanten zwischen den Mengen $A$ und $B$. Auf diese Art und Weise wird unsere Zusammenhangskomponente $C$ in zwei neue Zusammenhangskomponenten $A$ und $B$ aufgeteilt. Dieses Verfahren wiederholen wir solange, bis es keine \emph{mehrdeutigen} Äquivalenzklassen mehr gibt.  \improvement{Graphik aus CPM10 mit Zusammenhangskomponenten einfügen.}


\begin{algorithm}
	\caption{Algorithmus zum Auflösen von Mehrdeutigkeiten in einem \emph{Inzidenzgraphen}}
	\label{alg:amb_res}
	\begin{algorithmic}[1]
		\Require \emph{Inzidenzgraph} $G_{\mathcal{F}} = (\mathcal{S},E_{\mathcal{F}})$ über einem Satz \emph{Fragmente} $\mathcal{F}$
		\Procedure{ResolveAmbiguities}{$G_{\mathcal{F}}$}
			\State{$E \gets E_{\mathcal{F}}$}
			\State{Berechne Zusammenhangskomponenten von $G_{\mathcal{F}}$}
			\While{es ex. \emph{mehrdeutige} Zusammenhangskomponente $C$ von $G_{\mathcal{F}}$}
				\While{es ex. \emph{mehrdeutige} Knoten $x,y$ aus der selben Sequenz}
					\State{Wähle Sequenz $S_i$ mit max. Anzahl an \emph{mehrdeutigen} Knoten in $C$}
					\State{Wähle $s = \text{argmin}\{\deg(v)|v \in C\: \text{und}\: v \in S_i\}$}
					\State{Wähle $t = \text{argmax}\{\deg(v)|v \in C\: \text{und}\: v \in S_i\}$}
					\State{Definiere \emph{Flussnetzwerk} auf $C$ mit \emph{Quelle} $s$ und \emph{Senke} $t$}
					\State{Benutze \textrm{PushRelabel}, um \emph{minimalen Schnitt} $C_1$ und $C_2$ zu bestimmen}
					\State{Lösche Kanten zwischen $C_1$ und $C_2$ aus $E$}
				\EndWhile
			\EndWhile
		\State{\textbf{return} \emph{nicht-mehrdeutigen} Subgraphen $(\mathcal{S},E)$ von $G_{\mathcal{F}}$}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

Unglücklicherweise können die Zusammenhangskomponenten bei \emph{Alignments} zwischen vielen langen Sequenzen sehr groß werden. \cite{cpm10} haben daher eine Grenze $k = \max\{\deg(v)| v \in \mathcal{S}\}$ eingeführt, die sukzessive gesenkt wird, bis alle \emph{Mehrdeutigkeiten} aufgelöst wurden. $k$ wird so benutzt, dass alle Kanten zwischen Knoten mit Grad $< k$ zunächst nicht betrachtet werden, sodass andere kleinere Zusammenhangskomponenten vorliegen. Als erstes werden für den reduzierten Kantensatz $E_k = \{(u,v) \in E|\min\{\deg(u),\deg(v)\}\}$ solange \emph{minimale Schnitte} berechnet und Kanten gelöscht, bis für den Graph mit weniger Kanten keine \emph{mehrdeutigen} Zusammenhangskomponenten mehr existierten. Danach wird $k$ um eins reduziert und das Vorgehen auf dem neuen Kantensatz wiederholt. Sobald $k$ auf null gesetzt wurde, hat man alle Knoten betrachtet und es liegen nur noch \emph{partielle Zuweisungsspalten} vor. \improvement{keine eigene Implementierung, also Text über k anpassen.}

Abgesehen von der verbesserten Laufzeit scheint dieses Vorgehen aber keine Vorteile zu bieten und falls doch, werden diese in \cite{cpm10} nicht genannt. Deshalb verzichte ich auf die Grenze $k$ und gehe davon aus, dass aufgrund des deutlich effizienteren Algorithmus für den \emph{maximalen Flusses} und die Verwendung einer schnellen Graphimplementierung aus einer Bibliothek keine Laufzeitprobleme für die üblichen Anwendungsgrößen auftreten. \improvement{Dieser Abschnitt muss noch ein bisschen umformuliert werden, weil das Verfahren nicht implementiert wurde. Also lieber den ursprünglichen Algorithmus mit Grenze k zeigen.}

Bis jetzt wurde nur gesagt, dass wir \emph{mehrdeutige} Zusammenhangskomponenten auswählen und für diese jeweils einen Knoten aus der selben Sequenz als \emph{Quelle} und \emph{Senke} wählen. Abschließend müssen wir also noch festlegen in welcher Reihenfolge diese gewählt werden. Für die Zusammenhangskomponenten müssen wir keine Reihenfolge festlegen, weil diese unabhängig voneinander sind. Sei eine \emph{mehrdeutige} Zusammenhangskomponente $C$ gegeben. Dann mag es mehrere Sequenzen geben, die alle zu mehr als einem Knoten in $C$ korrespondieren. Außerdem muss es nicht unbedingt einen eindeutigen \emph{minimalen Schnitt} geben, sondern es kann mehrere solche geben. Wir entscheiden uns für dieses Vorgehen:

\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
	\item Falls es mehrere Sequenzen gibt, die zwei oder mehr \emph{mehrdeutige} \emph{Stellen} in $C$ enthalten, wähle die Sequenz $S_i$ mit den meisten \emph{mehrdeutigen} Knoten in dieser Zusammenhangskomponente.
	\item Sobald $S_i$ bestimmt ist, wähle unter allen \emph{Stellen} aus dieser Sequenz in $C$ den Knoten mit dem niedrigsten Knotengrad als Quelle und den mit dem höchsten als \emph{Senke}.
	\item Sollte es mehr als einen \emph{minimalen Schnitt} geben, dann orientieren wir uns an dem Ergebnis des Algorithmus der Implementierung und wählen die Partitionierung, die sich durch die nur durch Kanten mit \emph{Restwertkapazität} 0 verbundenen Subgraphen im \emph{Restwertgraphen} ergibt. 
\end{enumerate}

\subsection{Beispiel von \textrm{ResolveAmbiguities}}

Exemplarisch betrachten wir die grüne Zusammenhangskomponente, weil diese die meisten Knoten enthält und werden an ihrem Beispiel solange den \emph{minimalen Schnitt} durchführen, bis keine \emph{Inkonsistenzen} mehr vorliegen. Die \emph{Quelle} ist grün, während die \emph{Senke} rot dargestellt ist.

\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill=gray,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	mysenke/.style={
		circle,
		draw=black,
		fill=red,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},	
	myquelle/.style={
		circle,
		draw=black,
		fill=green,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 0.9cm
	]
	% erste Sequenz
	\node[mycircle] (c11) {A};
	\node[mycircle,right=of c11] (c12) {D};
	\node[mycircle,right=of c12] (c13) {G};
	\node[mycircle,right=of c13] (c14) {T};
	\node[mycircle,right=of c14] (c15) {C};
	\node[mycircle,right=of c15] (c16) {T};
	\node[mycircle,right=of c16] (c17) {C};
	\node[mycircle,right=of c17] (c18) {A};
	
	% zweite Sequenz
	\node[mycircle,below=of c11] (c21) {G};
	\node[mycircle,right=of c21] (c22) {T};
	\node[mycircle,right=of c22] (c23) {C};
	\node[mycircle,right=of c23] (c24) {A};
	\node[mycircle,right=of c24] (c25) {D};
	\node[mycircle,right=of c25] (c26) {C};
	\node[mycircle,right=of c26] (c27) {T};
	\node[mycircle,right=of c27] (c28) {C};
	\node[mycircle,right=of c28] (c29) {A};
	
	% dritte Sequenz
	\node[mycircle,below=of c21] (c31) {T};
	\node[mycircle,right=of c31] (c32) {A};
	\node[mycircle,right=of c32] (c33) {T};
	\node[mycircle,right=of c33] (c34) {C};
	\node[mycircle,right=of c34] (c35) {A};
	\node[mycircle,right=of c35] (c36) {D};
	\node[mycircle,right=of c36] (c37) {G};
	\node[mycircle,right=of c37] (c38) {G};
	
	% vierte Sequenz
	\node[mycircle,below=of c31] (c41) {D};
	\node[mycircle,right=of c41] (c42) {G};
	\node[mycircle,right=of c42] (c43) {T};
	\node[mysenke,right=of c43] (c44) {C};
	\node[mycircle,right=of c44] (c45) {A};
	\node[mycircle,right=of c45] (c46) {D};
	\node[myquelle,right=of c46] (c47) {A};
	\node[mycircle,right=of c47] (c48) {T};
	\node[mycircle,right=of c48] (c49) {C};

	% Zweite Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c17/c34/{0,6}/above,
		c15/c32/{0,2}/above,
		c34/c44/{0,6}/above,
		c23/c34/{0,6}/above,
		c17/c28/{0,6}/above,
		c23/c44/{0,6}/above,
		c28/c49/{0,6}/above,
		c15/c26/{2/6}/above,
		c15/c44/{2,6}/above,
		c17/c49/{0,6}/below}
	\draw [Stealth-Stealth, draw=black, double=white, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c26/c47/{2/2}/above}
	\draw [Stealth-Stealth, draw=black, double=white, dotted, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	\end{tikzpicture}
\end{center}

Sowohl $S_2$, als auch $S_4$ haben drei Knoten in $C$. Wir wählen hier $S_4[7]$ als \emph{Quelle} und $S_4[4]$ als \emph{Senke}, weil diese den kleinsten bzw. größten Knotengrad haben. In diesem Fall passiert nichts spannendes, weil der \emph{minimale Schnitt} aufgrund der Kante mit Kapazität 2 direkt die \emph{Quelle} vom Rest der Zusammenhangskomponente trennt.

Im nächsten Schritt hat $S_2$ mit 3 die meisten Knoten in $C$. Da alle zwei der Knoten Grad 2 haben, entscheiden wir uns für $S_2[6]$ als \emph{Quelle} und $S_2[8]$ als \emph{Senke}. Auch hier wird direkt die erste Kante an der \emph{Quelle} gelöscht.

\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill=gray,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	mysenke/.style={
		circle,
		draw=black,
		fill=red,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},	
	myquelle/.style={
		circle,
		draw=black,
		fill=green,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 0.9cm
	]
	% erste Sequenz
	\node[mycircle] (c11) {A};
	\node[mycircle,right=of c11] (c12) {D};
	\node[mycircle,right=of c12] (c13) {G};
	\node[mycircle,right=of c13] (c14) {T};
	\node[mycircle,right=of c14] (c15) {C};
	\node[mycircle,right=of c15] (c16) {T};
	\node[mycircle,right=of c16] (c17) {C};
	\node[mycircle,right=of c17] (c18) {A};
	
	% zweite Sequenz
	\node[mycircle,below=of c11] (c21) {G};
	\node[mycircle,right=of c21] (c22) {T};
	\node[mysenke,right=of c22] (c23) {C};
	\node[mycircle,right=of c23] (c24) {A};
	\node[mycircle,right=of c24] (c25) {D};
	\node[myquelle,right=of c25] (c26) {C};
	\node[mycircle,right=of c26] (c27) {T};
	\node[mycircle,right=of c27] (c28) {C};
	\node[mycircle,right=of c28] (c29) {A};
	
	% dritte Sequenz
	\node[mycircle,below=of c21] (c31) {T};
	\node[mycircle,right=of c31] (c32) {A};
	\node[mycircle,right=of c32] (c33) {T};
	\node[mycircle,right=of c33] (c34) {C};
	\node[mycircle,right=of c34] (c35) {A};
	\node[mycircle,right=of c35] (c36) {D};
	\node[mycircle,right=of c36] (c37) {G};
	\node[mycircle,right=of c37] (c38) {G};
	
	% vierte Sequenz
	\node[mycircle,below=of c31] (c41) {D};
	\node[mycircle,right=of c41] (c42) {G};
	\node[mycircle,right=of c42] (c43) {T};
	\node[mycircle,right=of c43] (c44) {C};
	\node[mycircle,right=of c44] (c45) {A};
	\node[mycircle,right=of c45] (c46) {D};
	\node[mycircle,right=of c46] (c47) {A};
	\node[mycircle,right=of c47] (c48) {T};
	\node[mycircle,right=of c48] (c49) {C};
	
	% Zweite Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c17/c34/{0,6}/above,
		c15/c32/{0,2}/above,
		c34/c44/{0,6}/above,
		c23/c34/{0,6}/above,
		c17/c28/{0,6}/above,
		c23/c44/{6,6}/above,
		c28/c49/{0,6}/above,
		c15/c44/{6,6}/above,
		c17/c49/{0,6}/below}
	\draw [Stealth-Stealth, draw=black, double=white, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c26/c15/{6/6}/above}
	\draw [Stealth-Stealth, draw=black, double=white, dotted, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	\end{tikzpicture}
\end{center}

Gehen nun wieder zu Sequenz $S_4$ und wählen $S_4[9]$ als \emph{Quelle} und $S_4[4]$ als \emph{Senke}. Hier haben wir zwei relativ dichte Subgraphen, die nur durch die eine Kante $(S_1[7] \rightarrow S_3[4])$ miteinander verbunden sind. Diese löschen wir und danach gibt es in der rechten Zusammenhangskomponente keine \emph{Mehrdeutigkeiten} mehr. 

\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill=gray,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	mysenke/.style={
		circle,
		draw=black,
		fill=red,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},	
	myquelle/.style={
		circle,
		draw=black,
		fill=green,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 0.9cm
	]
	% erste Sequenz
	\node[mycircle] (c11) {A};
	\node[mycircle,right=of c11] (c12) {D};
	\node[mycircle,right=of c12] (c13) {G};
	\node[mycircle,right=of c13] (c14) {T};
	\node[mycircle,right=of c14] (c15) {C};
	\node[mycircle,right=of c15] (c16) {T};
	\node[mycircle,right=of c16] (c17) {C};
	\node[mycircle,right=of c17] (c18) {A};
	
	% zweite Sequenz
	\node[mycircle,below=of c11] (c21) {G};
	\node[mycircle,right=of c21] (c22) {T};
	\node[mycircle,right=of c22] (c23) {C};
	\node[mycircle,right=of c23] (c24) {A};
	\node[mycircle,right=of c24] (c25) {D};
	\node[mycircle,right=of c25] (c26) {C};
	\node[mycircle,right=of c26] (c27) {T};
	\node[mycircle,right=of c27] (c28) {C};
	\node[mycircle,right=of c28] (c29) {A};
	
	% dritte Sequenz
	\node[mycircle,below=of c21] (c31) {T};
	\node[mycircle,right=of c31] (c32) {A};
	\node[mycircle,right=of c32] (c33) {T};
	\node[mycircle,right=of c33] (c34) {C};
	\node[mycircle,right=of c34] (c35) {A};
	\node[mycircle,right=of c35] (c36) {D};
	\node[mycircle,right=of c36] (c37) {G};
	\node[mycircle,right=of c37] (c38) {G};
	
	% vierte Sequenz
	\node[mycircle,below=of c31] (c41) {D};
	\node[mycircle,right=of c41] (c42) {G};
	\node[mycircle,right=of c42] (c43) {T};
	\node[mysenke,right=of c43] (c44) {C};
	\node[mycircle,right=of c44] (c45) {A};
	\node[mycircle,right=of c45] (c46) {D};
	\node[mycircle,right=of c46] (c47) {A};
	\node[mycircle,right=of c47] (c48) {T};
	\node[myquelle,right=of c48] (c49) {C};
	
	% Zweite Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c15/c32/{0,2}/above,
		c34/c44/{0,6}/above,
		c23/c34/{6,6}/above,
		c17/c28/{0,6}/above,
		c23/c44/{6,6}/above,
		c28/c49/{0,6}/above,
		c15/c44/{6,6}/above,
		c17/c49/{6,6}/below}
	\draw [Stealth-Stealth, draw=black, double=white, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c17/c34/{6/6}/above}
	\draw [Stealth-Stealth, draw=black, double=white, dotted, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	\end{tikzpicture}
\end{center}

Abschließend wird noch die Verbindung zwischen dem \texttt{A} aus der zweiten Sequenz und dem \texttt{C} an der fünften Stelle der ersten gelöscht, was hier nicht mehr graphisch dargestellt wurde. Danach sind alle \emph{Mehrdeutigkeiten} aufgelöst und es liegen nur noch \emph{partielle Zuweisungsspalten} vor.

Wenn wir den Algorithmus auch auf allen anderen Zusammenhangskomponenten anwenden, dann kommen wir zu folgendem \emph{Inzidenzgraphen} ohne \emph{Mehrdeutigkeiten}:
 
\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill=gray,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 1.1cm
	]
	% erste Sequenz
	\node[mycircle] (c11) {A};
	\node[mycircle,right=of c11] (c12) {D};
	\node[mycircle,right=of c12] (c13) {G};
	\node[mycircle,right=of c13] (c14) {T};
	\node[mycircle,right=of c14] (c15) {C};
	\node[mycircle,right=of c15] (c16) {T};
	\node[mycircle,right=of c16] (c17) {C};
	\node[mycircle,right=of c17] (c18) {A};
	
	% zweite Sequenz
	\node[mycircle,below=of c11] (c21) {G};
	\node[mycircle,right=of c21] (c22) {T};
	\node[mycircle,right=of c22] (c23) {C};
	\node[mycircle,right=of c23] (c24) {A};
	\node[mycircle,right=of c24] (c25) {D};
	\node[mycircle,right=of c25] (c26) {C};
	\node[mycircle,right=of c26] (c27) {T};
	\node[mycircle,right=of c27] (c28) {C};
	\node[mycircle,right=of c28] (c29) {A};
	
	% dritte Sequenz
	\node[mycircle,below=of c21] (c31) {T};
	\node[mycircle,right=of c31] (c32) {A};
	\node[mycircle,right=of c32] (c33) {T};
	\node[mycircle,right=of c33] (c34) {C};
	\node[mycircle,right=of c34] (c35) {A};
	\node[mycircle,right=of c35] (c36) {D};
	\node[mycircle,right=of c36] (c37) {G};
	\node[mycircle,right=of c37] (c38) {G};
	
	% vierte Sequenz
	\node[mycircle,below=of c31] (c41) {D};
	\node[mycircle,right=of c41] (c42) {G};
	\node[mycircle,right=of c42] (c43) {T};
	\node[mycircle,right=of c43] (c44) {C};
	\node[mycircle,right=of c44] (c45) {A};
	\node[mycircle,right=of c45] (c46) {D};
	\node[mycircle,right=of c46] (c47) {A};
	\node[mycircle,right=of c47] (c48) {T};
	\node[mycircle,right=of c48] (c49) {C};
	
	
	% Erste Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c14/c22//above,
		c33/c22//above,
		c33/c43//above,
		c22/c43//above,
		c27/c48//above,
		c14/c43//above,
		c16/c48//above,
		c16/c27//above}
	\draw [draw=blue, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	% Zweite Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c34/c44//above,
		c23/c34//above,
		c17/c28//above,
		c23/c44//above,
		c28/c49//above,
		c15/c44//above,
		c17/c49//above}
	\draw [draw=green, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	% Dritte Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c18/c35//above,
		c35/c24//above,
		c35/c45//above,
		c24/c45//above}
	\draw [draw=red, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	% Vierte Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c25/c36//above,
		c25/c46//above,
		c36/c46//above}
	\draw [draw=cyan, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	% Fünfte Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c21/c42//above,
		c13/c21//above,
		c13/c42//above}
	\draw [draw=lime, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);
	
	% Sechste Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c12/c41//above}
	\draw [draw=magenta, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);
	\end{tikzpicture}
\end{center}

Obwohl in diesem Graphen keine \emph{Mehrdeutigkeiten} mehr existieren, ist die Zuordnung noch nicht \emph{konsistent}. Das liegt an Überkreuzungen, wie der zwischen $(S_1[8] \rightarrow S_3[5] \rightarrow S_4[5])$ (rot) und $(S_1[7] \rightarrow S_4[9])$ (grün). 

\subsection{Komplexität}\label{ch:ra_kompl}

Der \emph{Inzidenzgraph} lässt sich aus den \emph{Fragmenten} der paarweisen \emph{Alignments} in $\oh(n^2\cdot L)$ berechnen, weil es $\oh(n^2)$ davon gibt, die bis zu $\oh(L)$ Verbindungen enthalten.

Die Laufzeit von \textrm{ResolveAmbiguities} wird von der Laufzeit zur Berechnung des \emph{minimalen Schnitts} durch \textrm{PushRelabel} dominiert \citep{cpm10}. Diese hängt von der Größe unserer Zusammenhangskomponenten ab. Im schlimmsten Fall besteht der \emph{Inzidenzgraph} aus einer einzigen Zusammenhangskomponente bei der jeder Knoten mit mindestens einem Knoten aus jeder anderen Sequenz verbunden ist. In diesem Fall muss sie in vielen Schritten zerkleinert werden muss, bis nur noch \emph{partielle Zuweisungsspalten} übrig bleiben. Eine Zusammenhangskomponente kann $n\cdot L$ Knoten und $\oh(n^2\cdot L)$ Kanten haben. Das liegt daran, dass wir als Grundlage unsere paarweisen \emph{Alignments} benutzen, bei denen jede \emph{Stelle} für jede andere Sequenz mit höchstens einer \emph{Stelle} aus dieser verbunden ist. 

\textrm{PushRelabel} hat eine Komplexität von $\oh(|V|^2\cdot \sqrt{|E|})$, um einen \emph{minimalen Schnitt} zu berechnen. Weil $|V| \in \oh(n\cdot L)$ und $|E| \in \oh(n^2\cdot L)$ gelten, braucht man für einen Durchlauf also $\oh(n^3\cdot L^{5/2})$. Wenn wir Pech haben, entfernt jeder Aufruf unseres Algorithmus für den \emph{maximalen Fluss} nur einen einzigen Knoten aus der Zusammenhangskomponente. Das resultiert in $n\cdot L$ Aufrufen und einer Gesamtlaufzeit von $\oh(n^4\cdot L^{7/2})$. Die Kosten für das Bestimmen der Zusammenhangskomponenten, das Finden der \emph{Quellen} und \emph{Senken} und das Löschen von Kanten liegen jeweils in linearer Größe zur Größe des Graphen und werden daher hier nicht weiter betrachtet. Für ersteres benutzt man beispielsweise eine Tiefensuche, fürs zweite eine Breitensuche und letzteres ist simples Iterieren über alle Kanten.

Möglicherweise lässt sich eine bessere Laufzeit erreichen, wenn alle Kanten eine uniforme Kapazität haben, wie das bei der Variante der Autoren der Fall ist. Es lässt sich zeigen, dass das ein deutlich leichteres Problem ist \citep{gt14} und nach Karzanov und Even hat eine Variante des Algorithmus von Dinic in diesem Fall gute Laufzeiten.

Glücklicherweise sind die Zusammenhangskomponenten im Allgemeinen nicht so groß und in den meisten Fällen trennt man mit dem \emph{minimalen Schnitt} auch nicht nur einzelne Knoten ab. Bei Testläufen auf der Protein-Referenzdatenbank BAliBASE haben \cite{cpm10} die Referenzmenge RV12 genauer betrachtet. Sie besteht aus 88 Sequenzfamilien, die im Schnitt zehn Sequenzen enthielten. Messungen haben ergeben, dass die \emph{Inzidenzgraphen} auf diesen Sequenzfamilien im Schnitt 2877 Knoten und 10952 in 223 Zusammenhangskomponenten enthalten haben. Auf diesen Sequenzen lassen sich in guten Laufzeiten von weniger als einer Minute multiple \emph{Sequenzalignments} berechnen. Anders sah es auf der Sequenzfamilie BB30003 aus. Diese besteht aus 142 Sequenzen und resultiert in einem monströsen \emph{Inzidenzgraphen}, der nur aus einer einzigen Zusammenhangskomponente besteht. Nach einer Laufzeit von 20 Stunden ohne Ergebnis wurde der Lauf auf Graphen, der $1,5\cdot 10^6$ Kanten enthält, erfolglos abgebrochen. Erst mit einer Begrenzung des minimalen \emph{Fragmentgewichts} auf 4 ließ sich in 13 Stunden ein Ergebnis erzielen. 

\section{Sukzessionsgraphen und der Algorithmus von Pitschi}

\subsection{Aufbau des Sukzessionsgraphen}

Wie wir gesehen haben, ist die Menge an Zuweisungen nach dem \textrm{ResolveAmbiguities}-Aufruf noch nicht \emph{konsistent}, aber es liegen keine \emph{Mehrdeutigkeiten} mehr vor. Das zwingt uns dazu weitere Verbindungen aus der Äquivalenzrelation zu löschen, bis diese ein \emph{Alignment}, also \emph{konsistent} ist. Wir führen dazu eine Datenstruktur ein, die die Zusammenhangskomponenten des reduzierten \emph{Inzidenzgraphen} nach ihrer Ordnung in den beteiligten Sequenzen ordnet.

\begin{definition}[Sukzessionsgraph]
	Es sei eine Menge an Sequenzen $S$ mit \emph{Stellenraum} $\mathcal{S}$ gegeben. Für diese Sequenzen liegt eine Menge $\mathcal{C}$ von Zusammenhangskomponenten vor, die alle \emph{partielle Zuweisungsspalten} sind. Dann definieren wir den \emph{Sukzessionsgraph} $SG(\mathcal{C}) = (\mathcal{C},E)$ als gerichteten, gewichteten Graphen. In $SG(\mathcal{C})$ fügen wir genau dann eine Kante $e = (C,C')$ für $C,C' \in \mathcal{C}$ ein, wenn eine Sequenz $S_i \in S$ existiert, für die \emph{Stellen} $s = (i,p) \in C$ und $s' = (i,p') \in C'$ vorliegen mit $p < p'$ und außerdem keine \emph{Stelle} $s = (i,p'')$ in einem anderen Knoten $C''$ mit $p < p'' < p'$ vorhanden ist. Das Gewicht von $e$ ist die Anzahl an Sequenzen für die die obige Bedingung gilt. Des Weiteren setzen wir voraus, dass alle Zusammenhangskomponenten mindestens zwei Knoten enthalten.
	Außerdem fügen wir zwei zusätzliche Knoten $v_{start}$ und $v_{end}$ ein. $v_{start}$ ist mit allen Knoten verbunden, die die erste \emph{Stelle} einer Sequenz enthalten. Zusätzlich sind alle Knoten, die die letzte \emph{Stelle} einer Sequenz enthalten mit $v_{end}$ über eine Kante verbunden.
\end{definition}


Wie man sieht sind zwei Knoten $C$ und $C'$ aus dem \emph{Sukzessionsgraphen} genau dann miteinander verbunden, wenn es in diesen \emph{Stellen} aus der selben Sequenz gibt, bei denen die aus $C$ links von der in $C'$ stehen. Die zusätzlich eingefügten Knoten $v_{start}$ und $v_{end}$ brauchen wir im Algorithmus von Pitschi, um die Konnektivität des Graphen zu erhalten, wenn wir Kanten aus dem Graph löschen. Das werden wir später tun, um \emph{Inkonsistenzen} zu entfernen. Setzen wir nicht voraus, dass die Knoten von $SG$ mindestens zwei \emph{Stellen} enthalten, dann liefert der Algorithmus von Pitschi suboptimale Ergebnisse, wie wir später sehen werden. \cite{cpm10} und \cite{pdc10} sind an dieser Stelle nicht eindeutig. 

Als Beispiel konstruieren wir aus dem reduzierten \emph{Inzidenzgraphen} des letzten Schritts einen \emph{Sukzessionsgraphen}. Die Knotenfarben wurden in der Farbe der Zusammenhangskomponenten des \emph{Inzidenzgraphen} aus dem letzten Schritt gewählt.

\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 1.1cm
	]
	
	\node[mycircle,fill=gray] at (0.5,-0.5) (c1) {$v_{start}$};
	\node[mycircle,fill=magenta] at (2,1.5) (c2) {\begin{tabular}{c}
		$(1,2)$ \\ $(4,1)$
		\end{tabular}};
	\node[mycircle,fill=lime] at (4,1.5) (c3) {\begin{tabular}{c}
		$(1,3)$ \\ $(2,1)$ \\ $(4,2)$
		\end{tabular}};
	\node[mycircle,fill=blue] at (4,-0.5) (c4) {\begin{tabular}{c}
		$(1,4)$ \\ $(2,2)$ \\ $(3,3)$ \\ $(4,3)$
		\end{tabular}};
	\node[mycircle,fill=green] at (6,-0.5) (c5) {\begin{tabular}{c}
		$(1,5)$ \\ $(2,3)$ \\ $(3,4)$ \\ $(4,4)$
		\end{tabular}};
	\node[mycircle,fill=red] at (8,-0.5) (c6) {\begin{tabular}{c}
		$(1,8)$ \\ $(2,4)$ \\ $(3,5)$ \\ $(4,5)$
		\end{tabular}};
	\node[mycircle,fill=cyan] at (10,-0.5) (c7) {\begin{tabular}{c}
		$(2,5)$ \\ $(3,6)$ \\ $(4,6)$
		\end{tabular}};
	\node[mycircle,fill=blue] at (8,1.5) (c8) {\begin{tabular}{c}
		$(1,6)$ \\ $(2,7)$ \\ $(4,8)$
		\end{tabular}};
	\node[mycircle,fill=green] at (10,1.5) (c9) {\begin{tabular}{c}
		$(1,7)$ \\ $(2,8)$ \\ $(4,9)$
		\end{tabular}};
	\node[mycircle, fill=gray] at (12,-0.5) (c10) {$v_{end}$};
	% Kanten ohne Cycle
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c2/2/above,
		c1/c3/1/below,
		c2/c3/2/above,	
		c1/c4/1/above,
		c3/c4/3/above,
		c4/c5/4/above,
		c5/c6/3/above,
		c5/c8/1/above,
		c9/c10/2/above,
		c7/c10/1/above}	
	\draw [myarrow] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);
	
	\draw [myarrow] (c6) to[bend right] node[sloped,font=\tiny,below] {1} (c10);
		
	% Cycle%	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c6/c7/3/above,
		c8/c9/3/above}
	\draw [myarrow, draw=red] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	

	\draw [myarrow, draw=red] (c7) -- node[sloped,font=\tiny,above, pos=0.25] {2} (c8);	
	\draw [myarrow, draw=red] (c9) -- node[sloped,font=\tiny,above, pos=0.25] {1} (c6);		
	\end{tikzpicture}
\end{center}

\begin{lemma}
	Die Menge $\mathcal{C}$ ist genau dann \emph{konsistent}, wenn $SG(\mathcal{C})$ ein gerichteter, azyklischer Graph (DAG) ist.
\end{lemma}

\begin{beweis}
	\bewhin \hspace{2pt} Sei $\mathcal{C}$ eine \emph{konsistente} Menge von Zusammenhangskomponenten mit der induzierten Äquivalenzrelation $\mathcal{R}$. Dann folgt aus $x \preceq_{\mathcal{R}} y$ auch $x \preceq y$ für alle Sequenzen. Angenommen es gibt in $SG(\mathcal{C})$ einen Zyklus. Dann gibt es Knoten $C, C'$ mit \emph{Stellen} $s_1, s_2 \in C$ und $s_1',s_2' \in C'$ mit folgenden Eigenschaften:
	\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
		\item $s_1, s_1' \in S_1$ und $s_2,s_2' \in S_2$ 
		\item $pos(s_1) < pos(s_1')$ und $pos(s_2') < pos(s_2)$
	\end{enumerate}
	Diese muss es geben, sonst gäbe es keinen Zyklus im Graphen. Für diese \emph{Stellen} gelten $s_1' \mathcal{R} s_2'$, $s_2' \preceq s_2$ und $s_2 \mathcal{R} s_1$, woraus $s_1' \preceq_{\mathcal{R}} s_1$ folgt. Es gilt aufgrund von $pos(s_1) < pos(s_1')$, weshalb $s_1 \npreceq s_1'$ folgt. Das steht im Widerspruch dazu, dass $\mathcal{C}$ \emph{konsistent} war.
	
	\bewrueck \hspace{2pt} Wir benutzen einen Kontrapositionsbeweis und es sei eine \emph{inkonsistente} Menge $\mathcal{C}$ gegeben. Dann existiert eine Sequenz $S_1$, die \emph{Stellen} $s_1,s_1' \in S_1$ mit folgenden Eigenschaften enthält:
	\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
		\item $s_1 \preceq_{\mathcal{R}} s_1'$
		\item $s_1 \npreceq s_1'$
	\end{enumerate}
	Aus $s_1 \preceq_{\mathcal{R}} s_1'$ folgt, dass es zwei Knoten $C$ und $C'$ gibt mit $s_1 \in C$ und $s_1' \in C'$, die in $SG(\mathcal{C})$ über einen Pfad verbunden sind. Der Pfad kann aber nicht über eine Kante laufen, die zu $S_1$ gehört, denn $s_1 \npreceq s_1'$ gilt. Aus dieser Bedingung folgt aber, dass einen Pfad von $C'$ zu $C'$ geben muss. Da es sowohl von $C$ nach $C'$, als auch von $C'$ nach $C$ Pfade gibt, kann $SG(\mathcal{C})$ nicht azyklisch sein.
\end{beweis}

\subsection{Der Algorithmus von Pitschi}

Aufgrund des gerade gezeigten Lemmas folgt, dass wir eine \emph{konsistente} Menge von \emph{partiellen Zuweisungsspalten} finden, wenn deren \emph{Sukzessionsgraph} azyklisch ist. Der Algorithmus von \cite{pdc10} sieht zwei Schritte vor, um aus dem potentiell zyklischen \emph{Sukzessionsgraphen} einen kreisfreien zu konstruieren, der mit einer \emph{konsistenten} Menge von \emph{partiellen Zuweisungsspalten} korrespondiert:

\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
	\item Lösche Kanten aus dem \emph{Sukzessionsgraphen}, bis dieser kreisfrei ist.
	\item Benutze den so entstandenen DAG, um zu entscheiden, welche \emph{Stellen} aus den jeweiligen Knoten gelöscht werden müssen, um \emph{Inkonsistenzen} zu entfernen.
\end{enumerate}

\subsubsection{Entfernen der Kanten}

Wir beginnen mit der Transformation des zyklischen Graphen in einen azyklischen, indem wir Kanten entfernen. Optimal wäre es Kanten mit einer minimalen Summe von Kantengewichten zu entfernen. Leider ist dieses Problem, das auch als \enquote{minimal weighted feedback arc set}-Problem bekannt ist, NP-schwer. Als Folge bedienen wir uns einfach einer einfachen Heuristik, indem wir sukzessive eine Grenze $k$ erhöhen und in jedem Schritt alle Kanten mit einem Gewicht kleiner als $k$ löschen, bis es keinen Zyklus im Graphen mehr gibt. Formal definieren wir die Menge an Kanten, die mindestens das Gewicht $k\in \mathbb{N}$ hat als 
\begin{equation}
\begin{split}
	& E_k \coloneqq \{(u,v)\in E | w(u,v) > k\: \text{oder}\: u = v_{start}\: \text{oder}\: v = v_{end}\}\: \text{mit} \\
	& k^* \coloneqq \min\{k > 0 | (V,E_k)\: \text{ist azyklisch}\}
\end{split}
\end{equation}

Für den Fall, dass durch das Löschen von Kanten der Graph nicht mehr zusammenhängend ist, fügen wir für jede Zusammenhangskomponente und jede an dieser beteiligten Sequenz eine Kante vom Startzustand $v_{start}$ zum Knoten mit der kleinsten \emph{Stelle} aus der gewählten Sequenz ein. Analog gehen wir mit den größten \emph{Stellen} und dem Endzustand $v_{end}$ vor. Diese Menge dieser Kanten nennen wir $E_c$. Auf diese Weise ist sichergestellt, dass der azyklische Graph $G^* = (V,E_{k^*}\cup E_c)$ zusammenhängend ist und über jeden Knoten ein Pfad vom Start- zum Endzustand führt.

\subsubsection{Beispiel zum Entfernen von Kanten}

Bei unserem Beispiel ist glücklicherweise nur Zyklus vorhanden, der bereits in $(V,E_1)$ nicht mehr vorhanden ist. Wir löschen also alle Kanten mit Kantengewicht 1 oder weniger, die nicht zum Start- oder Endknoten gehören. Entfernte Kanten wurden gestrichelt dargestellt. Das sieht dann so aus:  

\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 1.1cm
	]
	
	\node[mycircle,fill=gray] at (0.5,-0.5) (c1) {$v_{start}$};
	\node[mycircle,fill=magenta] at (2,1.5) (c2) {\begin{tabular}{c}
		$(1,2)$ \\ $(4,1)$
		\end{tabular}};
	\node[mycircle,fill=lime] at (4,1.5) (c3) {\begin{tabular}{c}
		$(1,3)$ \\ $(2,1)$ \\ $(4,2)$
		\end{tabular}};
	\node[mycircle,fill=blue] at (4,-0.5) (c4) {\begin{tabular}{c}
		$(1,4)$ \\ $(2,2)$ \\ $(3,3)$ \\ $(4,3)$
		\end{tabular}};
	\node[mycircle,fill=green] at (6,-0.5) (c5) {\begin{tabular}{c}
		$(1,5)$ \\ $(2,3)$ \\ $(3,4)$ \\ $(4,4)$
		\end{tabular}};
	\node[mycircle,fill=red] at (8,-0.5) (c6) {\begin{tabular}{c}
		$(1,8)$ \\ $(2,4)$ \\ $(3,5)$ \\ $(4,5)$
		\end{tabular}};
	\node[mycircle,fill=cyan] at (10,-0.5) (c7) {\begin{tabular}{c}
		$(2,5)$ \\ $(3,6)$ \\ $(4,6)$
		\end{tabular}};
	\node[mycircle,fill=blue] at (8,1.5) (c8) {\begin{tabular}{c}
		$(1,6)$ \\ $(2,7)$ \\ $(4,8)$
		\end{tabular}};
	\node[mycircle,fill=green] at (10,1.5) (c9) {\begin{tabular}{c}
		$(1,7)$ \\ $(2,8)$ \\ $(4,9)$
		\end{tabular}};
	\node[mycircle, fill=gray] at (12,-0.5) (c10) {$v_{end}$};
	% Kanten ohne Cycle
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c2/2/above,
		c1/c3/1/below,
		c2/c3/2/above,	
		c1/c4/1/above,
		c3/c4/3/right,
		c4/c5/4/above,
		c5/c6/3/above,
		c9/c10/2/above,
		c7/c10/1/above}	
	\draw [myarrow] (\i) -- node[font=\tiny,\p] {\txt} (\j);
	
	\draw [myarrow] (c6) to[bend right] node[font=\tiny,below] {1} (c10);

	\draw [myarrow,dotted] (c5) -- node[font=\tiny,above] {1} (c8);
	
	% Cycle%	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c6/c7/3/above,
		c8/c9/3/above}
	\draw [myarrow, draw=red] (\i) -- node[font=\tiny,\p] {\txt} (\j);	
	
	\draw [myarrow, draw=red] (c7) -- node[font=\tiny,above, pos=0.25] {2} (c8);	
	\draw [myarrow, draw=red, dotted] (c9) -- node[font=\tiny,above, pos=0.25] {1} (c6);		
	\end{tikzpicture}
\end{center}

\subsubsection{Entfernen von Stellen}

Als nächstes lernen wir einen von \cite{pdc10} entwickelten Algorithmus kennen, der eine minimale Anzahl an \emph{Stellen} aus dem verkleinerten \emph{Sukzessionsgraphen} $G^{*}$ löscht, um alle \emph{Inkonsistenzen} zu entfernen. Dabei orientiert sich der Algorithmus an der linearen Halbordnung $\preceq$ über $\mathcal{S}$ und der Halbordnung auf dem DAG, die ersterer angepasst werden soll. Wir bezeichnen die Halbordnung auf $\mathcal{C}$, die durch den Graph $G^{*}$ induziert wird, als $\preceq^{*}$.

Sei eine Sequenz $S_i \in S$ gegeben. Dann ist $\mathcal{C}_{S_i}$ die Teilmenge von Zusammenhangskomponenten aus $\mathcal{C}$, die \emph{Stellen} aus $S_i$ enthalten. Wir definieren außerdem eine Beschränkung der Knoten aus $G^{*}$ von $\mathcal{C}_{S_i}$ als $V_{S_i} = \mathcal{C}_{S_i} \cup \{v_{start},v_{end}\}$. Auf diesen existiert eine Ordnung $\preceq_{S_i}$, die durch die natürliche Ordnung auf $S_i$ gegeben ist, und die Halbordnung $\preceq_{S_i}^{*}$ des Graphen mit reduzierter Knotenmenge. Wir definieren $\mathcal{R}_{S_i} = \preceq_{S_i}\, \cap\, \preceq_{S_i}^{*}$. Diese Relation entspricht genau den Verbindungen der transitiven Hülle im ursprünglichen Graph, wenn man $G^{*}$ auf die Knoten aus $\mathcal{C}_{S_i}$ beschränkt. Sei dafür $G^{+} = (V,E^{+})$ die transitive Hülle $TC(G^{*})$ des DAG. Weil $G^{*}$ keine Zyklen enthält, kann auch $TC(G^{*})$ keine enthalten. Wir können $G^{*}$ auf jede unserer Sequenzen $S_i$ beschränken, indem wir den reduzierten Knotensatz $V_{S_i}$ benutzen und genau dann eine Kante zwischen zwei Knoten einfügen, falls diese in unserer Relation $\mathcal{R}_{S_i}$ liegen.

\begin{definition}
	Der Graph $G_{S_i}$ einer Sequenz $S_i$ ist definiert als Graph über der Knotenmenge $V_{S_i}$ und den Kanten $E_{S_i}$. Es gilt
	\begin{equation}
		(u,v) \in E_{S_i} \Longleftrightarrow u,v \in V_{S_i}, (u,v) \in E^{+}\: \text{und}\: u \preceq_{S_i} v \Longleftrightarrow u\: \mathcal{R}_{S_i}\: v
	\end{equation}
\end{definition}

Pfade von $v_{start}$ nach $v_{end}$ in $G_{S_i}$ sind genau die Teilmengen von \emph{partiellen Zuweisungsspalten}, die bezüglich $S_i$ \emph{konsistent} sind. Das liegt daran, dass für zwei Knoten $u,v \in V_{S_i}$ auf einem solchen Pfad sowohl $u \preceq_{S_i} v$, als auch $u \preceq_{S_i}^{*} v$ gelten und damit $u \preceq_{S_i}^{*} v \implies u \preceq_{S_i} v$. Das war aber genau die Definition für \emph{Konsistenz}: dass die Relation die natürliche Ordnung auf den Sequenzen erhält. Wie entfernen daher alle \emph{Stellen} unserer Sequenz aus den Knoten, die nicht auf dem gewählten Pfad liegen, weil diese die \emph{Konsistenz} verletzen würden. \unsure{Reicht das als Beweis oder muss das mit dem DAG aus dem Lemma gezeigt werden?}

Wenn wir jetzt für jede unserer Sequenzen $S_j$ einen Pfad von $v_{start}$ nach $v_{end}$ in $G_{S_j}$ wählen und alle nicht-besuchten \emph{Stellen}  aus ihren Knoten entfernen, dann hält die \emph{Konsistenzbedingung} auf der Relation, die durch die übriggebliebenen Zusammenhangskomponenten induziert wird. Das Resultat ist also ein \emph{Alignment}. Da wir aber nicht irgendein \emph{Alignment} erhalten wollen, sondern ein möglichst großes, wählen wir für jede Sequenz $S_i \in S$ den Pfad maximaler Länge durch $G_{S_i}$. Auf diese Weise löschen wir die minimale Anzahl an \emph{Stellen} aus ihren Zusammenhangskomponenten. Formal: Sei für eine Sequenz $S_i$ $g_{S_i}$ ein Pfad maximaler Länge $(v_{start}, u_1, \dots, u_n, v_{end})$ gegeben. Wir iterieren über alle Knoten $C \in \mathcal{C}_{S_i}$ und entfernen \emph{Stellen} $(i,p) \in C$, falls $C \notin g_{S_i}$. Wenn wir das für alle Sequenzen tun, dann nennen wir die Menge der reduzierten Zusammenhangskomponenten $C°$. Weil die mit $C°$ korrespondierende Relation ein \emph{Alignment} ist, wäre der \emph{Sukzessionsgraph} $SG(C°)$ ein DAG. Da die Pfade für jede Sequenz unabhängig voneinander sind, können wir diese in beliebiger Reihenfolge wählen, ohne dass dies einen Einfluss auf unser Ergebnis hat.

Letztendlich folgt, dass das Problem die \emph{partiellen Zuweisungsspalten} auf \emph{konsistente Zuweisungsspalten} zu reduzieren auf die Suche nach Pfaden maximaler Länge durch gerichtete azyklische Graphen abbildbar ist \citep{cpm10}.

\subsubsection{Beispiel zum Entfernen von Stellen mit dem Algorithmus von Pitschi}

Bevor wir im nächsten Abschnitt genauer kennenlernen, wie man einen längsten Pfad im DAG eigentlich genau berechnet, betrachten wir unser Beispiel, in diesem Fall nur für die Sequenz $S_1$. Weil der Algorithmus zum Berechnen des längsten Pfades die Kantengewichte ignoriert und nur Knoten zählt, wurden diese nicht mehr angegeben. Formal handelt es sich hierbei um die Suche nach einem längsten Pfad in einem ungewichteten Graphen. Die Pfade, die über die Bildung der transitiven Hülle neu dazu kamen, sind grau angedeutet.

\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 1.1cm
	]
	
	\node[mycircle,fill=gray] at (0.5,-0.5) (c1) {$v_{start}$};
	\node[mycircle,fill=magenta] at (2,2) (c2) {\begin{tabular}{c}
		$(1,2)$ \\ $(4,1)$
		\end{tabular}};
	\node[mycircle,fill=lime] at (4,2) (c3) {\begin{tabular}{c}
		$(1,3)$ \\ $(2,1)$ \\ $(4,2)$
		\end{tabular}};
	\node[mycircle,fill=blue] at (4,-0.5) (c4) {\begin{tabular}{c}
		$(1,4)$ \\ $(2,2)$ \\ $(3,3)$ \\ $(4,3)$
		\end{tabular}};
	\node[mycircle,fill=green] at (6,-0.5) (c5) {\begin{tabular}{c}
		$(1,5)$ \\ $(2,3)$ \\ $(3,4)$ \\ $(4,4)$
		\end{tabular}};
	\node[mycircle,fill=red] at (8,-0.5) (c6) {\begin{tabular}{c}
		$(1,8)$ \\ $(2,4)$ \\ $(3,5)$ \\ $(4,5)$
		\end{tabular}};
	\node[mycircle,fill=blue] at (8,2) (c8) {\begin{tabular}{c}
		$(1,6)$ \\ $(2,7)$ \\ $(4,8)$
		\end{tabular}};
	\node[mycircle,fill=green] at (10,2) (c9) {\begin{tabular}{c}
		$(1,7)$ \\ $(2,8)$ \\ $(4,9)$
		\end{tabular}};
	\node[mycircle, fill=gray] at (12,-0.5) (c10) {$v_{end}$};
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c8//above,
		c2.310/c10//above,
		c2.310/c4//above,
		c2.310/c5//above,
		c2.310/c6//above,
		c3/c5//above,
		c3/c6//above,
		c3/c8//above,
		c3/c10//above,
		c4/c8//above,
		c4/c9//above,
		c5/c9//above,
		c8/c10//above}	
	\draw [myarrow,lightgray] (\i) -- node[font=\tiny,\p] {\txt} (\j);
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c5//above,
		c1/c6//above,
		c1/c10//above,
		c4/c6//above,
		c4/c10//above,
		c5/c10//above}	
	\draw [myarrow,lightgray] (\i) to[bend right] node[font=\tiny,\p] {\txt} (\j);
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c2/c8//above,
		c2/c9//above,
		c3/c9//above}	
	\draw [myarrow,lightgray] (\i) to[bend left] node[font=\tiny,\p] {\txt} (\j);
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c3//below,	
		c1/c4//above,
		c5/c6//above,
		c8/c9//above,
		c6/c10//above}	
	\draw [myarrow] (\i) -- node[font=\tiny,\p] {\txt} (\j);
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c2//above,
		c2/c3//above,	
		c3/c4//right,
		c4/c5//above,
		c5/c8//above,
		c8/c9//above,
		c9/c10//above}	
	\draw [myarrow,red] (\i) -- node[font=\tiny,\p] {\txt} (\j);
	
	\end{tikzpicture}
\end{center}

\vspace{-5pt}

Im Graphen $G_{S_1}$ gibt es eine Kante vom $(1,5)$er-Knoten zum $(1,6)$er, obwohl diese direkte Kante in $G^{*}$ gelöscht wurde. Das liegt an der Verbindung $(2,3) \rightarrow (2,4) \rightarrow (2,5) \rightarrow (2,7)$ und $(1,5) \preceq_{S_1} (1,6)$. Die Kante von $(1,8) \rightarrow (1,6)$ gibt es aber nicht, obwohl diese in der transitiven Hülle verbunden sind. Das liegt an $(1,8) \npreceq_{S_i} (1,6)$. Der hellblaue Knoten enthält keine \emph{Stelle} aus $S_1$ und ist daher nicht Teil von $V_{S_1}$.

Wie schon beim Blick auf den ersten Sukzessionsgraphen vermutet, ist das Problem die \emph{Stelle} $(1,8)$, die für Überkreuzungen sorgt. Sie liegt nicht auf dem längsten Pfad (rot markiert) und wird daher aus der Zusammenhangskomponente des Knoten entfernt. Die längsten Pfade aller anderen Sequenzen besuchen auch alle Knoten der jeweiligen Graphen, weshalb keine weiteren Knoten modifiziert werden müssen.

\subsection{Algorithmus zur Bestimmung des längsten Pfads}

Es sei ein gerichteter, azyklischer Graph $G = V,E$ gegeben. Ein einfacher Ansatz wäre es einen Algorithmus, der auch mit negativen Kantengewichten rechnen kann zu benutzen, um im Graphen mit negierten Kantengewichten $G^{-} = (V,E^{-})$ den kürzesten Pfad zu berechnen. Der kürzeste Pfad mit umgedrehten Kantengewichten entspricht genau dem längsten Pfad im Ursprungsgraph. Der Algorithmus von Bellman-Ford berechnet den kürzesten Pfad eines Ausgangsknotens (hier $v_start$) zu allen anderen Knoten im Graphen in $\oh(|V|\cdot |E|) = \oh(n^3\cdot L^3)$ in unserem Fall \cite{clrs09}. Anders als beispielsweise der gierige Algorithmus von Dijkstra kann Bellman-Ford auch mit negativen Kanten umgehen, solange der Graph keine negativen Zyklen enthält. 

Weil unsere Graphen $G_{S_i}$ azyklisch sind lässt sich die Berechnung der längsten Pfade mit der \emph{topologischen Sortierung} der Knoten effizienter umsetzen.

\begin{definition}[Topologische Sortierung]
	Eine \emph{topologische Sortierung} von DAG $G = (V,E)$ ist eine lineare Aufzählung der Knoten in $V$ bei der ein Knoten $v$ hinter einem Knoten $u$ auftaucht, wenn es eine Kante $(u,v)\in E$ gibt.
\end{definition}

Die \emph{topologische Sortierung} liefert eine mögliche Aufzählung der Knoten, die die Halbordnung des Graphen erhält. Im Allgemeinen ist sie nicht eindeutig. Das Standardmotivation dafür ist das Anziehen von Kleidungsstücken: Ich muss meine Socken anziehen, bevor ich in meine Schuhe schlüpfen kann und es kommt zuerst die Unterwäsche und dann die Hose. Erst muss ich Unterhemd und oder T-Shirt anziehen, bevor der Pullover kommt. Diese Reihenfolgen lassen sich als DAG modellieren, wobei jede Kante zwischen Kleidungsstück $a$ und $b$ bedeutet, dass $a$ vor $b$ angezogen werden muss. Gültige \emph{topologische Sortierungen} für dieses vereinfachte Beispiel \footnote{Mr. Bean hat mit seiner Badehose bewiesen, dass die übliche Reihenfolge des An- und Ausziehens nicht unbedingt eingehalten werden muss.} sind also beispielsweise diese: \enquote{(Socken, Schuhe, Unterhose, Hose, Unterhemd, T-Shirt,Pullover)}, \enquote{(Socken, Unterhose, Schuhe, Unterhemd, T-Shirt, Pullover, Hose)} oder \enquote{(Unterhose, Hose, Socken, Unterhemd, T-Shirt, Pullover, Schuhe)}. 

Mit Hilfe einer Tiefensuche lässt sich die \emph{topologische Sortierung} in $\theta(|V| + |E|)$ berechnen \cite{clrs09}. Das Ergebnis ist eine Liste der Knoten in topologischer Reihenfolge. Die Liste können wir zur Berechnung des längsten Pfades nutzen, indem wir zusätzlich für jeden Knoten die Länge des längsten Pfades und den Vorgänger in diesem speichern. Wir starten bei $v_{start}$ mit Länge 0 und Vorgänger \enquote{NIL}. Aufgrund der gegebenen \emph{topologischen Sortierung} wissen wir, dass jeder Knoten erst dann bearbeitet wird, wenn seine Vorgänger bereits behandelt wurden. Vorgänger und längster Pfad bis zu diesem Knoten lassen sich also rekursiv berechnen:

\begin{equation}
\begin{split}
	distance(v_{start}) & = 0 \\
	distance(v)       & = \max\{distance(u) + w((u,v))\:|\:(u,v) \in E\ \wedge v \neq v_{start}\} 
\end{split}
\end{equation} 

Der Vorgänger für die Knoten wird dann abhängig von der gewählten Kante gesetzt, die die größte Gesamtlänge liefert. Das Kantengewicht $w((u,v))$ ist in unserem Fall 1, weil wir nur die Anzahl der Knoten von $v_{start}$ bis $v_{end}$. Die Korrektheit des Algorithmus folgt aufgrund der Korrektheit der Rekursionsgleichung und weil $distance(u)$ wegen der \emph{topologischen Sortierung} bereits berechnet wurde für Knoten $v$, wenn eine Kante $(u,v)$ existiert.

Jetzt kennen wir die Länge des längsten Pfades, müssen aber noch die Knoten, die zu diesem gehören, berechnen. Dazu nutzen wir einen Backtracking-Algorithmus, indem wir mit dem letzten Knoten starten und solange den Vorgänger auswählen, bis es keinen mehr gibt. Bei unserem Graphen $G_{S_i}$ ist das einfach, weil mit $v_{start}$ und $v_{end}$ auf jeden Fall der erste und letzte Knoten unseres längsten Pfads bekannt sind. In allgemeinen DAGs muss man zuerst in linearer Zeit einmal über die Liste iterieren und den Knoten mit der maximalen gespeicherten Länge als Endknoten des Pfads und Startknoten des Backtrackings wählen.

\begin{algorithm}
	\caption{Algorithmus zum berechnen des längsten Pfads in einem Graphen $G_{S_i}$}
	\label{alg:longestpath}
	\begin{algorithmic}[1]
		\Require DAG $G = (V,E)$ mit $v_{start}$ und $v_{end}$
		\Procedure{LongestPath}{$G$}
			\State {$list_{ts} \gets topologicalSort(G)$}
			\State {$v_{start}.distance \gets 0$}
			\State {$v_{start}.pred \gets $ NIL}
			\State {$current \gets list_{ts}.head$}
			\While {$current.next \neq $ NIL} \Comment {Berechne Distanz zu $v_{end}$}
				\State {$current \gets current.next$}
				\State {$maxPred \gets argmax\{u.distance\:|\:(u,current.key) \in E\}$}
				\State {$current.key.distance \gets maxPred.distance + 1$}
				\State {$current.key.pred \gets maxPred$}			
			\EndWhile
			\State {$longestPath \gets empty$} \Comment {initialisiere Ausgabeliste}
			\State {$longestPath.addFirst(v_{end}$}	
			\State {$currentNode \gets v_{end}$}		
			\While {$currentNode.pred \neq $ NIL} \Comment {Backtracking}
				\State {$currentNode \gets currentNode.pred$}
				\State {$longestPath.addFirst(currentNode)$}	
			\EndWhile
			\Return {$longestPath$}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

Im kommenden Kapitel wird die Implementierung dieses Algorithmus beispielhaft für das ganze Verfahren beschrieben. Da dort auch ein konkretes Beispiel behandelt wird, verzichte ich darauf an dieser Stelle.

\subsubsection{Laufzeit}

Im Allgemeinen hat dieser Algorithmus eine Laufzeit von $\oh(|V| + |E|)$. Die \emph{topologische Sortierung} benötigt $\theta(|V| + |E|)$ Rechenschritte. Für die Berechnung der Distanzwerte müssen in jedem der $\oh(|V|)$ Knoten das Maximum der Knoten über eingehende Kanten berechnet werden. Jede Kante wird nur einmalig betrachtet und somit ist die Summe der Kosten aller Maximumsoperationen in $\oh(|E|)$. Der Backtrackingprozess ist in linearer Zeit in der Größe der Anzahl der Knoten möglich.

Wenn wir $G_{S_i}$ betrachten, dann stellen wir fest, dass $|V| \in \oh(L)$ und $|E| \in \oh(L^2)$ liegen aufgrund der zuvor berechneten transitiven Hülle. Für die Berechnung des längsten Pfads folgt somit eine Laufzeit von $\oh(L^2)$.

\subsection{Verankerungen}

\cite{mpps06} haben einen semiautomatisierten Ansatz für das \emph{multiple sequence alignment}-Problem entwickelt. Bei diesem kann der Benutzer, der über Expertenwissen verfügt, vorgeben, welche Positionen der Sequenzen auf jeden Fall miteinander \emph{aligniert} werden sollen. Das ist insbesondere dann nützlich, wenn es Abschnitte gibt, die sich mathematisch gesehen sehr ähnlich sind, deren Zuordnung biologisch aber falsch wäre. Ein Beispiel für solche Sequenzen haben wir mit den \emph{Hoxgenen} des Pufferfisches im Abschnitt über die Schwächen von DIALIGN bereits kennengelernt. Es hat sich herausgestellt, dass das semiautomatisierte Verfahren bei vielen Tests auf der Referenzdatenbank BAliBASE bessere Ergebnisse geliefert hat, wenn die Startpunkte aller Motive als \emph{Verankerungen} gesetzt wurden.

Das Verfahren funktioniert so, dass der Experte \emph{Fragmente} (die auch nur paarweise Zuweisungen sein können) wählt und diese mit einer Dringlichkeit an DIALIGN übergibt. Wie im Standardalgorithmus werden dann gierig die \emph{Fragmente} mit maximaler Dringlichkeit gewählt, die zu allen bereits gewählten \emph{konsistent} sind. Zwischen diesen \emph{Verankerungen} wird dann ganz normal DIALIGN durchgeführt, wodurch die restlichen Abschnitte automatisiert \emph{aligniert} werden.

\cite{cpm10} benutzten die bereits vorliegende Infrastruktur in DIALIGN, um die \emph{Zuweisungsspalten}, die der Algorithmus von Pitschi geliefert hat, als \emph{Verankerungen} ins \emph{Alignment} zu integrieren. Weil diese Möglichkeit in unserer Implementierung nicht bereits vorhanden ist, werden wir die Resultate des \emph{Min-Cut}-Ansatzes direkt mit Hilfe von \textrm{EdgeAddition} und dem \emph{Alignmentgraphen} in unser Ergebnis integrieren. Neben dem Übergeben von \emph{konsistenten Zuweisungsspalten} als \emph{Verankerungen} hat man auch Versuche mit \emph{partiellen Zuweisungsspalten} gemacht. Hier war die gierige Heuristik von DIALIGN aber nicht erfolgreich und die Ergebnisse waren schlechter als mit dem Algorithmus von Pitschi. Aus diesem Grund werden wir den zweiten Ansatz nicht weiter behandeln.

\subsection{Komplexität}

Der \emph{Sukzessionsgraph} $SG(C)$ kann aus dem \emph{Inzidenzgraphen} in $\oh(n\cdot L)$ Rechenschritten berechnet werden, weil es höchstens so viele \emph{partielle Zuweisungsspalten} gegeben kann. Außerdem ist die Anzahl der direkten Nachfolger pro Sequenz auch durch $L$ begrenzt, was eine obere Grenze für die Anzahl der Kanten liefert.

Die transitive Hülle für $G^{+}$ berechnen wir beispielsweise durch Breitensuche für jeden Knoten. Weil es $\oh(n\cdot L)$ Knoten geben kann und jeder dieser Aufrufe $\oh(n\cdot L)$ Rechenschritte benötigt, weil dies die maximale Anzahl an Kanten und Knoten ist, folgt hierfür die Laufzeit von $\oh(n^2\cdot L^2)$. Überlegungen für einen effizienteren Algorithmus haben sich als fruchtlos erwiesen. Für diesen hätten wir zuerst eine topologische Sortierung berechnet und von hinten nach vorne bearbeitet. Die transitive Hülle eines Knotens $u$ kann dabei durch Vereinigung aller Knoten $v$ mit $(u,v) \in E$ sowie deren transitive Hülle berechnet werden. Im schlimmsten Fall habe ich $\oh(L)$ Ebenen in meinem Graphen mit je bis zu $\oh(n)$ Knoten und $\oh(n)$ Kanten in andere Ebenen. Es kommt pro Ebene also zu bis zu $\oh(n)$ vielen Vereinigungen. Weil die Mengen, die miteinander vereinigt werden, aber bis zu $\oh(n\cdot L)$ Elemente enthalten (potentiell erreichbare Knoten), komme ich selbst unter Verwendung von Bit-Sets und Vereinigung über das bitweise Oder auf Kosten von $\oh(n^2\cdot L)$ pro Ebene. Auf Grund der $\oh(L)$ Ebenen bleibt es bei der Laufzeit von $\oh(n^2\cdot L^2)$, selbst wenn dieser Algorithmus in der Praxis sehr schnell sein dürfte. Weil ich im Allgemeinen nicht davon ausgehen kann, dass mein Graph signifikant viel weniger starke Zusammenhangskomponenten als Knoten insgesamt enthält, liefert auch der Algorithmus von Purdom keine bessere Laufzeit.

Für das Konstruieren der Graphen $G_{S_i}$ unserer Sequenzen $S_1, \dots, S_n$ benötigen wir jeweils lineare Zeit in der Größe des Graphen $G^{+}$, um zu überprüfen welche Knoten \emph{Stellen} der jeweiligen Sequenz enthalten und welche Kanten die Relation $\mathcal{R}_{S_i}$ erfüllen. Zwar kann es grundsätzlich $\oh(n\cdot L)$ Knoten geben und jeder Knoten bis zu $\oh(n)$ \emph{Stellen}, aber die Gesamtanzahl an \emph{Stellen} ist durch $n\cdot L$ begrenzt. Bei $n$ Sequenzen folgt die Laufzeit in $\oh(n^2\cdot L)$.

Die längsten Pfade für alle Sequenzen benötigen $n$-mal $\oh(L^2)$. Zudem müssen die nicht besuchten \emph{Stellen} für jede Sequenz aus ihren Knoten gelöscht werden. Dazu iterieren wir wieder $n$-mal über die Graphen mit bis zu $\oh(L)$ Knoten. Wie schnell es möglich ist die \emph{Stellen} aus dem dazugehörigen Knoten zu löschen, hängt von der intern verwendeten Datenstruktur ab. Im schlimmsten Fall brauchen wir $\oh(n)$ Zeit für die Entfernung einer \emph{Stelle} aus einem Knoten. Alle Löschoperationen sind dementsprechend in $\oh(n^2\cdot L)$ Rechenschritten möglich.

Insgesamt dominiert das Berechnen der transitiven Hülle die Komplexität des Algorithmus von Pitschi mit $\oh(n^2\cdot L^2)$. Weil der vorangegangene Schritt mit den \emph{minimalen Schnitten} aber ohnehin eine deutlich höhere Komplexität hat, ist es müßig hier nach effizienteren Ansätzen zu suchen. 

\section{Abschluss und Zusammenfassung}

Nach dem Algorithmus von Pitschi liegen uns \emph{konsistente Zuweisungsspalten} vor. Diese können aufgrund ihrer \emph{Konsistenz} direkt in das finale \emph{Alignment} eingefügt werden. Trotzdem müssen wir die \emph{Transitivitätsgrenzen} mit dem \emph{Alignmentgraphen} verwalten. Das liegt daran, dass wir wie bei DIALIGN zwischen den ursprünglichen \emph{Konsistenzgrenzen} auf den Teilsequenzen DIALIGN benutzen. Auch hier kann davon ausgegangen werden, dass die Anzahl der Durchläufe konstant ist. Zu guter Letzt wird die Ausgabe vorbereitet. Das Vorgehen ist an dieser Stelle identisch mit dem der ursprünglichen DIALIGN-Implementierung, das in Kapitel 3 beschrieben wurde.

Ein Nachteil des \emph{Min-Cut}-Ansatzes ist das Lösen vom streng segmentbasierten Ansatz von DIALIGN. Dieser stellt zwar nach wie vor die Basis des Verfahrens dar, aber durch die \emph{minimalen Schnitte} und den Algorithmus von Pitschi werden einzelne Verbindungen gelöscht. Möglicherweise sind unter den gelöschten Zuweisungen auch \emph{konsistente} dabei. Um sicherzustellen, dass diese Teil des \emph{Alignments} sind, wenn sie zu Unrecht gelöscht wurden, iterieren wir einmalig über alle ursprünglichen \emph{Fragmente} der paarweisen \emph{Alignments} und fügen \emph{konsistente} Zuweisungen von nicht mehr miteinander alignierten \emph{Stellen} ein, wenn diese einen positiven Ähnlichkeitswert haben. 

\subsection{Beispiel}

Nach dem Berechnen der \emph{konsistenten Zuweisungsspalten} sieht das multiple \emph{Alignment} für unser Beispiel so aus. Der Übersichtlichkeit halber wurden hier bereits Lücken eingefügt, um die Zuweisungen zu verdeutlichen. Eigentlich erfolgt dieser Schritt erst ganz am Ende des Verfahrens.

\ttfamily
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
	\item aDGTC---TCa
	\item --GTCADcTCa
	\item -taTCADgg--
	\item -DGTCADaTC-
\end{enumerate}
\normalfont

Schaut man sich die letzten \emph{Stellen} der Sequenzen 1 und 2 an, dann stellt man fest, dass deren Symbole identisch sind und sie zudem Teil des \emph{Fragments} \texttt{CTCA} am Ende der beiden Sequenzen waren. Beim linearen Durchlauf über alle \emph{Fragmente} stellen wir also fest, dass diese \emph{konsistent} sind und fügen sie in das \emph{Alignment} ein.

Versucht man jetzt DIALIGN auf den Teilsequenzen zwischen den bereits zugewiesenen \emph{Stellen} zu benutzen, stellt man fest, dass es keine weiteren \emph{Fragmente} mit positiven \emph{Überlappgewichten} gibt. Nach dem ersten \emph{DIALIGN}-Lauf wird also abgebrochen und die finale Ausgabe kann vorbereitet werden.

Das Ergebnis sieht letztendlich wie folgt aus. Auf der rechten Seite sehen Sie zum Vergleich das Ergebnis von DIALIGN 2.2 mit der gierigen Heuristik.

\ttfamily
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
	\item aDGTC---TCA \hspace{3cm} aDGTCTCA-----
	\item --GTCADcTCA \hspace{3cm} --G--TCADCTCa
	\item -taTCADgg-- \hspace{3cm} ---TATCADgg--
	\item -DGTCADaTC- \hspace{3cm} -DG--TCADATC-
\end{enumerate}
\normalfont

Wie man sieht kommt dieses \emph{Alignment} mit deutlich weniger eingefügten Lücken aus. Das liegt daran, dass nicht wenige große \emph{Fragmente} mit hohen Gewichten das ganze \emph{Alignment} dominieren können und andere sinnvolle Zuweisungen aufgrund der gierigen Heuristik verhindern.

Welches der beiden Ergebnisse besser ist, lässt sich nicht zweifelsfrei sagen, weil diese DNA-Sequenzen willkürlich gewählt wurden und es in ihnen keine biologisch bedeutenden Motive gibt. Man kann die Ergebnisse aber mathematisch über die Anzahl an identischen und abweichenden Paaren von Zuweisungen vergleichen.

Beim \emph{Min-Cut-Alignment} gibt es dabei 30 Zuweisungen identischer Symbole und keine von abweichenden. DIALIGN 2.2 liefert uns hingegen 28 übereinstimmende und 2 abweichende Paare von \emph{Stellen}. Dieses Ergebnis deutet auf jeden Fall auf die Vorteile des graphtheoretischen Ansatzes hin. 
 
\subsection{Gesamtkomplexität}

\begin{korollar}
	Mit dem \emph{Min-Cut}-Ansatz von \cite{cpm10} lässt sich ein multiples \emph{Sequenzalignment} in $\oh(n^4\cdot L^{7/2})$ Zeit berechnen.
\end{korollar}

\begin{beweis}
	DIALIGN berechnet die $\oh(n^2)$ paarweisen \emph{Alignments} in $\oh(n^2\cdot L^2)$ Rechenschritten. Die Laufzeit der Berechnung der \emph{partiellen Zuweisungsspalten} mit Hilfe des \emph{minimalen Schnitts} liegt in $\oh(n^4\cdot L^{7/2})$. Der Algorithmus von Pitschi hat eine Komplexität in $\oh(n^2\cdot L^2)$. Für den \emph{Alignmentgraphen} sind nach wie vor $\oh(n^3\cdot L^2 + n^2\cdot L^2)$ Berechnungen nötig. Es folgt, dass der \emph{minimalen Schnitte} auf dem \emph{Inzidenzgraphen} die Laufzeit dominiert, womit sich eine Gesamtkomplexität von $\oh(n^4\cdot L^{7/2})$ ergibt.
\end{beweis}

\subsection{Probleme bei der Heuristik zum Entfernen von Kanten}

Die Heuristik zum Entfernen von Kanten kann bei einigen Sequenzen zu größeren Problemen führen. Diese Heuristik funktionierte so, dass sie eine Grenze $k$ für das minimale Kantengewicht sukzessive erhöhte und alle Kanten unter $k$ entfernte, bis die Menge an Kanten ohne Zyklus war. 

In den meisten Fällen ist das kein Problem, weil man davon ausgeht, dass die wichtigen Motive, die wir finden wollen, in der gleichen Reihenfolge innerhalb der Eingabesequenzen liegen. Problematisch sind hingegen Sequenzfamilien, bei denen zwei (oder mehr)größere Abschnitte in ihrer Reihenfolge vertauscht wurden und einige Sequenzen diese in der einen, die anderen aber in den der anderen Reihenfolge haben. \improvement{Grafik einfügen.}

Sind die vertauschten Abschnitte lang genug, dann werden sie Teil der paarweisen \emph{Alignments}. Weil es sich bei ihnen um Überkreuzungen handelt, entfernt der \emph{Min-Cut}-Ansatz sie im Allgemeinen auch nicht. Kommen diese Permutationen in genug Sequenzen vor (beispielsweise allen), dann ist das Ergebnis ein \emph{Sukzessionsgraph} mit einem Zyklus, der Kanten mit sehr hohen Kantengewichten enthält. Um diese Zyklen aufzulösen, wird dann auch eine sehr hohe Grenze $k^{*}$ benötigt. Im schlimmsten Fall werden so alle Kanten im Graph gelöscht, in jedem Fall aber sehr viele, die nichts mit dem eigentlichen Zyklus zu tun haben, weil der Schritt global Kanten löscht. Somit gehen viele wichtige oder alle Verbindungen in unserem \emph{Alignment} verloren. Wir wollen am liebsten, dass die überkreuzten Stellen nicht aligniert werden, der Rest hingegen ganz normal.

Bis jetzt haben wir dieses Problem nur auf theoretischer Ebene betrachtet. Es gibt aber Sequenzfamilien, bei denen ein solches Verhalten wirklich auftritt. Ein Beispiel dafür ist die sogenannte \emph{zirkuläre Permutation} bei Proteinen. Bei diesen kommen zwar die selben Abschnitte, aber in veränderter Reihenfolge vor. Ein Beispiel sind die abstrahierten Proteinsequenzen [A,B,C,D] und [B,C,D,A]. Erstmalig wurde dieses Phänomen von \cite{chhe79} beschrieben. Wenn es im Laufe der Zeit zu einer zirkulären Permutation gekommen ist, deren Ergebnis zwei Sequenzen sind und wir eine Sequenzfamilie vorliegen haben, bei der die eine Hälfte von der einen und die andere von der anderen dieser Sequenzen abstammt, dann kann es durchaus zum oben beschriebenen Verhalten kommen.

Mit einer verbesserten Heuristik zum Entfernen von Kanten aus dem \emph{Sukzessionsgraph}, um die Zyklen aufzulösen, könnte man diesem Problem vorbeugen. Ein solcher Algorithmus wurde beispielsweise von \cite{els93} beschrieben, der in $\oh(|E|)$ Zeit einen DAG berechnet und dabei höchstens $|E|/2 - |V|/6$ Kanten entfernt. Leider löscht auch dieser im Allgemeinen sehr viele Kanten, die nichts mit dem Zyklus zu tun haben. Weil man bei nahezu allen Sequenzfamilien davon ausgehen kann, dass Permutationen kein Problem sind, halte ich die ursprüngliche Heuristik zunächst für praktikabler. Denkbar ist aber ein hybrider Ansatz, bei dem wir überprüfen wie viele Kanten $E_{k^{*}}$ im Verhältnis zur alten Kantenanzahl enthält. Ist dieses Verhältnis zu gering (beispielsweise kleiner als 3/4), benutzen wir stattdessen den Algorithmus von Eades, Lin und Smyth auf den ursprünglichen Kanten unseres \emph{Sukzessionsgraphen}.

\subsection{Evaluierung}

\cite{cpm10} haben ihre Implementierung des Verfahrens einer Vielzahl an Tests unterzogen, um sie mit einer breiten Auswahl an etablierten Verfahren zu vergleichen. Die Ergebnisse wurden dabei auf drei Referenzdatenbanken getestet, die jeweils eine Menge von \emph{alignierten} Sequenzfamilien zur Verfügung stellen für die eine biologisch korrekte Zuweisung bekannt ist. Zum Test auf globale verwandten Sequenzen wurde die Referenzdatenbank BAliBASE benutzt. Diese enthält \emph{Alignments} von Proteinsequenzen, die auf der ganzen Länge der Sequenzen Ähnlichkeiten haben. Lokal verwandte Sequenzen haben hingegen nur an einzelnen Stellen Ähnlichkeiten und zwischen diesen verwandten Segmenten sind nicht in Beziehung zueinander stehende Symbole. Für diese Art von Sequenzen wurden Tests auf den Datenbanken IRMBASE (Proteine) und DIRMBASE (DNA) durchgeführt.

Für mehrere Sequenzfamilien dieser Datenbanken wurden dann mit dem \emph{Min-Cut-Ansatz} und durch andere \emph{Alignmentprogramme}, wie DIALIGN 2.2 und DIALIGN TX (neueste Version von DIALIGN), T-COFFEE, MAFFT, \emph{Alignments} berechnet und miteinander verglichen. Für die Vergleiche dienten wieder der \emph{sum-of-pairs-Score} (SP) und der \emph{(total-)column-Score} (TC), die die Anzahl an korrekten Paaren beziehungsweise komplett korrekten Spalten im Vergleich zum \emph{Referenzalignment} angeben.

\subsubsection{Lokale Alignments auf IRMBASE2 und DIRMBASE1}

Die Sequenzen der Referenzdatenbanken IRMBASE2 und DIRMBASE1 wurden so konstruiert, dass in zufällig erstellte und nicht zueinander in Beziehung stehende Sequenzen lokal einzelne Motive hinzugefügt wurden.

Es hat sich herausgestellt, dass die neueste Version von DIALIGN TX nach wie vor die besten Ergebnisse auf lokal verwandten Sequenzen liefert, gefolgt von DIALIGN 2.2. Der \emph{Min-Cut}-Ansatz hatte auf den Proteinsequenzen von DIRMBASE starke Ergebnisse ergeben, die mit denen der beiden DIALIGN-Varianten vergleichbar sind. Auf DNA-Sequenzen waren die Ergebnisse nach wie vor ordentlich, aber signifikant schlechter im Vergleich zu den beiden gierigen Verfahren. Im Vergleich zu den globalen \emph{Alignierern}, wie CLUSTALW 2.0 oder T-COFFEE schnitt das \emph{Min-Cut}-Verfahren aber sehr gut ab \citep{cpm10}. Die einzige Ausnahme ist das Programm MAFFT, das Sequenzen mit Hilfe der schnellen Fouriertransformation gruppiert und sie dann progressiv \emph{aligniert}. MAFFT lieferte auf Protein- und DNA-Sequenzen gute Ergebnisse, die beinahe an die der DIALIGN-Varianten herankommen.

\subsubsection{BALiBASE3}

Ein Problem bei den Tests war aber die Laufzeit auf einigen Sequenzfamilien. Wie bereits in Abschnitt \ref{ch:ra_kompl} über die Komplexität von \textrm{ResolveAmbiguities} beschrieben, war die Laufzeit auf einigen Sequenzfamilien so lang, dass es nicht möglich war in angemessener Zeit \emph{Alignments} zu berechnen. Deshalb sah man sich dazu gezwungen zusätzlich eine Grenze $T$ für das minimale Gewicht eines \emph{Fragments} einzuführen, wodurch die Anzahl der Kanten im \emph{Inzidenzgraphen} verringert wird. Diese Grenze $T$ sorgt im Gegenzug zur verbesserten Laufzeit aber auch für etwas schlechtere Ergebnisse \citep{cpm10}.

Insgesamt stellt der neue Ansatz auf global verwandten \emph{Sequenzfamilien} eine deutliche Verbesserung zu \emph{DIALIGN 2.2} dar. So waren bei allen sechs getesteten Referenzmengen, die jeweils eine Vielzahl an Sequenzfamilien enthalten, die Ergebnisse des \emph{min-cut}-Verfahrens besser. Das gilt sowohl für die SP-, als auch für die TC-\emph{Scores}. Mit den besten globalen \emph{Alignierer} kann das Verfahren zwar nicht konkurrieren, der verbreitete progressive Algorithmus CLUSTALW 2.0 wurde aber in allen Tests geschlagen. 

\subsubsection{Betrachtung der Scores im Vergleich}

Um festzustellen wie ihr Ansatz numerisch abschneidet, haben \cite{cpm10} die \emph{Scores} der Gewichtsfunktionen von den Ergebnissen ihres Algorithmus mit denen von DIALIGN auf den sechs getesteten \emph{Referenzfamilien} von BAliBASE verglichen. Man hat festgestellt, dass die \emph{Scores} des \emph{min-cut}-Algorithmus auf vier der sechs Referenzfamilien geringer sind, als mit DIALIGN 2.2. Weil gleichzeitig die \emph{TC-} und \emph{SP-Scores} ihres Algorithmus besser waren, werten die Autoren dies als Indiz dafür, dass die Gütefunktion letztendlich nur eingeschränkt funktioniert. Wenn diese ein schlechtes Maß für die Güte eines \emph{Alignments} ist und gleichzeitig mit Hilfe von Heuristiken versucht wird die Gütefunktion zu maximieren, dann ist es logisch, dass bessere Heuristiken mit höheren \emph{Scores} nicht automatisch auch biologisch bessere Ergebnisse liefern. \cite{cpm10} folgern aufgrund der Ergebnisse mit biologisch besseren \emph{Alignments} bei gleichzeitig numerisch schlechteren Ergebnissen in der Gütefunktion, dass unter Weiterverwendung dieser keine größeren Verbesserungen der DIALIGN-Verfahren mehr möglich sind. Mögliche Auswege daraus werden im Abschnitt Future Works \ref{sec:fut_work} des letzten Kapitels beschrieben.

Leider gehen die Autoren nicht darauf ein wie für die \emph{Alignments} des \emph{min-cut}-Ansatzes nachträglich die numerischen \emph{Scores} berechnet wurden. Durch die Berechnung der \emph{minimalen Schnitte} und den Algorithmus von Pitschi werden mitunter auch einzelne \emph{Stellen} aus den Zuweisungen gelöscht, sodass die Ergebnisse nicht immer längere, zusammenhängende \emph{Fragmente} sind. Interpretiert man einfach ohne Lücke aufeinanderfolgenden, einander zugewiesene \emph{Stellen} als \emph{Fragmente}, dann erscheint es logisch, dass die \emph{Scores} geringer sind. Das liegt an der Gewichtsfunktion von DIALIGN, die bei jedem \emph{Fragment} einen Korrekturterm $K$ abzieht, der von der Länge der Sequenzen abhängt. Viele kurze \emph{Fragmente}, wie sie beim \emph{min-cut}-Algorithmus auftreten, haben im Allgemeinen immer ein geringeres Gesamtgewicht als weniger kurze mit der selben Gesamtlänge. Im Kontext dieser Erkenntnis stimme ich mit der Schlussfolgerung von Corel also nicht uneingeschränkt überein, solange nicht klar ist wie die \emph{Scores} genau berechnet wurden.