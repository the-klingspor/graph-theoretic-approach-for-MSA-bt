\chapter{Ein Min-Cut-Ansatz für das Konsistenzproblem}
\label{ch:min-cut}
Da es, wie im letzten Kapitel gezeigt, auf manchen Sequenzfamilien durch die gierige Heuristik von DIALIGN zu suboptimalen Scores und Alignments kommt, werden wir jetzt einen verbesserten graphtheoretischen Ansatz von \cite{cpm10} betrachten.

Dazu benötigen wir zwei verschiedene Graphen: zum einen den \emph{Inzidenzgraphen}, bei dem alle Stellen Knoten sind und ihre Anker Zusammenhangskomponenten. Der zweite ist der \emph{Sukzessionsgraph}, der die Zusammenhangskomponenten unseres Inzidenzgraphen als Knoten und die natürliche Ordnung auf den Sequenzen als Kanten benutzt. Man kann sich vorstellen, dass genau dann eine Kante zwischen zwei  Diese beiden Datenstrukturen werden wir benutzen, um Inkonsistenzen aufzulösen. Wenn wir uns an die Definition von Konsistenz aus dem letzten Kapitel erinnern, dann stellen wir fest, dass es zwei Arten von ihr gibt: zum einen implizite, transitive Mehrfachzuweisungen bei denen einer Stelle einer Sequenz mehrere Stellen einer anderen Sequenz zugeordnet sind und zum anderen überkreuzte Zuweisungen.

Als Ausgangspunkt starten wir wieder mit unseren paarweisen Alignments aus DIALIGN. Überlappgewichte brauchen wir in unserem Fall nicht. Dann konstruieren wir mit Hilfe dieser Zuweisungen unseren Inzidenzgraphen und benutzen einen Algorithmus zur Berechnung des minimalen Schnitts (\enquote{min-cut}) auf den Zusammenhangskomponenten, um alle Inkonsistenzen aufgrund von transitiven Mehrfachzuweisungen aufzulösen. Die so entstehenden Zusammenhangskomponenten benutzen wir, um einen Sukzessionsgraphen aufzubauen. Dank eines Algorithmus von \cite{pdc10} können wir mit diesem Überkreuzungen aus unserer Relation löschen. Alle dieser Konzepte werden wir im Laufe dieses Kapitel formal definieren, genauer analysieren und die Korrektheit der Aussagen beweisen.
 
\section{Flussnetzwerke}

\subsection{Einführung}

Stell dir eine Produktionsstätte vor, wo ein bestimmtes Produkt hergestellt wird und eine Verwendungsstätte bei der das Produkt benötigt wird. Zwischen dem Ort der Produktion und dem Ort der Verwendung gibt es ein Schienennetz über das die Produkte geliefert werden können. Über jeden Abschnitt der Schienen kann aber nur eine bestimmte Anzahl an Waggons geleitet werden. So mag es weniger stark befestigte Strecken geben, wo nur kleinere oder leichtere Züge fahren können und andere gut ausgebaute mit mehreren Gleisen nebeneinander. Die Anzahl an Einheiten, die über einen Abschnitt geleitet werden können, nennt man Kapazität, während Produktionsstätte und Zielort Quelle und Senke heißen. Ziel ist es zu bestimmen wie viele Produktionseinheiten über dieses Netz von der Fabrik zum Verbraucher geliefert werden können.

Neben diesem Problem kann man mit Flussnetzwerken noch viele andere Anwendungen modellieren, zum Beispiel eine chemische Produktionsstraße mit vielen Rohren, die jeweils nur den Durchfluss einer bestimmten Menge erlauben. Oder Stromnetze mit Kraftwerken und Endverbrauchern zwischen denen es Stromleitungen gibt, die jeweils höchstens einen bestimmten Stromfluss erlauben. Für eine Hochspannungsleitung mag dieser sehr hoch und für die Leitungen, die direkt zu den Häusern gehen, sehr klein sein. Stromnetze bieten uns auch eine direkte Analogie, wie mit den Knotenpunkten zwischen den Verbindungen umzugehen ist. An jedem Knoten, außer der Quelle und Senke, wird genauso viel Fluss hinein- wie wieder hinausgeleitet. Es wird nichts gespeichert oder geht verloren. Dieses Konzept, das man Flusserhaltung nennt, funktioniert genau wie das erste \emph{Kirchhoffsche Gesetz} bei Strömen \cite[S.708]{clrs09}. 

Ursprünglich stammt das Konzept des Flussnetzwerkes aus dem Kalten Krieg und der militärischen Forschung. Es wurde das Schienennetzwerk des Sowjetunion in Osteuropa untersucht, um herauszufinden wie viel Material die UdSSR im Kriegsfall aus dem russischen Kerngebiet nach Mitteleuropa transportieren könnte und welche Strecken man zerstören müsste, um den Nachschub am schnellsten Abzuschneiden. Die Untersuchung bietet uns sogleich einen intuitiven Begriff des \emph{maximalen Flusses} und des \emph{minimalen Schnitts}. Der \emph{maximale Fluss} ist die maximale Anzahl an Einheiten, die von der Quelle zur Senke transportiert werden kann, während der \emph{minimale Schnitt} den \enquote{Bottleneck} des Netzwerkes darstellt, der den Fluss von der Quelle zur Senke minimiert. Wie wir später formal zeigen werden sind diese beiden Werte in einem Flussnetzwerk äquivalent.

Zunächst betrachten wir Flussnetzwerke auf einer formalen Ebene, dann widmen wir uns kurz und skizziert einigen der wichtigsten Algorithmen zum berechnen von maximalen Flüssen und danach beweisen wir die Äquivalenz von diesen mit minimalen Schnitten. Diese minimalen Schnitte brauchen wir später zum Auflösen von Inkonsistenzen im Inzidenzgraphen.

\begin{definition}[Flussnetzwerk]
	Ein Flussnetzwerk ist ein gerichteter Graph $G = (V,E)$ bei dem jeder Kante $(u,v) \in E$ eine nicht-negative Kapazität $c(u,v) \geq 0$ zugeordnet ist und bei dem es zwei ausgezeichnete Knoten $s, t \in V$ gibt, die wir Quelle und Senke nennen.  
\end{definition}

Der Einfachheit halber nehmen wir im Folgenden an, dass jeder Knoten von $G$ auf einem Pfad von $s$ nach $t$ liegt. Sollte es doch solche Knoten geben, dann wären sie ohnehin nicht relevant, weil kein Fluss durch sie von der Quelle zur Senke geschickt werden kann.

\begin{definition}[Fluss]
	Sei $G = (V,E)$ mit Kapazitätsfunktion $c$ ein Flussnetzwerk. Dann ist definieren wir einen Fluss als eine Funktion $f: V \times V \rightarrow \mathbb{R}$, die die folgenden beiden Eigenschaften erfüllt:
	\begin{itemize}[leftmargin=12em]
		\item[\textbf{Kapazitätsbeschränkung:}] Für alle Knoten $u,v \in V$ gelte $0 \leq f(u,v) \leq c(u,v)$.
		\item[\textbf{Flusserhaltung:}] Für alle Knoten $u \in V\setminus\{s,t\}$ gelte \\ 
			$\sum_{v \in V}{f(v,u)} = \sum_{v \in V}{f(u,v)}$.
	\end{itemize} 
	Für Knoten $u,v\in V$, die nicht durch eine Kante verbunden sind ($(u,v)\notin E$), setzen wir den Fluss auf 0: $f(u,v) = 0$.
\end{definition}

Ein Fluss ist also eine Zuordnung von Werten an die Kanten unseres Flussnetzwerkes, sodass keine Kapazität verletzt wird und in jeden Knoten soviel hinein wie hinaus fließt. Wir nennen auch den Wert $f(u,v)$ Fluss zwischen den beiden Knoten $u$ und $v$.


Hier ein Beispiel für ein einfaches Flussnetzwerk mit sechs Knoten:
\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill=gray,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=20pt,
		font=\footnotesize},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 1.2cm
	]
	\node[mycircle] (c1) {$s$};
	\node[mycircle,below right=of c1] (c2) {$v_2$};
	\node[mycircle,right=of c2] (c3) {$v_4$};
	\node[mycircle,above right=of c1] (c4) {$v_1$};
	\node[mycircle,right=of c4] (c5) {$v_3$};
	\node[mycircle,below right=of c5] (c6) {$t$};
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c2/8/below,
		c1/c4/11/above,
		c2/c3/11/below,
		c3/c6/4/below,
		c4/c5/12/above,
		c5/c6/15/above,
		c5/c2/4/below,
		c3/c5/7/below,
		c2.70/c4.290/1/below}
	\draw [myarrow] (\i) -- node[sloped,font=\footnotesize,\p] {\txt} (\j);
	
	
	% draw this outside loop to get proper orientation of 10
	\draw [myarrow] (c4.250) -- node[sloped,font=\small,above,rotate=180] {10} (c2.110);
	\end{tikzpicture}
\end{center}

Viele Autoren setzen voraus, dass es keine Kanten in die Senke und keine aus der Quelle gibt. Diese Beschränkung ist zwar hilfreich fürs Verständnis, wird aber grundsätzlich nicht benötigt. Man kann sich leicht überlegen, dass selbst wenn es solche Kanten gibt, ihr Fluss 0 sein muss. Bei den Flussnetzwerken, die wir für unseren Algorithmus benutzen, wird es diese Kanten geben, weshalb wir auf die Beschränkung verzichten. 

\begin{definition}[Wert eines Flusses]
	Der Wert eines Flusses ist definiert als
	\begin{equation}
		|f| = \sum_{v \in V}{f(s,v)} - \sum_{v \in V}{f(v,s)}.
	\end{equation} 
\end{definition}

Eine andere Einschränkung, die man in vielen Definitionen sieht, ist der Verzicht auf antiparallele Kanten. Das heißt, dass es gleichzeitig Kanten $(u,v), (v,u) \in E$ gibt. Manche Implementierungen für Algorithmen, die den maximalen Fluss berechnen, sind so programmiert, dass sie vom Benutzer eine Menge von Gegenkanten mit Fluss 0 erwarten, die intern benötigt werden. Grundsätzlich sind antiparallele Kanten aber unproblematisch, weil man jeden Fluss mit positiven Werten auf antiparallelen Kanten in einen Fluss umwandeln kann, bei dem zwischen zwei Knoten höchstens eine Kante einen positiven Fluss hat. Sei beispielsweise $f(u,v) = x, f(v,u) = y$ mit $x \not 0 \not y$. Dann kann man einen Fluss mit dem selben Wert definieren, wenn man den Fluss auf den antiparallelen Kanten wie folgt definiert: $f(u,v) = x - \min\{x,y\}, f(v,u) = y - \min\{x,y\}$. Alternativ lässt sich jedes Flussnetzwerk mit antiparallelen Kanten auch in ein äquivalentes übertragen, das keine solchen Kanten enthält.

\begin{center}
\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill=gray,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=20pt,
		font=\footnotesize},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 1.2cm
	]
	\node[mycircle] at (0,0) (c1) {$u$};
	\node[mycircle] at (0,-2) (c2) {$v$};
	
	\node[mycircle] at (5,0) (c3) {$u$};
	\node[mycircle] at (3,-1) (c4) {$c$};
	\node[mycircle] at (7,-1) (c5) {$c'$};
	\node[mycircle] at (5,-2) (c6) {$v$};

	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1.255/c2.105/2/below,
		c2.75/c1.285/5/below,
		c3/c4/2/below,
		c4/c6/2/below,
		c6/c5/5/below,
		c5/c3/5/below}
	\draw [myarrow] (\i) -- node[sloped,font=\footnotesize,\p] {\txt} (\j);
\end{tikzpicture}
\end{center}

\subsection{Wichtige Algorithmen}

Es gibt eine Vielzahl von Algorithmen zur Berechnung eines maximalen Flusses auf einem Flussnetzwerk. Die wichtigsten und bekanntesten Vertreter möchte ich kurz vorstellen. 

Der erste Algorithmus, der speziell zum Berechnen des maximalen Flusses entwickelt wurde war der Algorithmus von Ford-Fulkerson \cite{gt14}. Dieser verwendete einen sogenannten \emph{Restwertgraphen}, der für jede Kante im ursprünglichen Flussnetzwerk eine Vorwärts- und eine Rückwärtskante enthält. Die Vorwärtskante hat als Kapazität die der Kante im ursprünglichen Graph minus den Fluss im Flussnetzwerk. Die der Rückwärtskante im Restwertgraphen ist genau die Kapazität des Flusses der zugehörigen Kante im Flussnetzwerk. Für einen Pfad von der Quelle zur Senke im Restwertgraph kann man den Fluss im Flussnetzwerk um die kleinste Kapazität auf diesem Pfad erhöhen, ohne die Kapazitätsbeschränkung zu verletzen. Diese Pfade nennt man \emph{augmentierend} und nach jeder Aktualisierung des Flusses, müssen auch die Kapazitäten im Restwertgraphen angepasst werden. Der Algorithmus von Ford-Fulkerson besucht solange augmentierende Pfade, bis es keine solchen mehr gibt. Ist das der Fall, dann kann der Fluss nicht mehr vergrößert werden und der maximale Fluss wurde berechnet. Der Algorithmus von Ford-Fulkerson hat für einen zusammenhängenden Graphen mit ganzzahligen Kapazitäten eine pseudopolynomielle Laufzeit von $\oh(|E|\cdot U)$, wobei $U$ die Summe der Kapazitäten der ausgehenden Kanten von der Quelle ist.

Oft wird Ford-Fulkerson eher als eine \enquote{Methode} statt als Algorithmus bezeichnet, weil die Reihenfolge, nach der augmentierende Pfade gewählt werden, nicht spezifiziert ist. In der Folge wurde der Algorithmus von Ford-Fulkerson an mehreren Stellen verbessert. Edmond-Karps benutzt eine Breitensuche, um den Pfad mit den wenigsten Knoten von der Quelle zur Senke zu finden. Dadurch lässt sich die Laufzeit auf $\oh(|V|\cdot |E|^2)$ verbessern \cite[S.727ff.]{clrs09}. Auch der Algorithmus von Dinic \footnote{Eigentlich entwickelt vom sowjetischen Forscher Yefim A. Dinitz. Die verbreitete Schreibweise beruht auf einem Übersetzungsfehler. Später wanderte Dinitz nach Israel aus \cite{d06}.} sucht nach kürzesten Pfaden im Restwertgraphen. Zusätzlich wird das Konzept von sogenannten \emph{blockierenden Flüssen} benutzt, bei denen jeder $s-t$-Pfad mindestens eine Kante enthält, deren Kapazität komplett ausgereizt ist. Der Algorithmus von Dinic benötigt nur $\oh(|V|^2\cdot |E|)$ Rechenschritte \cite{d06}.

Die nächste Klasse von Algorithmen waren die sogenannten \emph{Push-Relabel-Algorithmen}. Diese benutzen bei ihren Schritten statt Flüssen nur sogenannten \emph{Präflüsse} bei denen zwar die Kapazitätsbeschränkung gilt, die Flusserhaltung aber nicht. Das heißt, dass jeder Knoten einen positiven \emph{Überfluss} haben kann, wenn mehr in ihn hinein als hinaus fließt. Jeder Knoten hat einen Index, der seine Höhe im Netz angibt, wobei Fluss immer nur von oben nach unten geleitet werden kann. Wie der Name schon sagt, sind die zwei Grundschritte bei allen Push-Relabel-Algorithmen die Methoden Push und Relabel. Bei Push verschieben wir den \emph{Überfluss} eines Knotens zu einem tiefer gelegenen Nachbarn. Bei Relabel wird hingegen die Höhe eines Knotens vergrößert, wenn er noch Überfluss hat, aber alle benachbarten Knoten einen größeren Index haben. Die Push- und Relabelmethoden werden solange angewendet, bis es keine Knoten mehr gibt auf die sie anwendbar sind \cite[S.736ff.]{clrs09}. Je nachdem wie der nächste zu bearbeitende Knoten ausgewählt wird, kann die Laufzeit stark variieren. Ein generischer Algorithmus, der die Knoten mehr oder minder zufällig auswählt, läuft in $\oh(|V|^2\cdot |E|)$. Mit einer FIFO-Warteschlange in dem jeder bearbeitete Knoten, der noch Überfluss hat, wieder ans Ende eingereiht wird, lässt sich die Laufzeit auf $\oh(|V|^3)$ verbessern \cite{gt88}. Der in der Praxis verbreitetste Ansatz benutzt die \emph{Highest-Label}-Regel, bei der alle Knoten in Töpfen nach ihrem Index eingeordnet sind und jeweils ein Knoten mit maximalem Index ausgewählt wird. Hier verbessert sich die Laufzeit auf $\oh(|V|^2\cdot \sqrt{|E|})$ Rechenschritte im schlimmsten Fall \cite{akmo97}.

Weiter verbessern ließ sich die asymptotische Laufzeit durch die Verwendung von \emph{dynamischen Bäumen. Dynamische Bäume sind eine Datenstruktur, die die nicht-saturierten Teile von augmentierenden Pfaden speichern. Mit Operationen auf dieser Datenstruktur ist es möglich die Laufzeit zu senken. Beschränkt man beispielsweise die maximale Baumgröße und benutzt eine weitere Datenstruktur, dann lässt sich der maximale Fluss mit Hilfe von blockierenden Flüssen in $\oh(|V|\cdot |E|\cdot \log(|V|^2/|E|))$ berechnen \cite{gt14}. In der Praxis sind diese Algorithmen aber nicht von Relevanz, weil die verwendete Baumstruktur einen großen konstanten Vorfaktor benötigt. Einer der schnellsten bekannten Algorithmen ist der von Orlin. Er verwendet eine Kombination verschiedener Techniken in unterschiedlichen Situationen und kommt insgesamt auf eine Laufzeit von $\oh(|V|\cdot |E|)$ \cite{gt14}.
 
\subsection{Schnitte und der \emph{Max-Flow-Min-Cut-Satz}}

\begin{definition}[Schnitt]
	Sei $G = (V,E)$ mit Senke und Quelle $s, t \in V$ und einer Kapazitätsfunktion $c$. Dann ist ein Schnitt von $G$ eine Partitionierung von $V$ in zwei Mengen $A$ und $B$, sodass $s \in A$ und $t \in B$ gelten.
	
	Die Kapazität $c_{A,B}$ dieses Schnitts ist definiert als die Summe der Kapazitäten aller Kanten von $A$ nach $B$:
	\begin{equation}
		c_{A,B} \coloneqq \sum_{(v,w) \in E \cap (A \times B)}{c(v,w)}
	\end{equation}
\end{definition}

Betrachten wir ein Beispiel für einen Schnitt auf einem Flussnetzwerk. Alle Knoten in $A$ wurden grün und alle in $B$ rot markiert. Kanten zwischen den beiden Mengen wurden gepunktet dargestellt.

\vspace{-8pt}
\begin{center}
	\begin{tikzpicture}[
	mygreennode/.style={
		circle,
		draw=black,
		fill=green,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=20pt,
		font=\footnotesize},
	myrednode/.style={
		circle,
		draw=black,
		fill=red,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=20pt,
		font=\footnotesize},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 1.2cm
	]
	\node[mygreennode] (c1) {$s$};
	\node[myrednode,below right=of c1] (c2) {$v_2$};
	\node[mygreennode,right=of c2] (c3) {$v_4$};
	\node[mygreennode,above right=of c1] (c4) {$v_1$};
	\node[mygreennode,right=of c4] (c5) {$v_3$};
	\node[myrednode,below right=of c5] (c6) {$t$};
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c4/11/above,
		c4/c5/12/above,
		c5/c6/15/above,
		c3/c5/7/below}
	\draw [myarrow] (\i) -- node[sloped,font=\footnotesize,\p] {\txt} (\j);
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c2/8/below,
		c2/c3/11/below,
		c3/c6/4/below,
		c5/c2/4/below,
		c2.70/c4.290/1/below}
	\draw [myarrow, dotted] (\i) -- node[sloped,font=\footnotesize,\p] {\txt} (\j);
	
	% draw this outside loop to get proper orientation of 10
	\draw [myarrow, dotted] (c4.250) -- node[sloped,font=\small,above,rotate=180] {10} (c2.110);
	\end{tikzpicture}
\end{center}
\vspace{-8pt}

Es gilt $A = \{s, v_1, v_3, v_4\}$ und $B = \{v_2, t\}$. Zur Kapazität des Schnitts tragen alle Kanten bei, die von Knoten aus $A$ zu solchen aus $B$ verlaufen. Die Kante $(v_3,v_2)$ also schon, die Kante $(v_2, v_4)$ aber nicht. Insgesamt beträgt die Kapazität des Schnitts $c(s,v_2) + c(v_1, v_2) + c(v_3,v_2) + c(v_3,t) + c(v_4,t) = 8 + 10 + 4 + 15 + 4 = 41$.  

\begin{definition}[Minimaler Schnitt]
	Als \emph{minimalen Schnitt} bezeichnet man den oder einen Schnitt mit minimaler Kapazität. Es kann mehrere solcher \emph{minimaler Schnitte} geben. 
\end{definition}

Dem geneigten Leser mag die Asymmetrie zwischen Flüssen, die maximal sein können und Schnitten, die minimal sein können, aufgefallen sein. Den Zusammenhang der beiden Konzepte möchte ich im Folgenden kurz beschreiben. Dazu werden wir zwei formale Aussagen betrachten, die hier aber nicht bewiesen werden. Diese sind zwar für unseren Algorithmus von Bedeutung, aber für einen formalen Beweis bräuchten wir einen größeren theoretischen Hintergrund, der obgleich furchtbar spannend, hier den Rahmen sprengte und wenig zielführend wäre.

\begin{lemma}
	Der Wert eines beliebigen Flusses über einem Flussnetzwerk ist beschränkt durch einen beliebigen Schnitt.
\end{lemma}

\begin{beweis}
	Siehe \cite[S.723]{clrs09}.
\end{beweis}

\begin{satz}[Max-Flow-Min-Cut-Satz]
	Für ein Flussnetzwerk $G = (V,E)$ mit Senke und Quelle $s, t \in V$ entspricht die minimale Kapazität aller Schnitte auf $G$ dem maximalen Wert aller Flüsse von $s$ nach $t$. 
\end{satz} 

\begin{beweis}
	Wenn wir an den Algorithmus von Ford-Fulkerson denken, dann erinnern wir uns, dass dieser solange augmentierende Pfade mit positiver Restwertkapazität von der Quelle zur Senke gesucht hat, bis es keinen solchen mehr gab. Sobald das der Fall war, hatte man den maximalen Fluss berechnet. Das bedeutet gleichzeitig, dass man den Bottleneck zwischen $s$ und $t$ gefunden hat. Genau dieser trennt die Knoten von $G$ in zwei Hälften und ist unser minimaler Schnitt. Für einen formalen Beweis neben dieser Skizze siehe auch hier \cite[S.723f.]{clrs09}.
\end{beweis} 

Die Erkenntnis, die man aus diesem Satz zieht, ist, dass man Algorithmen zur Berechnung des maximalen Flusses benutzen kann, um den minimalen Schnitt eines Flussnetzwerkes zu bestimmen. Das werden wir im nächsten Abschnitt ausnutzen, um Inkonsistenzen zwischen unseren Sequenzen aufzulösen.

\section{Inzidenzgraphen und das Auflösen von Inkonsistenzen mit Hilfe von Flussnetzwerken}

\subsection{Konstruieren des Inzidenzgraphen}

Wir starten wie bei DIALIGN	zunächst mit paarweisen Alignments. Unser Ziel ist es Übereinstimmungen zu finden, die in möglichst vielen Sequenzen gleichzeitig vorkommen. Das ist ein vielversprechender Ansatz, weil die gesuchten Motive oft in vielen sich überlappenden Fragmente vorkommen, während Überlappungen bei zufälligen Übereinstimmungen unwahrscheinlich sind. Dazu konstruieren wir einen sogenannten Inzidenzgraphen, der alle Stellen als Knoten enthält, die durch Kanten verbunden sind, falls es ein sie verbindendes Fragment gibt. Mit Hilfe eines Algorithmus zur Berechnung des maximalen Flusses bestimmen wir minimale Schnitte, bis nur noch dichte Zusammenhangskomponenten übrig bleiben, die jeweils höchstens einen Knoten aus jeder Sequenz enthalten \cite{cpm10}.

\begin{definition}[Mehrdeutigkeit und partielle Zuweisungsspalten]
	Es sei eine Menge an Sequenzen $S$ mit Stellenraum $\mathcal{S}$ gegeben. Eine Teilmenge $C \subset \mathcal{S}$ nennen wir \emph{mehrdeutig}, wenn es mindestens eine Sequenz $S_i$ gibt, sodass $C \cap S_i$ zwei oder mehr Stellen $(i,p), (i,p') \in \mathcal{S}$ enthält. In diesem Fall nennen wir auch $(i,p)$ und $(i,p')$ mehrdeutig. Analog nennen wir eine Äquivalenzrelation $\mathcal{R}$ mehrdeutig, wenn $\mathcal{R}$ eine mehrdeutige Äquivalenzklasse enthält.
	
	Eine \emph{nicht-mehrdeutige} Teilmenge $C \subset \mathcal{S}$ bezeichnen wir als \emph{partielle Zuweisungsspalte}.
\end{definition}

Es lässt sich folgern, dass eine konsistente Äquivalenzrelation auch nicht mehrdeutig ist, während das Gegenteil im Allgemeinen nicht gilt. Das liegt daran, dass es überkreuzte Zuweisungen geben kann, die aber partielle Zuweisungsspalten sind. Nicht-mehrdeutige Äquivalenzrelationen bestehen nur aus partiellen Zuweisungsspalten.

Im Folgenden sei eine Menge von Fragmenten $\mathcal{F} = \{f_1, \dots, f_k\}$ gegeben. Normalerweise werden dies die Fragmente unserer paarweisen Alignments sein, aber theoretisch kann man auch anders bestimmte benutzen. Wir wollen möglichst wenige Verbindungen aus der durch die Fragmente induzierte Relation $\mathcal{R}$ löschen, bis eine nicht-mehrdeutige Äquivalenzrelation $\mathcal{R'}$ bleibt.

\begin{definition}[Inzidenzgraph]
	Es sei ein Stellenraum $\mathcal{S}$ mit einer Menge von Fragmenten $\mathcal{F}$ auf diesem gegeben. Dann bezeichnen wir den ungerichteten Graphen $G_{\mathcal{F}} = (\mathcal{S},E_{\mathcal{F}})$ als Inzidenzgraphen. In diesem Graph existiert genau dann eine Kante $(u,v) \in E_{\mathcal{F}}$, wenn die Stellen $u$ und $v$ in einem gemeinsamen Fragment $f_i \in \mathcal{F}$ vorkommen.
\end{definition} 

Wie man sieht, sind die Zusammenhangskomponenten unseres Inzidenzgraphen genau die Äquivalenzklassen der durch $\mathcal{F}$ induzierten Kanten. Weil diese nicht weiter nützlich sind, kann man Stellen, die nicht mit anderen verbunden sind, von vornherein ignorieren beziehungsweise sie löschen, wenn sie nach dem Entfernen von Kanten Grad 0 haben. So lässt sich etwas Speicherplatz sparen und die Ergebnisse dieses Algorithmus können direkt für den nächsten Schritt weiterverwendet werden. Um die Positionierung der Zusammenhangskomponenten im Inzidenzgraphen zu verdeutlichen, lassen wir sie in unseren graphischen Beispiele aber stehen.

\subsection{Beispiel Inzidenzgraph}

Auch hier werden wir wieder unser Beispiel aus dem letzten Kapitel benutzen. Erinnern wir uns zunächst an die Fragmente der paarweisen Alignments. Die Überlappgewichte brauchen wir für diesen Schritt des Algorithmus nicht, weil wir versuchen mit Hilfe von minimalen Schnitten ein ähnliches aber besseres Ergebnis zu erreichen.

\begin{tabular}{r|c|c||r|c|c||r|c|c}
	Seq. & Frag. & Ü-Gew. & Seq. & Frag. & Ü-Gew. & Seq. & Frag. & Ü-Gew.\\
	\hline
	2 & \texttt{GTCADCTC} & \multirow{2}{*}{16} & 1 & \texttt{TCTCA} & \multirow{2}{*}{7} & 1 & \texttt{GT} &\multirow{2}{*}{2} \\
	4 & \texttt{GTCADATC} &                     & 3 & \texttt{TATCA} &                     & 2 & \texttt{GT} & \\
	3 & \texttt{TCAD} & \multirow{2}{*}{8} & 1 & \texttt{CTCA} & \multirow{2}{*}{8} & 1 & \texttt{TC} & \multirow{2}{*}{2} \\
	4 & \texttt{TCAD} &                     & 2 & \texttt{CTCA} &                          & 4 & \texttt{TC} & \\
	2 & \texttt{TCAD} & \multirow{2}{*}{8} & 1 & \texttt{DGTC} & \multirow{2}{*}{8} &    &   & \\
	3 & \texttt{TCAD} &                     & 4 & \texttt{DGTC} &                     &    &   & \\
\end{tabular}

Diese Fragmente überführen wir direkt in einen Inzidenzgraphen bei denen die Stellen als Knoten und die Verbindungen in gemeinsamen Fragmenten als Kanten übertragen werden. Um die Übersichtlichkeit halbwegs zu wahren, wurden die Kanten jeder Zusammenhangskomponente jeweils in einer Farbe markiert.

\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill=gray,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 1.1cm
	]
	% erste Sequenz
	\node[mycircle] (c11) {A};
	\node[mycircle,right=of c11] (c12) {D};
	\node[mycircle,right=of c12] (c13) {G};
	\node[mycircle,right=of c13] (c14) {T};
	\node[mycircle,right=of c14] (c15) {C};
	\node[mycircle,right=of c15] (c16) {T};
	\node[mycircle,right=of c16] (c17) {C};
	\node[mycircle,right=of c17] (c18) {A};
	
	% zweite Sequenz
	\node[mycircle,below=of c11] (c21) {G};
	\node[mycircle,right=of c21] (c22) {T};
	\node[mycircle,right=of c22] (c23) {C};
	\node[mycircle,right=of c23] (c24) {A};
	\node[mycircle,right=of c24] (c25) {D};
	\node[mycircle,right=of c25] (c26) {C};
	\node[mycircle,right=of c26] (c27) {T};
	\node[mycircle,right=of c27] (c28) {C};
	\node[mycircle,right=of c28] (c29) {A};
	
	% dritte Sequenz
	\node[mycircle,below=of c21] (c31) {T};
	\node[mycircle,right=of c31] (c32) {A};
	\node[mycircle,right=of c32] (c33) {T};
	\node[mycircle,right=of c33] (c34) {C};
	\node[mycircle,right=of c34] (c35) {A};
	\node[mycircle,right=of c35] (c36) {D};
	\node[mycircle,right=of c36] (c37) {G};
	\node[mycircle,right=of c37] (c38) {G};
	
	% vierte Sequenz
	\node[mycircle,below=of c31] (c41) {D};
	\node[mycircle,right=of c41] (c42) {G};
	\node[mycircle,right=of c42] (c43) {T};
	\node[mycircle,right=of c43] (c44) {C};
	\node[mycircle,right=of c44] (c45) {A};
	\node[mycircle,right=of c45] (c46) {D};
	\node[mycircle,right=of c46] (c47) {A};
	\node[mycircle,right=of c47] (c48) {T};
	\node[mycircle,right=of c48] (c49) {C};
		
	
	% Erste Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c16/c33//above,
		c14/c22//above,
		c33/c22//above,
		c33/c43//above,
		c22/c43//above,
		c27/c48//above,
		c14/c31//above,
		c14/c43//above,
		c16/c48//above,
		c16/c27//above}
	\draw [draw=blue, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	% Zweite Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c17/c34//above,
		c34/c44//above,
		c23/c34//above,
		c17/c28//above,
		c23/c44//above,
		c28/c49//above,
		c15/c32//above,
		c15/c26//above,
		c15/c44//above,
		c17/c49//above,
		c26/c47//above}
	\draw [draw=green, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	% Dritte Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c18/c35//above,
		c35/c24//above,
		c35/c45//above,
		c24/c45//above,
		c18/c29//above}
	\draw [draw=red, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	% Vierte Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c25/c36//above,
		c25/c46//above,
		c36/c46//above}
	\draw [draw=cyan, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	% Fünfte Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c21/c42//above,
		c13/c21//above,
		c13/c42//above}
	\draw [draw=lime, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);
	
	% Sechste Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c12/c41//above}
	\draw [draw=magenta, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);
	\end{tikzpicture}
\end{center}

\subsection{Mehrdeutigkeiten Auflösen}

Die Zusammenhangskomponenten unseres Inzidenzgraphen werden wir jetzt als Flussnetzwerke interpretieren, um mit Hilfe eines Algorithmus zur Berechnung eines minimalen Schnitts solange Kanten zu entfernen, bis alle Mehrdeutigkeiten aufgelöst sind.

Sei $C$ eine Zusammenhangskomponente von $G_{\mathcal{F}}$, die mehrdeutig ist, also zwei Knoten $x,y$ aus der selben Sequenz enthält. Wir wählen die mehrdeutigen Knoten $x$ und $y$ als Quelle und Senke unseres Flussnetzwerks. Die ungerichteten Kanten des Inzidenzgraphen werden durch zwei antiparallele gerichtete Kanten ersetzt. \cite{cpm10} benutzen als Kapazitäten jeweils 1. Meiner Meinung nach wäre es hingegen sinnvoller die Kapazitäten so zu wählen, dass ähnliche Symbole seltener voneinander durch den minimalen Schnitt und das damit einhergehende Löschen von Kanten getrennt werden. Das können wir beispielsweise erreichen, indem wir uns an den Ähnlichkeitswerten unserer Substitutionsmatrix orientieren. Weil Flussnetzwerke nicht-negative Kapazitäten erwarten, müssen die Werte gegebenenfalls modifiziert werden, indem wir den betragsmäßig größten Eintrag der Matrix zu allen Einträgen addieren. Das bedeutet, dass bei unserer $(+3/-1)$-Substitutionsmatrix aus dem Beispiel zwischen übereinstimmenden Symbolen eine Kapazität von 6 und zwischen nicht übereinstimmendes eine von 2 benutzt wird. Leider war es aufgrund der begrenzten Zeit und des hohen Aufwands nicht möglich das ganze Verfahren zu implementieren. Eine Evaluierung mit echten Alignments fehlt also leider. Weil ich den Ansatz mit den nicht-unitären Kapazitäten als sehr sinnvoll erachte, habe ich ihn bei unseren Beispielen verwendet.

Als nächstes benutzen wir einen Algorithmus zur Bestimmung des maximalen Flusses. Wie mit dem Max-Flow-Min-Cut-Satz gezeigt, bestimmen wir durch die Bestimmung des maximalen Flusses auch gleichzeitig einen minimalen Schnitt. Das ist die \enquote{schmalste} Verbindungsstelle zwischen der Quelle und der Senke und wir hoffen durch Löschen der dazugehörigen Kanten die Mehrdeutigkeit aufzulösen und die dichtbesetzten Untergraphen zu erhalten. Im Original wird der Algorithmus von Edmonds-Karp benutzt, während wir stattdessen einen Push-Relabel-Algorithmus mit Laufzeit $\oh(|V|^2\cdot \sqrt{|E|})$ für die Komplexität betrachten und in unserer Implementierung verwenden beziehungsweise ihn verwendet hätten, wenn die Zeit dafür gereicht hätte. Nachdem der minimale Schnitt bestimmt wurde, löschen wir alle Kanten zwischen den Mengen $A$ und $B$. Auf diese Art und Weise wird unsere Zusammenhangskomponente $C$ in zwei neue Zusammenhangskomponenten $A$ und $B$ aufgeteilt. Dieses Verfahren wiederholen wir solange, bis es keine mehrdeutigen Äquivalenzklassen mehr gibt.  \improvement{Graphik aus CPM10 mit Zusammenhangskomponenten einfügen.}


\begin{algorithm}
	\caption{Algorithmus zum Auflösen von Mehrdeutigkeiten in einem Inzidenzgraphen}
	\label{alg:amb_res}
	\begin{algorithmic}[1]
		\Require Inzidenzgraph $G_{\mathcal{F}} = (\mathcal{S},E_{\mathcal{F}})$ über einem Satz Fragmente $\mathcal{F}$
		\Procedure{ResolveAmbiguities}{$G_{\mathcal{F}}$}
			\State{$E \gets E_{\mathcal{F}}$}
			\State{Berechne Zusammenhangskomponenten von $G_{\mathcal{F}}$}
			\While{es ex. mehrdeutige Zusammenhangskomponente $C$ von $G_{\mathcal{F}}$}
				\While{es ex. mehrdeutige Knoten $x,y$ aus der selben Sequenz}
					\State{Wähle Sequenz $S_i$ mit max. Anzahl an mehrdeutigen Knoten in $C$}
					\State{Wähle $s = \text{argmin}\{\deg(v)|v \in C\: \text{und}\: v \in S_i\}$}
					\State{Wähle $t = \text{argmax}\{\deg(v)|v \in C\: \text{und}\: v \in S_i\}$}
					\State{Definiere Flussnetzwerk auf $C$ mit Quelle $s$ und Senke $t$}
					\State{Benutze \textrm{PushRelabel}, um minimalen Schnitt $C_1$ und $C_2$ zu bestimmen}
					\State{Lösche Kanten zwischen $C_1$ und $C_2$ aus $E$}
				\EndWhile
			\EndWhile
		\State{\textbf{return} nicht-mehrdeutigen Subgraphen $(\mathcal{S},E)$ von $G_{\mathcal{F}}$}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

Unglücklicherweise können die Zusammenhangskomponenten bei Alignments zwischen vielen langen Sequenzen sehr groß werden. \cite{cpm10} haben daher eine Grenze $k = \max\{\deg(v)| v \in \mathcal{S}\}$ eingeführt, die sukzessive gesenkt wird, bis alle Mehrdeutigkeiten aufgelöst wurden. $k$ wird so benutzt, dass alle Kanten zwischen Knoten mit Grad $< k$ zunächst nicht betrachtet werden, sodass andere kleinere Zusammenhangskomponenten vorliegen. Als erstes werden für den reduzierten Kantensatz $E_k = \{(u,v) \in E|\min\{\deg(u),\deg(v)\}\}$ solange minimale Schnitte berechnet und Kanten gelöscht, bis für den Graph mit weniger Kanten keine mehrdeutigen Zusammenhangskomponenten mehr existierten. Danach wird $k$ um eins reduziert und das Vorgehen auf dem neuen Kantensatz wiederholt. Sobald $k$ auf null gesetzt wurde, hat man alle Knoten betrachtet und es liegen nur noch partielle Zuweisungsspalten vor. \improvement{keine eigene Implementierung, also Text über k anpassen.}

Abgesehen von der verbesserten Laufzeit scheint dieses Vorgehen aber keine Vorteile zu bieten und falls doch, werden diese in \cite{cpm10} nicht genannt. Deshalb verzichte ich auf die Grenze $k$ und gehe davon aus, dass aufgrund des deutlich effizienteren Algorithmus für den maximalen Flusses und die Verwendung einer schnellen Graphimplementierung aus einer Bibliothek keine Laufzeitprobleme für die üblichen Anwendungsgrößen auftreten. \improvement{Dieser Abschnitt muss noch ein bisschen umformuliert werden, weil das Verfahren nicht implementiert wurde. Also lieber den ursprünglichen Algorithmus mit Grenze k zeigen.}

Bis jetzt wurde nur gesagt, dass wir mehrdeutige Zusammenhangskomponenten auswählen und für diese jeweils einen Knoten aus der selben Sequenz als Quelle und Senke wählen. Abschließend müssen wir also noch festlegen in welcher Reihenfolge diese gewählt werden. Für die Zusammenhangskomponenten müssen wir keine Reihenfolge festlegen, weil diese unabhängig voneinander sind. Sei eine mehrdeutige Zusammenhangskomponente $C$ gegeben. Dann mag es mehrere Sequenzen geben, die alle zu mehr als einem Knoten in $C$ korrespondieren. Außerdem muss es nicht unbedingt einen eindeutigen minimalen Schnitt geben, sondern es kann mehrere solche geben. Wir entscheiden uns für dieses Vorgehen:

\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
	\item Falls es mehrere Sequenzen gibt, die zwei oder mehr mehrdeutige Stellen in $C$ enthalten, wähle die Sequenz $S_i$ mit den meisten mehrdeutigen Knoten in dieser Zusammenhangskomponente.
	\item Sobald $S_i$ bestimmt ist, wähle unter allen Stellen aus dieser Sequenz in $C$ den Knoten mit dem niedrigsten Knotengrad als Quelle und den mit dem höchsten als Senke.
	\item Sollte es mehr als einen minimalen Schnitt geben, dann orientieren wir uns an dem Ergebnis des Algorithmus der Implementierung und wählen die Partitionierung, die sich durch die nur durch Kanten mit Restwertkapazität 0 verbundenen Subgraphen im Restwertgraphen ergibt. 
\end{enumerate}

\subsection{Beispiel von \textrm{ResolveAmbiguities}}

Exemplarisch betrachten wir die grüne Zusammenhangskomponente, weil diese die meisten Knoten enthält und werden an ihrem Beispiel solange den minimalen Schnitt durchführen, bis keine Inkonsistenzen mehr vorliegen. Die Quelle ist grün, während die Senke rot dargestellt ist.

\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill=gray,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	mysenke/.style={
		circle,
		draw=black,
		fill=red,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},	
	myquelle/.style={
		circle,
		draw=black,
		fill=green,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 0.9cm
	]
	% erste Sequenz
	\node[mycircle] (c11) {A};
	\node[mycircle,right=of c11] (c12) {D};
	\node[mycircle,right=of c12] (c13) {G};
	\node[mycircle,right=of c13] (c14) {T};
	\node[mycircle,right=of c14] (c15) {C};
	\node[mycircle,right=of c15] (c16) {T};
	\node[mycircle,right=of c16] (c17) {C};
	\node[mycircle,right=of c17] (c18) {A};
	
	% zweite Sequenz
	\node[mycircle,below=of c11] (c21) {G};
	\node[mycircle,right=of c21] (c22) {T};
	\node[mycircle,right=of c22] (c23) {C};
	\node[mycircle,right=of c23] (c24) {A};
	\node[mycircle,right=of c24] (c25) {D};
	\node[mycircle,right=of c25] (c26) {C};
	\node[mycircle,right=of c26] (c27) {T};
	\node[mycircle,right=of c27] (c28) {C};
	\node[mycircle,right=of c28] (c29) {A};
	
	% dritte Sequenz
	\node[mycircle,below=of c21] (c31) {T};
	\node[mycircle,right=of c31] (c32) {A};
	\node[mycircle,right=of c32] (c33) {T};
	\node[mycircle,right=of c33] (c34) {C};
	\node[mycircle,right=of c34] (c35) {A};
	\node[mycircle,right=of c35] (c36) {D};
	\node[mycircle,right=of c36] (c37) {G};
	\node[mycircle,right=of c37] (c38) {G};
	
	% vierte Sequenz
	\node[mycircle,below=of c31] (c41) {D};
	\node[mycircle,right=of c41] (c42) {G};
	\node[mycircle,right=of c42] (c43) {T};
	\node[mysenke,right=of c43] (c44) {C};
	\node[mycircle,right=of c44] (c45) {A};
	\node[mycircle,right=of c45] (c46) {D};
	\node[myquelle,right=of c46] (c47) {A};
	\node[mycircle,right=of c47] (c48) {T};
	\node[mycircle,right=of c48] (c49) {C};

	% Zweite Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c17/c34/{0,6}/above,
		c15/c32/{0,2}/above,
		c34/c44/{0,6}/above,
		c23/c34/{0,6}/above,
		c17/c28/{0,6}/above,
		c23/c44/{0,6}/above,
		c28/c49/{0,6}/above,
		c15/c26/{2/6}/above,
		c15/c44/{2,6}/above,
		c17/c49/{0,6}/below}
	\draw [Stealth-Stealth, draw=black, double=white, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c26/c47/{2/2}/above}
	\draw [Stealth-Stealth, draw=black, double=white, dotted, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	\end{tikzpicture}
\end{center}

Sowohl $S_2$, als auch $S_4$ haben drei Knoten in $C$. Wir wählen hier $S_4[7]$ als Quelle und $S_4[4]$ als Senke, weil diese den kleinsten bzw. größten Knotengrad haben. In diesem Fall passiert nichts spannendes, weil der minimale Schnitt aufgrund der Kante mit Kapazität 2 direkt die Quelle vom Rest der Zusammenhangskomponente trennt.

Im nächsten Schritt hat $S_2$ mit 3 die meisten Knoten in $C$. Da alle zwei der Knoten Grad 2 haben, entscheiden wir uns für $S_2[6]$ als Quelle und $S_2[8]$ als Senke. Auch hier wird direkt die erste Kante an der Quelle gelöscht.

\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill=gray,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	mysenke/.style={
		circle,
		draw=black,
		fill=red,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},	
	myquelle/.style={
		circle,
		draw=black,
		fill=green,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 0.9cm
	]
	% erste Sequenz
	\node[mycircle] (c11) {A};
	\node[mycircle,right=of c11] (c12) {D};
	\node[mycircle,right=of c12] (c13) {G};
	\node[mycircle,right=of c13] (c14) {T};
	\node[mycircle,right=of c14] (c15) {C};
	\node[mycircle,right=of c15] (c16) {T};
	\node[mycircle,right=of c16] (c17) {C};
	\node[mycircle,right=of c17] (c18) {A};
	
	% zweite Sequenz
	\node[mycircle,below=of c11] (c21) {G};
	\node[mycircle,right=of c21] (c22) {T};
	\node[mysenke,right=of c22] (c23) {C};
	\node[mycircle,right=of c23] (c24) {A};
	\node[mycircle,right=of c24] (c25) {D};
	\node[myquelle,right=of c25] (c26) {C};
	\node[mycircle,right=of c26] (c27) {T};
	\node[mycircle,right=of c27] (c28) {C};
	\node[mycircle,right=of c28] (c29) {A};
	
	% dritte Sequenz
	\node[mycircle,below=of c21] (c31) {T};
	\node[mycircle,right=of c31] (c32) {A};
	\node[mycircle,right=of c32] (c33) {T};
	\node[mycircle,right=of c33] (c34) {C};
	\node[mycircle,right=of c34] (c35) {A};
	\node[mycircle,right=of c35] (c36) {D};
	\node[mycircle,right=of c36] (c37) {G};
	\node[mycircle,right=of c37] (c38) {G};
	
	% vierte Sequenz
	\node[mycircle,below=of c31] (c41) {D};
	\node[mycircle,right=of c41] (c42) {G};
	\node[mycircle,right=of c42] (c43) {T};
	\node[mycircle,right=of c43] (c44) {C};
	\node[mycircle,right=of c44] (c45) {A};
	\node[mycircle,right=of c45] (c46) {D};
	\node[mycircle,right=of c46] (c47) {A};
	\node[mycircle,right=of c47] (c48) {T};
	\node[mycircle,right=of c48] (c49) {C};
	
	% Zweite Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c17/c34/{0,6}/above,
		c15/c32/{0,2}/above,
		c34/c44/{0,6}/above,
		c23/c34/{0,6}/above,
		c17/c28/{0,6}/above,
		c23/c44/{6,6}/above,
		c28/c49/{0,6}/above,
		c15/c44/{6,6}/above,
		c17/c49/{0,6}/below}
	\draw [Stealth-Stealth, draw=black, double=white, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c26/c15/{6/6}/above}
	\draw [Stealth-Stealth, draw=black, double=white, dotted, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	\end{tikzpicture}
\end{center}

Gehen nun wieder zu Sequenz $S_4$ und wählen $S_4[9]$ als Quelle und $S_4[4]$ als Senke. Hier haben wir zwei relativ dichte Subgraphen, die nur durch die eine Kante $(S_1[7] \rightarrow S_3[4])$ miteinander verbunden sind. Diese löschen wir und danach gibt es in der rechten Zusammenhangskomponente keine Mehrdeutigkeiten mehr. 

\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill=gray,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	mysenke/.style={
		circle,
		draw=black,
		fill=red,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},	
	myquelle/.style={
		circle,
		draw=black,
		fill=green,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 0.9cm
	]
	% erste Sequenz
	\node[mycircle] (c11) {A};
	\node[mycircle,right=of c11] (c12) {D};
	\node[mycircle,right=of c12] (c13) {G};
	\node[mycircle,right=of c13] (c14) {T};
	\node[mycircle,right=of c14] (c15) {C};
	\node[mycircle,right=of c15] (c16) {T};
	\node[mycircle,right=of c16] (c17) {C};
	\node[mycircle,right=of c17] (c18) {A};
	
	% zweite Sequenz
	\node[mycircle,below=of c11] (c21) {G};
	\node[mycircle,right=of c21] (c22) {T};
	\node[mycircle,right=of c22] (c23) {C};
	\node[mycircle,right=of c23] (c24) {A};
	\node[mycircle,right=of c24] (c25) {D};
	\node[mycircle,right=of c25] (c26) {C};
	\node[mycircle,right=of c26] (c27) {T};
	\node[mycircle,right=of c27] (c28) {C};
	\node[mycircle,right=of c28] (c29) {A};
	
	% dritte Sequenz
	\node[mycircle,below=of c21] (c31) {T};
	\node[mycircle,right=of c31] (c32) {A};
	\node[mycircle,right=of c32] (c33) {T};
	\node[mycircle,right=of c33] (c34) {C};
	\node[mycircle,right=of c34] (c35) {A};
	\node[mycircle,right=of c35] (c36) {D};
	\node[mycircle,right=of c36] (c37) {G};
	\node[mycircle,right=of c37] (c38) {G};
	
	% vierte Sequenz
	\node[mycircle,below=of c31] (c41) {D};
	\node[mycircle,right=of c41] (c42) {G};
	\node[mycircle,right=of c42] (c43) {T};
	\node[mysenke,right=of c43] (c44) {C};
	\node[mycircle,right=of c44] (c45) {A};
	\node[mycircle,right=of c45] (c46) {D};
	\node[mycircle,right=of c46] (c47) {A};
	\node[mycircle,right=of c47] (c48) {T};
	\node[myquelle,right=of c48] (c49) {C};
	
	% Zweite Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c15/c32/{0,2}/above,
		c34/c44/{0,6}/above,
		c23/c34/{6,6}/above,
		c17/c28/{0,6}/above,
		c23/c44/{6,6}/above,
		c28/c49/{0,6}/above,
		c15/c44/{6,6}/above,
		c17/c49/{6,6}/below}
	\draw [Stealth-Stealth, draw=black, double=white, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c17/c34/{6/6}/above}
	\draw [Stealth-Stealth, draw=black, double=white, dotted, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	\end{tikzpicture}
\end{center}

Abschließend wird noch die Verbindung zwischen dem \texttt{A} aus der zweiten Sequenz und dem \texttt{C} an der fünften Stelle der ersten gelöscht, was hier nicht mehr graphisch dargestellt wurde. Danach sind alle Mehrdeutigkeiten aufgelöst und es liegen nur noch partielle Zuweisungsspalten vor.

Wenn wir den Algorithmus auch auf allen anderen Zusammenhangskomponenten anwenden, dann kommen wir zu folgendem Inzidenzgraphen ohne Mehrdeutigkeiten:
 
\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill=gray,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 1.1cm
	]
	% erste Sequenz
	\node[mycircle] (c11) {A};
	\node[mycircle,right=of c11] (c12) {D};
	\node[mycircle,right=of c12] (c13) {G};
	\node[mycircle,right=of c13] (c14) {T};
	\node[mycircle,right=of c14] (c15) {C};
	\node[mycircle,right=of c15] (c16) {T};
	\node[mycircle,right=of c16] (c17) {C};
	\node[mycircle,right=of c17] (c18) {A};
	
	% zweite Sequenz
	\node[mycircle,below=of c11] (c21) {G};
	\node[mycircle,right=of c21] (c22) {T};
	\node[mycircle,right=of c22] (c23) {C};
	\node[mycircle,right=of c23] (c24) {A};
	\node[mycircle,right=of c24] (c25) {D};
	\node[mycircle,right=of c25] (c26) {C};
	\node[mycircle,right=of c26] (c27) {T};
	\node[mycircle,right=of c27] (c28) {C};
	\node[mycircle,right=of c28] (c29) {A};
	
	% dritte Sequenz
	\node[mycircle,below=of c21] (c31) {T};
	\node[mycircle,right=of c31] (c32) {A};
	\node[mycircle,right=of c32] (c33) {T};
	\node[mycircle,right=of c33] (c34) {C};
	\node[mycircle,right=of c34] (c35) {A};
	\node[mycircle,right=of c35] (c36) {D};
	\node[mycircle,right=of c36] (c37) {G};
	\node[mycircle,right=of c37] (c38) {G};
	
	% vierte Sequenz
	\node[mycircle,below=of c31] (c41) {D};
	\node[mycircle,right=of c41] (c42) {G};
	\node[mycircle,right=of c42] (c43) {T};
	\node[mycircle,right=of c43] (c44) {C};
	\node[mycircle,right=of c44] (c45) {A};
	\node[mycircle,right=of c45] (c46) {D};
	\node[mycircle,right=of c46] (c47) {A};
	\node[mycircle,right=of c47] (c48) {T};
	\node[mycircle,right=of c48] (c49) {C};
	
	
	% Erste Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c14/c22//above,
		c33/c22//above,
		c33/c43//above,
		c22/c43//above,
		c27/c48//above,
		c14/c43//above,
		c16/c48//above,
		c16/c27//above}
	\draw [draw=blue, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	% Zweite Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c34/c44//above,
		c23/c34//above,
		c17/c28//above,
		c23/c44//above,
		c28/c49//above,
		c15/c44//above,
		c17/c49//above}
	\draw [draw=green, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	% Dritte Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c18/c35//above,
		c35/c24//above,
		c35/c45//above,
		c24/c45//above}
	\draw [draw=red, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	% Vierte Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c25/c36//above,
		c25/c46//above,
		c36/c46//above}
	\draw [draw=cyan, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	
	
	% Fünfte Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c21/c42//above,
		c13/c21//above,
		c13/c42//above}
	\draw [draw=lime, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);
	
	% Sechste Zusammenhangskomponente
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c12/c41//above}
	\draw [draw=magenta, thick] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);
	\end{tikzpicture}
\end{center}

Obwohl in diesem Graphen keine Mehrdeutigkeiten mehr existieren, ist die Zuordnung noch nicht konsistent. Das liegt an Überkreuzungen, wie der zwischen $(S_1[8] \rightarrow S_3[5] \rightarrow S_4[5])$ (rot) und $(S_1[7] \rightarrow S_4[9])$ (grün). 

\subsection{Komplexität}\label{ch:ra_kompl}

Der Inzidenzgraph lässt sich aus den Fragmenten der paarweisen Alignments in $\oh(n^2\cdot L)$ berechnen, weil es $\oh(n^2)$ davon gibt, die bis zu $\oh(L)$ Verbindungen enthalten.

Die Laufzeit von \textrm{ResolveAmbiguities} wird von der Laufzeit zur Berechnung des minimalen Schnitt durch \textrm{PushRelabel} dominiert \cite{cpm10}. Diese hängt von der Größe unserer Zusammenhangskomponenten ab. Im schlimmsten Fall besteht der Inzidenzgraph aus einer einzigen Zusammenhangskomponente bei der jeder Knoten mit mindestens einem Knoten aus jeder anderen Sequenz verbunden ist. In diesem Fall muss sie in vielen Schritten zerkleinert werden muss, bis nur noch partielle Zuweisungsspalten übrig bleiben. Eine Zusammenhangskomponente kann $n\cdot L$ Knoten und $\oh(n^2\cdot L)$ Kanten haben. Das liegt daran, dass wir als Grundlage unsere paarweisen Alignments benutzen, bei denen jede Stelle für jede andere Sequenz mit höchstens einer Stelle aus dieser verbunden ist. 

\textrm{PushRelabel} hat eine Komplexität von $\oh(|V|^2\cdot \sqrt{|E|})$, um einen minimalen Schnitt zu berechnen. Weil $|V| \in \oh(n\cdot L)$ und $|E| \in \oh(n^2\cdot L)$ gelten, braucht man für einen Durchlauf also $\oh(n^3\cdot L^{5/2})$. Wenn wir Pech haben, entfernt jeder Aufruf unseres Algorithmus für den maximalen Fluss nur einen einzigen Knoten aus der Zusammenhangskomponente. Das resultiert in $n\cdot L$ Aufrufen und einer Gesamtlaufzeit von $\oh(n^4\cdot L^{7/2})$. Die Kosten für das Bestimmen der Zusammenhangskomponenten, das Finden der Quellen und Senken und das Löschen von Kanten liegen jeweils in linearer Größe zur Größe des Graphen und werden daher hier nicht weiter betrachtet. Für ersteres benutzt man beispielsweise eine Tiefensuche, fürs zweite eine Breitensuche und letzteres ist simples Iterieren über alle Kanten.

Möglicherweise lässt sich eine bessere Laufzeit erreichen, wenn alle Kanten eine uniforme Kapazität haben, wie das bei der Variante der Autoren der Fall ist. Es lässt sich zeigen, dass das ein deutlich leichteres Problem ist \cite{gt14} und nach Karzanov und Even hat eine Variante des Algorithmus von Dinic in diesem Fall gute Laufzeiten.

Glücklicherweise sind die Zusammenhangskomponenten im Allgemeinen nicht so groß und in den meisten Fällen trennt man mit dem minimalen Schnitt auch nicht nur einzelne Knoten ab. Bei Testläufen auf der Protein-Referenzdatenbank BAliBASE haben \cite{cpm10} die Referenzmenge RV12 genauer betrachtet. Sie besteht aus 88 Sequenzfamilien, die im Schnitt zehn Sequenzen enthielten. Messungen haben ergeben, dass die Inzidenzgraphen auf diesen Sequenzfamilien im Schnitt 2877 Knoten und 10952 in 223 Zusammenhangskomponenten enthalten haben. Auf diesen Sequenzen lassen sich in guten Laufzeiten von weniger als einer Minute multiple Sequenzalignments berechnen. Anders sah es auf der Sequenzfamilie BB30003 aus. Diese besteht aus 142 Sequenzen und resultiert in einem monströsen Inzidenzgraphen, der nur aus einer einzigen Zusammenhangskomponente besteht. Nach einer Laufzeit von 20 Stunden ohne Ergebnis wurde der Lauf auf Graphen, der $1,5\cdot 10^6$ Kanten enthält, erfolglos abgebrochen. Erst mit einer Begrenzung des minimalen Fragmentgewichts auf 4 ließ sich in 13 Stunden ein Ergebnis erzielen. 

\section{Sukzessionsgraphen und der Algorithmus von Pitschi}

\subsection{Aufbau des Sukzessionsgraphen}

Wie wir gesehen haben, ist die Menge an Zuweisungen nach dem \textrm{ResolveAmbiguities}-Aufruf noch nicht konsistent, aber es liegen keine Mehrdeutigkeiten mehr vor. Das zwingt uns dazu weitere Verbindungen aus der Äquivalenzrelation zu löschen, bis diese ein Alignment, also konsistent ist. Wir führen dazu eine Datenstruktur ein, die die Zusammenhangskomponenten des reduzierten Inzidenzgraphen nach ihrer Ordnung in den beteiligten Sequenzen ordnet.

\begin{definition}[Sukzessionsgraph]
	Es sei eine Menge an Sequenzen $S$ mit Stellenraum $\mathcal{S}$ gegeben. Für diese Sequenzen liegt eine Menge $\mathcal{C}$ von Zusammenhangskomponenten vor, die alle partielle Zuweisungsspalten sind. Dann definieren wir den Sukzessionsgraph $SG(\mathcal{C}) = (\mathcal{C},E)$ als gerichteten, gewichteten Graphen. In $SG(\mathcal{C})$ fügen wir genau dann eine Kante $e = (C,C')$ für $C,C' \in \mathcal{C}$ ein, wenn eine Sequenz $S_i \in S$ existiert, für die Stellen $s = (i,p) \in C$ und $s' = (i,p') \in C'$ vorliegen mit $p < p'$ und außerdem keine Stelle $s = (i,p'')$ in einem anderen Knoten $C''$ mit $p < p'' < p'$ vorhanden ist. Das Gewicht von $e$ ist die Anzahl an Sequenzen für die die obige Bedingung gilt. Des Weiteren setzen wir voraus, dass alle Zusammenhangskomponenten mindestens zwei Knoten enthalten.
	Außerdem fügen wir zwei zusätzliche Knoten $v_{start}$ und $v_{end}$ ein. $v_{start}$ ist mit allen Knoten verbunden, die die erste Stelle einer Sequenz enthalten. Zusätzlich sind alle Knoten, die die letzte Stelle einer Sequenz enthalten mit $v_{end}$ über eine Kante verbunden.
\end{definition}


Wie man sieht sind zwei Knoten $C$ und $C'$ aus dem Sukzessionsgraphen genau dann miteinander verbunden, wenn es in diesen Stellen aus der selben Sequenz gibt, bei denen die aus $C$ links von der in $C'$ stehen. Die zusätzlich eingefügten Knoten $v_{start}$ und $v_{end}$ brauchen wir im Algorithmus von Pitschi, um die Konnektivität des Graphen zu erhalten, wenn wir Kanten aus dem Graph löschen. Das werden wir später tun, um Inkonsistenzen zu entfernen. Setzen wir nicht voraus, dass die Knoten von $SG$ mindestens zwei Stellen enthalten, dann liefert der Algorithmus von Pitschi suboptimale Ergebnisse, wie wir später sehen werden. \cite{cpm10} und \cite{pdc10} sind an dieser Stelle nicht eindeutig. 

Als Beispiel konstruieren wir aus dem reduzierten Inzidenzgraphen des letzten Schritts einen Sukzessionsgraphen. Die Knotenfarben wurden in der Farbe der Zusammenhangskomponenten des Inzidenzgraphen aus dem letzten Schritt gewählt.

\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 1.1cm
	]
	
	\node[mycircle,fill=gray] at (0.5,-0.5) (c1) {$v_{start}$};
	\node[mycircle,fill=magenta] at (2,1.5) (c2) {\begin{tabular}{c}
		$(1,2)$ \\ $(4,1)$
		\end{tabular}};
	\node[mycircle,fill=lime] at (4,1.5) (c3) {\begin{tabular}{c}
		$(1,3)$ \\ $(2,1)$ \\ $(4,2)$
		\end{tabular}};
	\node[mycircle,fill=blue] at (4,-0.5) (c4) {\begin{tabular}{c}
		$(1,4)$ \\ $(2,2)$ \\ $(3,3)$ \\ $(4,3)$
		\end{tabular}};
	\node[mycircle,fill=green] at (6,-0.5) (c5) {\begin{tabular}{c}
		$(1,5)$ \\ $(2,3)$ \\ $(3,4)$ \\ $(4,4)$
		\end{tabular}};
	\node[mycircle,fill=red] at (8,-0.5) (c6) {\begin{tabular}{c}
		$(1,8)$ \\ $(2,4)$ \\ $(3,5)$ \\ $(4,5)$
		\end{tabular}};
	\node[mycircle,fill=cyan] at (10,-0.5) (c7) {\begin{tabular}{c}
		$(2,5)$ \\ $(3,6)$ \\ $(4,6)$
		\end{tabular}};
	\node[mycircle,fill=blue] at (8,1.5) (c8) {\begin{tabular}{c}
		$(1,6)$ \\ $(2,7)$ \\ $(4,8)$
		\end{tabular}};
	\node[mycircle,fill=green] at (10,1.5) (c9) {\begin{tabular}{c}
		$(1,7)$ \\ $(2,8)$ \\ $(4,9)$
		\end{tabular}};
	\node[mycircle, fill=gray] at (12,-0.5) (c10) {$v_{end}$};
	% Kanten ohne Cycle
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c2/2/above,
		c1/c3/1/below,
		c2/c3/2/above,	
		c1/c4/1/above,
		c3/c4/3/above,
		c4/c5/4/above,
		c5/c6/3/above,
		c5/c8/1/above,
		c9/c10/2/above,
		c7/c10/1/above}	
	\draw [myarrow] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);
	
	\draw [myarrow] (c6) to[bend right] node[sloped,font=\tiny,below] {1} (c10);
		
	% Cycle%	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c6/c7/3/above,
		c8/c9/3/above}
	\draw [myarrow, draw=red] (\i) -- node[sloped,font=\tiny,\p] {\txt} (\j);	

	\draw [myarrow, draw=red] (c7) -- node[sloped,font=\tiny,above, pos=0.25] {2} (c8);	
	\draw [myarrow, draw=red] (c9) -- node[sloped,font=\tiny,above, pos=0.25] {1} (c6);		
	\end{tikzpicture}
\end{center}

\begin{lemma}
	Die Menge $\mathcal{C}$ ist genau dann konsistent, wenn $SG(\mathcal{C})$ ein gerichteter, azyklischer Graph (DAG) ist.
\end{lemma}

\begin{beweis}
	\bewhin \hspace{2pt} Sei $\mathcal{C}$ eine konsistente Menge von Zusammenhangskomponenten mit der induzierten Äquivalenzrelation $\mathcal{R}$. Dann folgt aus $x \preceq_{\mathcal{R}} y$ auch $x \preceq y$ für alle Sequenzen. Angenommen es gibt in $SG(\mathcal{C})$ einen Zyklus. Dann gibt es Knoten $C, C'$ mit Stellen $s_1, s_2 \in C$ und $s_1',s_2' \in C'$ mit folgenden Eigenschaften:
	\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
		\item $s_1, s_1' \in S_1$ und $s_2,s_2' \in S_2$ 
		\item $pos(s_1) < pos(s_1')$ und $pos(s_2') < pos(s_2)$
	\end{enumerate}
	Diese muss es geben, sonst gäbe es keinen Zyklus im Graphen. Für diese Stellen gelten $s_1' \mathcal{R} s_2'$, $s_2' \preceq s_2$ und $s_2 \mathcal{R} s_1$, woraus $s_1' \preceq_{\mathcal{R}} s_1$ folgt. Es gilt aufgrund von $pos(s_1) < pos(s_1')$, weshalb $s_1 \npreceq s_1'$ folgt. Das steht im Widerspruch dazu, dass $\mathcal{C}$ konsistent war.
	
	\bewrueck \hspace{2pt} Wir benutzen einen Kontrapositionsbeweis und es sei eine inkonsistente Menge $\mathcal{C}$ gegeben. Dann existiert eine Sequenz $S_1$, die Stellen $s_1,s_1' \in S_1$ mit folgenden Eigenschaften enthält:
	\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
		\item $s_1 \preceq_{\mathcal{R}} s_1'$
		\item $s_1 \npreceq s_1'$
	\end{enumerate}
	Aus $s_1 \preceq_{\mathcal{R}} s_1'$ folgt, dass es zwei Knoten $C$ und $C'$ gibt mit $s_1 \in C$ und $s_1' \in C'$, die in $SG(\mathcal{C})$ über einen Pfad verbunden sind. Der Pfad kann aber nicht über eine Kante laufen, die zu $S_1$ gehört, denn $s_1 \npreceq s_1'$ gilt. Aus dieser Bedingung folgt aber, dass einen Pfad von $C'$ zu $C'$ geben muss. Da es sowohl von $C$ nach $C'$, als auch von $C'$ nach $C$ Pfade gibt, kann $SG(\mathcal{C})$ nicht azyklisch sein.
\end{beweis}

\subsection{Der Algorithmus von Pitschi}

Aufgrund des gerade gezeigten Lemmas folgt, dass wir eine konsistente Menge von partiellen Zuweisungsspalten finden, wenn deren Sukzessionsgraph azyklisch ist. Der Algorithmus von \cite{pdc10} sieht zwei Schritte vor, um aus dem potentiell zyklischen Sukzessionsgraphen einen kreisfreien zu konstruieren, der mit einer konsistenten Menge von partiellen Zuweisungsspalten korrespondiert:

\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
	\item Lösche Kanten aus dem Sukzessionsgraphen, bis dieser kreisfrei ist.
	\item Benutze den so entstandenen DAG, um zu entscheiden, welche Stellen aus den jeweiligen Knoten gelöscht werden müssen, um Inkonsistenzen zu entfernen.
\end{enumerate}

\subsubsection{Entfernen der Kanten}

Wir beginnen mit der Transformation des zyklischen Graphen in einen azyklischen, indem wir Kanten entfernen. Optimal wäre es Kanten mit einer minimalen Summe von Kantengewichten zu entfernen. Leider ist dieses Problem, das auch als \enquote{minimal weighted feedback arc set}-Problem bekannt ist, NP-schwer. Als Folge bedienen wir uns einfach einer einfachen Heuristik, indem wir sukzessive eine Grenze $k$ erhöhen und in jedem Schritt alle Kanten mit einem Gewicht kleiner als $k$ löschen, bis es keinen Zyklus im Graphen mehr gibt. Formal definieren wir die Menge an Kanten, die mindestens das Gewicht $k\in \mathbb{N}$ hat als 
\begin{equation}
\begin{split}
	& E_k \coloneqq \{(u,v)\in E | w(u,v) > k\: \text{oder}\: u = v_{start}\: \text{oder}\: v = v_{end}\}\: \text{mit} \\
	& k^* \coloneqq \min\{k > 0 | (V,E_k)\: \text{ist azyklisch}\}
\end{split}
\end{equation}

Für den Fall, dass durch das Löschen von Kanten der Graph nicht mehr zusammenhängend ist, fügen wir für jede Zusammenhangskomponente und jede an dieser beteiligten Sequenz eine Kante vom Startzustand $v_{start}$ zum Knoten mit der kleinsten Stelle aus der gewählten Sequenz ein. Analog gehen wir mit den größten Stellen und dem Endzustand $v_{end}$ vor. Diese Menge dieser Kanten nennen wir $E_c$. Auf diese Weise ist sichergestellt, dass der azyklische Graph $G^* = (V,E_{k^*}\cup E_c)$ zusammenhängend ist und über jeden Knoten ein Pfad vom Start- zum Endzustand führt.

\subsubsection{Beispiel zum Entfernen von Kanten}

Bei unserem Beispiel ist glücklicherweise nur Zyklus vorhanden, der bereits in $(V,E_1)$ nicht mehr vorhanden ist. Wir löschen also alle Kanten mit Kantengewicht 1 oder weniger, die nicht zum Start- oder Endknoten gehören. Entfernte Kanten wurden gestrichelt dargestellt. Das sieht dann so aus:  

\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 1.1cm
	]
	
	\node[mycircle,fill=gray] at (0.5,-0.5) (c1) {$v_{start}$};
	\node[mycircle,fill=magenta] at (2,1.5) (c2) {\begin{tabular}{c}
		$(1,2)$ \\ $(4,1)$
		\end{tabular}};
	\node[mycircle,fill=lime] at (4,1.5) (c3) {\begin{tabular}{c}
		$(1,3)$ \\ $(2,1)$ \\ $(4,2)$
		\end{tabular}};
	\node[mycircle,fill=blue] at (4,-0.5) (c4) {\begin{tabular}{c}
		$(1,4)$ \\ $(2,2)$ \\ $(3,3)$ \\ $(4,3)$
		\end{tabular}};
	\node[mycircle,fill=green] at (6,-0.5) (c5) {\begin{tabular}{c}
		$(1,5)$ \\ $(2,3)$ \\ $(3,4)$ \\ $(4,4)$
		\end{tabular}};
	\node[mycircle,fill=red] at (8,-0.5) (c6) {\begin{tabular}{c}
		$(1,8)$ \\ $(2,4)$ \\ $(3,5)$ \\ $(4,5)$
		\end{tabular}};
	\node[mycircle,fill=cyan] at (10,-0.5) (c7) {\begin{tabular}{c}
		$(2,5)$ \\ $(3,6)$ \\ $(4,6)$
		\end{tabular}};
	\node[mycircle,fill=blue] at (8,1.5) (c8) {\begin{tabular}{c}
		$(1,6)$ \\ $(2,7)$ \\ $(4,8)$
		\end{tabular}};
	\node[mycircle,fill=green] at (10,1.5) (c9) {\begin{tabular}{c}
		$(1,7)$ \\ $(2,8)$ \\ $(4,9)$
		\end{tabular}};
	\node[mycircle, fill=gray] at (12,-0.5) (c10) {$v_{end}$};
	% Kanten ohne Cycle
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c2/2/above,
		c1/c3/1/below,
		c2/c3/2/above,	
		c1/c4/1/above,
		c3/c4/3/right,
		c4/c5/4/above,
		c5/c6/3/above,
		c9/c10/2/above,
		c7/c10/1/above}	
	\draw [myarrow] (\i) -- node[font=\tiny,\p] {\txt} (\j);
	
	\draw [myarrow] (c6) to[bend right] node[font=\tiny,below] {1} (c10);

	\draw [myarrow,dotted] (c5) -- node[font=\tiny,above] {1} (c8);
	
	% Cycle%	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c6/c7/3/above,
		c8/c9/3/above}
	\draw [myarrow, draw=red] (\i) -- node[font=\tiny,\p] {\txt} (\j);	
	
	\draw [myarrow, draw=red] (c7) -- node[font=\tiny,above, pos=0.25] {2} (c8);	
	\draw [myarrow, draw=red, dotted] (c9) -- node[font=\tiny,above, pos=0.25] {1} (c6);		
	\end{tikzpicture}
\end{center}

\subsubsection{Entfernen von Stellen}

Als nächstes lernen wir einen von \cite{pdc10} entwickelten Algorithmus kennen, der eine minimale Anzahl an Stellen aus dem verkleinerten Sukzessionsgraphen $G^{*}$ löscht, um alle Inkonsistenzen zu entfernen. Dabei orientiert sich der Algorithmus an der linearen Halbordnung $\preceq$ über $\mathcal{S}$ und der Halbordnung auf dem DAG, die ersterer angepasst werden soll. Wir bezeichnen die Halbordnung auf $\mathcal{C}$, die durch den Graph $G^{*}$ induziert wird, als $\preceq^{*}$.

Sei eine Sequenz $S_i \in S$ gegeben. Dann ist $\mathcal{C}_{S_i}$ die Teilmenge von Zusammenhangskomponenten aus $\mathcal{C}$, die Stellen aus $S_i$ enthalten. Wir definieren außerdem eine Beschränkung der Knoten aus $G^{*}$ von $\mathcal{C}_{S_i}$ als $V_{S_i} = \mathcal{C}_{S_i} \cup \{v_{start},v_{end}\}$. Auf diesen existiert eine Ordnung $\preceq_{S_i}$, die durch die natürliche Ordnung auf $S_i$ gegeben ist, und die Halbordnung $\preceq_{S_i}^{*}$ des Graphen mit reduzierter Knotenmenge. Wir definieren $\mathcal{R}_{S_i} = \preceq_{S_i}\, \cap\, \preceq_{S_i}^{*}$. Diese Relation entspricht genau den Verbindungen der transitiven Hülle im ursprünglichen Graph, wenn man $G^{*}$ auf die Knoten aus $\mathcal{C}_{S_i}$ beschränkt. Sei dafür $G^{+} = (V,E^{+})$ die transitive Hülle $TC(G^{*})$ des DAG. Weil $G^{*}$ keine Zyklen enthält, kann auch $TC(G^{*})$ keine enthalten. Wir können $G^{*}$ auf jede unserer Sequenzen $S_i$ beschränken, indem wir den reduzierten Knotensatz $V_{S_i}$ benutzen und genau dann eine Kante zwischen zwei Knoten einfügen, falls diese in unserer Relation $\mathcal{R}_{S_i}$ liegen.

\begin{definition}
	Der Graph $G_{S_i}$ einer Sequenz $S_i$ ist definiert als Graph über der Knotenmenge $V_{S_i}$ und den Kanten $E_{S_i}$. Es gilt
	\begin{equation}
		(u,v) \in E_{S_i} \Longleftrightarrow u,v \in V_{S_i}, (u,v) \in E^{+}\: \text{und}\: u \preceq_{S_i} v \Longleftrightarrow u\: \mathcal{R}_{S_i}\: v
	\end{equation}
\end{definition}

Pfade von $v_{start}$ nach $v_{end}$ in $G_{S_i}$ sind genau die Teilmengen von partiellen Zuweisungsspalten, die bezüglich $S_i$ konsistent sind. Das liegt daran, dass für zwei Knoten $u,v \in V_{S_i}$ auf einem solchen Pfad sowohl $u \preceq_{S_i} v$, als auch $u \preceq_{S_i}^{*} v$ gelten und damit $u \preceq_{S_i}^{*} v \implies u \preceq_{S_i} v$. Das war aber genau die Definition für Konsistenz: dass die Relation die natürliche Ordnung auf den Sequenzen erhält. Wie entfernen daher alle Stellen unserer Sequenz aus den Knoten, die nicht auf dem gewählten Pfad liegen, weil diese die Konsistenz verletzen würden.

Wenn wir jetzt für jede unserer Sequenzen $S_j$ einen Pfad von $v_{start}$ nach $v_{end}$ in $G_{S_j}$ wählen und alle nicht-besuchten Stellen  aus ihren Knoten entfernen, dann hält die Konsistenzbedingung auf der Relation, die durch die übriggebliebenen Zusammenhangskomponenten induziert wird. Das Resultat ist also ein Alignment. Da wir aber nicht irgendein Alignment erhalten wollen, sondern ein möglichst großes, wählen wir für jede Sequenz $S_i \in S$ den Pfad maximaler Länge durch $G_{S_i}$. Auf diese Weise löschen wir die minimale Anzahl an Stellen aus ihren Zusammenhangskomponenten. Formal: Sei für eine Sequenz $S_i$ $g_{S_i}$ ein Pfad maximaler Länge $(v_{start}, u_1, \dots, u_n, v_{end})$ gegeben. Wir iterieren über alle Knoten $C \in \mathcal{C}_{S_i}$ und entfernen Stellen $(i,p) \in C$, falls $C \notin g_{S_i}$. Wenn wir das für alle Sequenzen tun, dann nennen wir die Menge der reduzierten Zusammenhangskomponenten $C°$. Weil die mit $C°$ korrespondierende Relation ein Alignment ist, wäre der Sukzessionsgraph $SG(C°)$ ein DAG. Da die Pfade für jede Sequenz unabhängig voneinander sind, können wir diese in beliebiger Reihenfolge wählen, ohne dass dies einen Einfluss auf unser Ergebnis hat.

Letztendlich folgt, dass das Problem die partiellen Zuweisungsspalten auf konsistente Zuweisungsspalten zu reduzieren auf die Suche nach Pfaden maximaler Länge durch gerichtete azyklische Graphen abbildbar ist \cite{cpm10}.

\subsubsection{Beispiel zum Entfernen von Stellen mit dem Algorithmus von Pitschi}

Bevor wir im nächsten Abschnitt genauer kennenlernen, wie man einen längsten Pfad im DAG eigentlich genau berechnet, betrachten wir unser Beispiel, in diesem Fall nur für die Sequenz $S_1$. Weil der Algorithmus zum Berechnen des längsten Pfades die Kantengewichte ignoriert und nur Knoten zählt, wurden diese nicht mehr angegeben. Formal handelt es sich hierbei um die Suche nach einem längsten Pfad in einem ungewichteten Graphen. Die Pfade, die über die Bildung der transitiven Hülle neu dazu kamen, sind grau angedeutet.

\begin{center}
	\begin{tikzpicture}[
	mycircle/.style={
		circle,
		draw=black,
		fill opacity = 0.3,
		text opacity=1,
		inner sep=0pt,
		minimum size=15pt,
		font=\tiny},
	myarrow/.style={-Stealth},
	node distance=0.6cm and 1.1cm
	]
	
	\node[mycircle,fill=gray] at (0.5,-0.5) (c1) {$v_{start}$};
	\node[mycircle,fill=magenta] at (2,2) (c2) {\begin{tabular}{c}
		$(1,2)$ \\ $(4,1)$
		\end{tabular}};
	\node[mycircle,fill=lime] at (4,2) (c3) {\begin{tabular}{c}
		$(1,3)$ \\ $(2,1)$ \\ $(4,2)$
		\end{tabular}};
	\node[mycircle,fill=blue] at (4,-0.5) (c4) {\begin{tabular}{c}
		$(1,4)$ \\ $(2,2)$ \\ $(3,3)$ \\ $(4,3)$
		\end{tabular}};
	\node[mycircle,fill=green] at (6,-0.5) (c5) {\begin{tabular}{c}
		$(1,5)$ \\ $(2,3)$ \\ $(3,4)$ \\ $(4,4)$
		\end{tabular}};
	\node[mycircle,fill=red] at (8,-0.5) (c6) {\begin{tabular}{c}
		$(1,8)$ \\ $(2,4)$ \\ $(3,5)$ \\ $(4,5)$
		\end{tabular}};
	\node[mycircle,fill=blue] at (8,2) (c8) {\begin{tabular}{c}
		$(1,6)$ \\ $(2,7)$ \\ $(4,8)$
		\end{tabular}};
	\node[mycircle,fill=green] at (10,2) (c9) {\begin{tabular}{c}
		$(1,7)$ \\ $(2,8)$ \\ $(4,9)$
		\end{tabular}};
	\node[mycircle, fill=gray] at (12,-0.5) (c10) {$v_{end}$};
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c8//above,
		c2.310/c10//above,
		c2.310/c4//above,
		c2.310/c5//above,
		c2.310/c6//above,
		c3/c5//above,
		c3/c6//above,
		c3/c8//above,
		c3/c10//above,
		c4/c8//above,
		c4/c9//above,
		c5/c9//above,
		c8/c10//above}	
	\draw [myarrow,lightgray] (\i) -- node[font=\tiny,\p] {\txt} (\j);
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c5//above,
		c1/c6//above,
		c1/c10//above,
		c4/c6//above,
		c4/c10//above,
		c5/c10//above}	
	\draw [myarrow,lightgray] (\i) to[bend right] node[font=\tiny,\p] {\txt} (\j);
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c2/c8//above,
		c2/c9//above,
		c3/c9//above}	
	\draw [myarrow,lightgray] (\i) to[bend left] node[font=\tiny,\p] {\txt} (\j);
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c3//below,	
		c1/c4//above,
		c5/c6//above,
		c8/c9//above,
		c6/c10//above}	
	\draw [myarrow] (\i) -- node[font=\tiny,\p] {\txt} (\j);
	
	\foreach \i/\j/\txt/\p in {% start node/end node/text/position
		c1/c2//above,
		c2/c3//above,	
		c3/c4//right,
		c4/c5//above,
		c5/c8//above,
		c8/c9//above,
		c9/c10//above}	
	\draw [myarrow,red] (\i) -- node[font=\tiny,\p] {\txt} (\j);
	
	\end{tikzpicture}
\end{center}

\vspace{-5pt}

Im Graphen $G_{S_1}$ gibt es eine Kante vom $(1,5)$er-Knoten zum $(1,6)$er, obwohl diese direkte Kante in $G^{*}$ gelöscht wurde. Das liegt an der Verbindung $(2,3) \rightarrow (2,4) \rightarrow (2,5) \rightarrow (2,7)$ und $(1,5) \preceq_{S_1} (1,6)$. Die Kante von $(1,8) \rightarrow (1,6)$ gibt es aber nicht, obwohl diese in der transitiven Hülle verbunden sind. Das liegt an $(1,8) \npreceq_{S_i} (1,6)$. Der hellblaue Knoten enthält keine Stelle aus $S_1$ und ist daher nicht Teil von $V_{S_1}$.

Wie schon beim Blick auf den ersten Sukzessionsgraphen vermutet, ist das Problem die Stelle $(1,8)$, die für Überkreuzungen sorgt. Sie liegt nicht auf dem längsten Pfad (rot markiert) und wird daher aus der Zusammenhangskomponente des Knoten entfernt. Die längsten Pfade aller anderen Sequenzen besuchen auch alle Knoten der jeweiligen Graphen, weshalb keine weiteren Knoten modifiziert werden müssen.

\subsection{Algorithmus zur Bestimmung des längsten Pfads}

Es sei ein gerichteter, azyklischer Graph $G = V,E$ gegeben. Ein einfacher Ansatz wäre es einen Algorithmus, der auch mit negativen Kantengewichten rechnen kann zu benutzen, um im Graphen mit negierten Kantengewichten $G^{-} = (V,E^{-})$ den kürzesten Pfad zu berechnen. Der kürzeste Pfad mit umgedrehten Kantengewichten entspricht genau dem längsten Pfad im Ursprungsgraph. Der Algorithmus von Bellman-Ford berechnet den kürzesten Pfad eines Ausgangsknotens (hier $v_start$) zu allen anderen Knoten im Graphen in $\oh(|V|\cdot |E|) = \oh(n^3\cdot L^3)$ in unserem Fall \cite[S.651ff.]{clrs09}. Anders als beispielsweise der gierige Algorithmus von Dijkstra kann Bellman-Ford auch mit negativen Kanten umgehen, solange der Graph keine negativen Zyklen enthält. 

Weil unsere Graphen $G_{S_i}$ azyklisch sind lässt sich die Berechnung der längsten Pfade mit der \emph{topologischen Sortierung} der Knoten effizienter umsetzen.

\begin{definition}[Topologische Sortierung]
	Eine \emph{topologische Sortierung} von DAG $G = (V,E)$ ist eine lineare Aufzählung der Knoten in $V$ bei der ein Knoten $v$ hinter einem Knoten $u$ auftaucht, wenn es eine Kante $(u,v)\in E$ gibt.
\end{definition}

Die topologische Sortierung liefert eine mögliche Aufzählung der Knoten, die die Halbordnung des Graphen erhält. Im Allgemeinen ist sie nicht eindeutig. Das Standardmotivation dafür ist das Anziehen von Kleidungsstücken: Ich muss meine Socken anziehen, bevor ich in meine Schuhe schlüpfen kann und es kommt zuerst die Unterwäsche und dann die Hose. Erst muss ich Unterhemd und oder T-Shirt anziehen, bevor der Pullover kommt. Diese Reihenfolgen lassen sich als DAG modellieren, wobei jede Kante zwischen Kleidungsstück $a$ und $b$ bedeutet, dass $a$ vor $b$ angezogen werden muss. Gültige topologische Sortierungen für dieses vereinfachte Beispiel \footnote{Mr. Bean hat mit seiner Badehose bewiesen, dass die übliche Reihenfolge des An- und Ausziehens nicht unbedingt eingehalten werden muss.} sind also beispielsweise diese: \enquote{(Socken, Schuhe, Unterhose, Hose, Unterhemd, T-Shirt,Pullover)}, \enquote{(Socken, Unterhose, Schuhe, Unterhemd, T-Shirt, Pullover, Hose)} oder \enquote{(Unterhose, Hose, Socken, Unterhemd, T-Shirt, Pullover, Schuhe)}. 

Mit Hilfe einer Tiefensuche lässt sich die topologische Sortierung in $\theta(|V| + |E|)$ berechnen \cite[S.612ff.]{clrs09}. Das Ergebnis ist eine Liste der Knoten in topologischer Reihenfolge. Die Liste können wir zur Berechnung des längsten Pfades nutzen, indem wir zusätzlich für jeden Knoten die Länge des längsten Pfades und den Vorgänger in diesem speichern. Wir starten bei $v_{start}$ mit Länge 0 und Vorgänger \enquote{NIL}. Aufgrund der gegebenen topologischen Sortierung wissen wir, dass jeder Knoten erst dann bearbeitet wird, wenn seine Vorgänger bereits behandelt wurden. Vorgänger und längster Pfad bis zu diesem Knoten lassen sich also rekursiv berechnen:

\begin{equation}
\begin{split}
	distance(v_{start}) & = 0 \\
	distance(v)       & = \max\{distance(u) + w((u,v))\:|\:(u,v) \in E\ \wedge v \neq v_{start}\} 
\end{split}
\end{equation} 

Der Vorgänger für die Knoten wird dann abhängig von der gewählten Kante gesetzt, die die größte Gesamtlänge liefert. Das Kantengewicht $w((u,v))$ ist in unserem Fall 1, weil wir nur die Anzahl der Knoten von $v_{start}$ bis $v_{end}$. Die Korrektheit des Algorithmus folgt aufgrund der Korrektheit der Rekursionsgleichung und weil $distance(u)$ wegen der topologischen Sortierung bereits berechnet wurde für Knoten $v$, wenn eine Kante $(u,v)$ existiert.

Jetzt kennen wir die Länge des längsten Pfades, müssen aber noch die Knoten, die zu diesem gehören, berechnen. Dazu nutzen wir einen Backtracking-Algorithmus, indem wir mit dem letzten Knoten starten und solange den Vorgänger auswählen, bis es keinen mehr gibt. Bei unserem Graphen $G_{S_i}$ ist das einfach, weil mit $v_{start}$ und $v_{end}$ auf jeden Fall der erste und letzte Knoten unseres längsten Pfads bekannt sind. In allgemeinen DAGs muss man zuerst in linearer Zeit einmal über die Liste iterieren und den Knoten mit der maximalen gespeicherten Länge als Endknoten des Pfads und Startknoten des Backtrackings wählen.

\begin{algorithm}
	\caption{Algorithmus zum berechnen des längsten Pfads in einem Graphen $G_{S_i}$}
	\label{alg:longestpath}
	\begin{algorithmic}[1]
		\Require DAG $G = (V,E)$ mit $v_{start}$ und $v_{end}$
		\Procedure{LongestPath}{$G$}
			\State {$list_{ts} \gets topologicalSort(G)$}
			\State {$v_{start}.distance \gets 0$}
			\State {$v_{start}.pred \gets $ NIL}
			\State {$current \gets list_{ts}.head$}
			\While {$current.next \neq $ NIL} \Comment {Berechne Distanz zu $v_{end}$}
				\State {$current \gets current.next$}
				\State {$maxPred \gets argmax\{u.distance\:|\:(u,current.key) \in E\}$}
				\State {$current.key.distance \gets maxPred.distance + 1$}
				\State {$current.key.pred \gets maxPred$}			
			\EndWhile
			\State {$longestPath \gets empty$} \Comment {initialisiere Ausgabeliste}
			\State {$longestPath.addFirst(v_{end}$}	
			\State {$currentNode \gets v_{end}$}		
			\While {$currentNode.pred \neq $ NIL} \Comment {Backtracking}
				\State {$currentNode \gets currentNode.pred$}
				\State {$longestPath.addFirst(currentNode)$}	
			\EndWhile
			\Return {$longestPath$}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

Im kommenden Kapitel wird die Implementierung dieses Algorithmus beispielhaft für das ganze Verfahren beschrieben. Da dort auch ein konkretes Beispiel behandelt wird, verzichte ich darauf an dieser Stelle.

\subsubsection{Laufzeit}

Im Allgemeinen hat dieser Algorithmus eine Laufzeit von $\oh(|V| + |E|)$. Die topologische Sortierung benötigt $\theta(|V| + |E|)$ Rechenschritte. Für die Berechnung der Distanzwerte müssen in jedem der $\oh(|V|)$ Knoten das Maximum der Knoten über eingehende Kanten berechnet werden. Jede Kante wird nur einmalig betrachtet und somit ist die Summe der Kosten aller Maximumsoperationen in $\oh(|E|)$. Der Backtrackingprozess ist in linearer Zeit in der Größe der Anzahl der Knoten möglich.

Wenn wir $G_{S_i}$ betrachten, dann stellen wir fest, dass $|V| \in \oh(L)$ und $|E| \in \oh(L^2)$ liegen aufgrund der zuvor berechneten transitiven Hülle. Für die Berechnung des längsten Pfads folgt somit eine Laufzeit von $\oh(L^2)$.

\subsection{Verankerungen}

\cite{mpps06} haben einen semiautomatisierten Ansatz für das \emph{Multiple-Sequence-Alignment}-Problem entwickelt. Bei diesem kann der Benutzer, der über Expertenwissen verfügt, vorgeben, welche Positionen der Sequenzen auf jeden Fall miteinander aligniert werden sollen. Das ist insbesondere dann nützlich, wenn es Abschnitte gibt, die sich mathematisch gesehen sehr ähnlich sind, deren Zuordnung biologisch aber falsch wäre. Ein Beispiel für solche Sequenzen haben wir mit den Hoxgenen des Pufferfisches im Abschnitt über die Schwächen von DIALIGN bereits kennengelernt. Es hat sich herausgestellt, dass das semiautomatisierte Verfahren bei vielen Tests auf der Referenzdatenbank BAliBASE bessere Ergebnisse geliefert hat, wenn die Startpunkte aller Motive als Verankerungen gesetzt wurden.

Das Verfahren funktioniert so, dass der Experte Fragmente (die auch nur paarweise Zuweisungen sein können) wählt und diese mit einer Dringlichkeit an DIALIGN übergibt. Wie im Standardalgorithmus werden dann gierig die Fragmente mit maximaler Dringlichkeit gewählt, die zu allen bereits gewählten konsistent sind. Zwischen diesen Verankerungen wird dann ganz normal DIALIGN durchgeführt, wodurch die restlichen Abschnitte automatisiert aligniert werden.

\cite{cpm10} benutzten die bereits vorliegende Infrastruktur in DIALIGN, um die Zuweisungsspalten, die der Algorithmus von Pitschi geliefert hat, als Verankerungen ins Alignment zu integrieren. Weil diese Möglichkeit in unserer Implementierung nicht bereits vorhanden ist, werden wir die Resultate des Min-Cut-Ansatzes direkt mit Hilfe von \textrm{EdgeAddition} und dem Alignmentgraphen in unser Ergebnis integrieren. Neben dem Übergeben von konsistenten Zuweisungsspalten als Verankerungen hat man auch Versuche mit partiellen Zuweisungsspalten gemacht. Hier war die gierige Heuristik von DIALIGN aber nicht erfolgreich und die Ergebnisse waren schlechter als mit dem Algorithmus von Pitschi. Aus diesem Grund werden wir den zweiten Ansatz nicht weiter behandeln.

\subsection{Komplexität}

Der Sukzessionsgraph $SG(C)$ kann aus dem Inzidenzgraphen in $\oh(n\cdot L)$ Rechenschritten berechnet werden, weil es höchstens so viele partielle Zuweisungsspalten gegeben kann. Außerdem ist die Anzahl der direkten Nachfolger pro Sequenz auch durch $L$ begrenzt, was eine obere Grenze für die Anzahl der Kanten liefert.

Die transitive Hülle für $G^{+}$ berechnen wir beispielsweise durch Breitensuche für jeden Knoten. Weil es $\oh(n\cdot L)$ Knoten geben kann und jeder dieser Aufrufe $\oh(n\cdot L)$ Rechenschritte benötigt, weil dies die maximale Anzahl an Kanten und Knoten ist, folgt hierfür die Laufzeit von $\oh(n^2\cdot L^2)$. Überlegungen für einen effizienteren Algorithmus haben sich als fruchtlos erwiesen. Für diesen hätten wir zuerst eine topologische Sortierung berechnet und von hinten nach vorne bearbeitet. Die transitive Hülle eines Knotens $u$ kann dabei durch Vereinigung aller Knoten $v$ mit $(u,v) \in E$ sowie deren transitive Hülle berechnet werden. Im schlimmsten Fall habe ich $\oh(L)$ Ebenen in meinem Graphen mit je bis zu $\oh(n)$ Knoten und $\oh(n)$ Kanten in andere Ebenen. Es kommt pro Ebene also zu bis zu $\oh(n)$ vielen Vereinigungen. Weil die Mengen, die miteinander vereinigt werden, aber bis zu $\oh(n\cdot L)$ Elemente enthalten (potentiell erreichbare Knoten), komme ich selbst unter Verwendung von Bit-Sets und Vereinigung über das bitweise Oder auf Kosten von $\oh(n^2\cdot L)$ pro Ebene. Auf Grund der $\oh(L)$ Ebenen bleibt es bei der Laufzeit von $\oh(n^2\cdot L^2)$, selbst wenn dieser Algorithmus in der Praxis sehr schnell sein dürfte. Weil ich im Allgemeinen nicht davon ausgehen kann, dass mein Graph signifikant viel weniger starke Zusammenhangskomponenten als Knoten insgesamt enthält, liefert auch der Algorithmus von Purdom keine bessere Laufzeit.

Für das Konstruieren der Graphen $G_{S_i}$ unserer Sequenzen $S_1, \dots, S_n$ benötigen wir jeweils lineare Zeit in der Größe des Graphen $G^{+}$, um zu überprüfen welche Knoten Stellen der jeweiligen Sequenz enthalten und welche Kanten die Relation $\mathcal{R}_{S_i}$ erfüllen. Zwar kann es grundsätzlich $\oh(n\cdot L)$ Knoten geben und jeder Knoten bis zu $\oh(n)$ Stellen, aber die Gesamtanzahl an Stellen ist durch $n\cdot L$ begrenzt. Bei $n$ Sequenzen folgt die Laufzeit in $\oh(n^2\cdot L)$.

Die längsten Pfade für alle Sequenzen benötigen $n$-mal $\oh(L^2)$. Zudem müssen die nicht besuchten Stellen für jede Sequenz aus ihren Knoten gelöscht werden. Dazu iterieren wir wieder $n$-mal über die Graphen mit bis zu $\oh(L)$ Knoten. Wie schnell es möglich ist die Stellen aus dem dazugehörigen Knoten zu löschen, hängt von der intern verwendeten Datenstruktur ab. Im schlimmsten Fall brauchen wir $\oh(n)$ Zeit für die Entfernung einer Stelle aus einem Knoten. Alle Löschoperationen sind dementsprechend in $\oh(n^2\cdot L)$ Rechenschritten möglich.

Insgesamt dominiert das Berechnen der transitiven Hülle die Komplexität des Algorithmus von Pitschi mit $\oh(n^2\cdot L^2)$. Weil der vorangegangene Schritt mit den minimalen Schnitten aber ohnehin eine deutlich höhere Komplexität hat, ist es müßig hier nach effizienteren Ansätzen zu suchen. 

\section{Abschluss und Zusammenfassung}

Nach dem Algorithmus von Pitschi liegen uns konsistente Zuweisungsspalten vor. Diese können aufgrund ihrer Konsistenz direkt in das finale Alignment eingefügt werden. Trotzdem müssen wir die Transitivitätsgrenzen mit dem Alignmentgraphen verwalten. Das liegt daran, dass wir wie bei DIALIGN zwischen den ursprünglichen Konsistenzgrenzen auf den Teilsequenzen DIALIGN benutzen. Auch hier kann davon ausgegangen werden, dass die Anzahl der Durchläufe konstant ist. Zu guter Letzt wird die Ausgabe vorbereitet. Das Vorgehen ist an dieser Stelle identisch mit dem der ursprünglichen DIALIGN-Implementierung, das in Kapitel 3 beschrieben wurde.

Ein Nachteil des Min-Cut-Ansatzes ist das Lösen vom streng segmentbasierten Ansatz von DIALIGN. Dieser stellt zwar nach wie vor die Basis des Verfahrens dar, aber durch die minimalen Schnitt und den Algorithmus von Pitschi werden einzelne Verbindungen gelöscht. Möglicherweise sind unter den gelöschten Zuweisungen auch konsistente dabei. Um sicherzustellen, dass diese Teil des Alignments sind, wenn sie zu Unrecht gelöscht wurden, iterieren wir einmalig über alle ursprünglichen Fragmente der paarweisen Alignments und fügen konsistente Zuweisungen von nicht mehr miteinander alignierten Stellen ein, wenn diese einen positiven Ähnlichkeitswert haben. 

\subsection{Beispiel}

Nach dem Berechnen der konsistenten Zuweisungsspalten sieht das multiple Alignment für unser Beispiel so aus. Der Übersichtlichkeit halber wurden hier bereits Lücken eingefügt, um die Zuweisungen zu verdeutlichen. Eigentlich erfolgt dieser Schritt erst ganz am Ende des Verfahrens.

\ttfamily
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
	\item aDGTC---TCa
	\item --GTCADcTCa
	\item -taTCADgg--
	\item -DGTCADaTC-
\end{enumerate}
\normalfont

Schaut man sich die letzten Stellen der Sequenzen 1 und 2 an, dann stellt man fest, dass deren Symbole identisch sind und sie zudem Teil des Fragments \texttt{CTCA} am Ende der beiden Sequenzen waren. Beim linearen Durchlauf über alle Fragmente stellen wir also fest, dass diese konsistent sind und fügen sie in das Alignment ein.

Versucht man jetzt DIALIGN auf den Teilsequenzen zwischen den bereits zugewiesenen Stelle zu benutzen, stellt man fest, dass es keine weiteren Fragmente mit positiven Überlappgewichten gibt. Nach dem ersten DIALIGN-Lauf wird also abgebrochen und die finale Ausgabe kann vorbereitet werden.

Das Ergebnis sieht letztendlich wie folgt aus. Auf der rechten Seite sehen Sie zum Vergleich das Ergebnis von DIALIGN 2.2 mit der gierigen Heuristik.

\ttfamily
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
	\item aDGTC---TCA \hspace{3cm} aDGTCTCA-----
	\item --GTCADcTCA \hspace{3cm} --G--TCADCTCa
	\item -taTCADgg-- \hspace{3cm} ---TATCADgg--
	\item -DGTCADaTC- \hspace{3cm} -DG--TCADATC-
\end{enumerate}
\normalfont

Wie man sieht kommt dieses Alignment mit deutlich weniger eingefügten Lücken aus. Das liegt daran, dass nicht wenige große Fragmente mit hohen Gewichten das ganze Alignment dominieren können und andere sinnvolle Zuweisungen aufgrund der gierigen Heuristik verhindern.

Welches der beiden Ergebnisse besser ist, lässt sich nicht zweifelsfrei sagen, weil diese DNA-Sequenzen willkürlich gewählt wurden und es in ihnen keine biologisch bedeutenden Motive gibt. Man kann die Ergebnisse aber mathematisch über die Anzahl an identischen und abweichenden Paaren von Zuweisungen vergleichen.

Beim Min-Cut-Alignment gibt es dabei 30 Zuweisungen identischer Symbole und keine von abweichenden. DIALIGN 2.2 liefert uns hingegen 28 übereinstimmende und 2 abweichende Paare von Stelle. Dieses Ergebnis deutet auf jeden Fall auf die Vorteile des graphtheoretischen Ansatzes hin. 
 
\subsection{Gesamtkomplexität}

\begin{korollar}[{\cite{cpm10}}]
	Mit dem Min-Cut-Ansatz von Corel et al. lässt sich ein multiples Sequenzalignment in $\oh(n^4\cdot L^{7/2})$ Zeit berechnen.
\end{korollar}

\begin{beweis}
	DIALIGN berechnet die $\oh(n^2)$ paarweisen Alignments in $\oh(n^2\cdot L^2)$ Rechenschritten. Die Laufzeit der Berechnung der partiellen Zuweisungsspalten mit Hilfe des minimalen Schnitt liegt in $\oh(n^4\cdot L^{7/2})$. Der Algorithmus von Pitschi hat eine Komplexität in $\oh(n^2\cdot L^2)$. Für den Alignmentgraphen sind nach wie vor $\oh(n^3\cdot L^2 + n^2\cdot L^2)$ Berechnungen nötig. Es folgt, dass die Berechnnung des minimalen Schnitts auf dem Inzidenzgraphen die Laufzeit dominiert, womit sich eine Gesamtkomplexität von $\oh(n^4\cdot L^{7/2})$ ergibt.
\end{beweis}

\subsection{Probleme bei der Heuristik zum Entfernen von Kanten}

Die Heuristik zum Entfernen von Kanten kann bei einigen Sequenzen zu größeren Problemen führen. Diese Heuristik funktionierte so, dass sie eine Grenze $k$ für das minimale Kantengewicht sukzessive erhöhte und alle Kanten unter $k$ entfernte, bis die Menge an Kanten ohne Zyklus war. 

In den meisten Fällen ist das kein Problem, weil man davon ausgeht, dass die wichtigen Motive, die wir finden wollen, in der gleichen Reihenfolge innerhalb der Eingabesequenzen liegen. Problematisch sind hingegen Sequenzfamilien, bei denen zwei (oder mehr)größere Abschnitte in ihrer Reihenfolge vertauscht wurden und einige Sequenzen diese in der einen, die anderen aber in den der anderen Reihenfolge haben. \improvement{Grafik einfügen.}

Sind die vertauschten Abschnitte lang genug, dann werden sie Teil der paarweisen Alignments. Weil es sich bei ihnen um Überkreuzungen handelt, entfernt der Min-Cut-Ansatz sie im Allgemeinen auch nicht. Kommen diese Permutationen in genug Sequenzen vor (beispielsweise allen), dann ist das Ergebnis ein Sukzessionsgraph mit einem Zyklus, der Kanten mit sehr hohen Kantengewichten enthält. Um diese Zyklen aufzulösen, wird dann auch eine sehr hohe Grenze $k^{*}$ benötigt. Im schlimmsten Fall werden so alle Kanten im Graph gelöscht, in jedem Fall aber sehr viele, die nichts mit dem eigentlichen Zyklus zu tun haben, weil der Schritt global Kanten löscht. Somit gehen viele wichtige oder alle Verbindungen in unserem Alignment verloren. Wir wollen am liebsten, dass die überkreuzten Stellen nicht aligniert werden, der Rest hingegen ganz normal.

Bis jetzt haben wir dieses Problem nur auf theoretischer Ebene betrachtet. Es gibt aber Sequenzfamilien, bei denen ein solches Verhalten wirklich auftritt. Ein Beispiel dafür ist die sogenannte \emph{zirkuläre Permutation} bei Proteinen. Bei diesen kommen zwar die selben Abschnitte, aber in veränderter Reihenfolge vor. Ein Beispiel sind die abstrahierten Proteinsequenzen [A,B,C,D] und [B,C,D,A]. Erstmalig wurde dieses Phänomen von \cite{chhe79} beschrieben. Wenn es im Laufe der Zeit zu einer zirkulären Permutation gekommen ist, deren Ergebnis zwei Sequenzen sind und wir eine Sequenzfamilie vorliegen haben, bei der die eine Hälfte von der einen und die andere von der anderen dieser Sequenzen abstammt, dann kann es durchaus zum oben beschriebenen Verhalten kommen.

Mit einer verbesserten Heuristik zum Entfernen von Kanten aus dem Sukzessionsgraph, um die Zyklen aufzulösen, könnte man diesem Problem vorbeugen. Ein solcher Algorithmus wurde beispielsweise von \cite{els93} beschrieben, der in $\oh(|E|)$ Zeit einen DAG berechnet und dabei höchstens $|E|/2 - |V|/6$ Kanten entfernt. Leider löscht auch dieser im Allgemeinen sehr viele Kanten, die nichts mit dem Zyklus zu tun haben. Weil man bei nahezu allen Sequenzfamilien davon ausgehen kann, dass Permutationen kein Problem sind, halte ich die ursprüngliche Heuristik zunächst für praktikabler. Denkbar ist aber ein hybrider Ansatz, bei dem wir überprüfen wie viele Kanten $E_{k^{*}}$ im Verhältnis zur alten Kantenanzahl enthält. Ist dieses Verhältnis zu gering (beispielsweise kleiner als 3/4), benutzen wir stattdessen den Algorithmus von Eades, Lin und Smyth auf den ursprünglichen Kanten unseres Sukzessionsgraphen.

\subsection{Evaluierung}

\cite{cpm10} haben ihre Implementierung des Verfahrens einer Vielzahl an Tests unterzogen, um sie mit einer breiten Auswahl an etablierten Verfahren zu vergleichen. Die Ergebnisse wurden dabei auf drei Referenzdatenbanken getestet, die jeweils eine Menge von alignierten Sequenzfamilien zur Verfügung stellen für die eine biologisch korrekte Zuweisung bekannt ist. Zum Test auf globale verwandten Sequenzen wurde die Referenzdatenbank BAliBASE benutzt. Diese enthält Alignments von Proteinsequenzen, die auf der ganzen Länge der Sequenzen Ähnlichkeiten haben. Lokal verwandte Sequenzen haben hingegen nur an einzelnen Stellen Ähnlichkeiten und zwischen diesen verwandten Segmenten sind nicht in Beziehung zueinander stehende Symbole. Für diese Art von Sequenzen wurden Tests auf den Datenbanken IRMBASE (Proteine) und DIRMBASE (DNA) durchgeführt.

Für mehrere Sequenzfamilien dieser Datenbanken wurden dann mit dem Min-Cut-Ansatz und durch andere Alignmentprogramme, wie DIALIGN 2.2 und DIALIGN TX (neueste Version von DIALIGN), T-COFFEE, MAFFT, Alignments berechnet und miteinander verglichen. Für die Vergleiche dienten wieder der \emph{sum-of-pairs-Score} (SP) und der \emph{(total-)column-Score} (TC), die die Anzahl an korrekten Paaren beziehungsweise komplett korrekten Spalten im Vergleich zum Referenzalignment angeben.

\subsubsection{Lokale Alignments auf IRMBASE2 und DIRMBASE1}

Die Sequenzen der Referenzdatenbanken IRMBASE2 und DIRMBASE1 wurden so konstruiert, dass in zufällig erstellte und nicht zueinander in Beziehung stehende Sequenzen lokal einzelne Motive hinzugefügt wurden.

Es hat sich herausgestellt, dass die neueste Version von DIALIGN TX nach wie vor die besten Ergebnisse auf lokal verwandten Sequenzen liefert, gefolgt von DIALIGN 2.2. Der Min-Cut-Ansatz hatte auf den Proteinsequenzen von DIRMBASE starke Ergebnisse ergeben, die mit denen der beiden DIALIGN-Varianten vergleichbar sind. Auf DNA-Sequenzen waren die Ergebnisse nach wie vor ordentlich, aber signifikant schlechter im Vergleich zu den beiden gierigen Verfahren. Im Vergleich zu den globalen Alignierern, wie CLUSTALW 2.0 oder T-COFFEE schnitt das Min-Cut-Verfahren aber sehr gut ab \cite{cpm10}. Die einzige Ausnahme ist das Programm MAFFT, das Sequenzen mit Hilfe der schnellen Fouriertransformation gruppiert und sie dann progressiv aligniert. MAFFT lieferte auf Protein- und DNA-Sequenzen gute Ergebnisse, die beinahe an die der DIALIGN-Varianten herankommen.

\subsubsection{BALiBASE3}

Ein Problem bei den Tests war aber die Laufzeit auf einigen Sequenzfamilien. Wie bereits in Abschnitt \ref{ch:ra_kompl} über die Komplexität von \textrm{ResolveAmbiguities} beschrieben, war die Laufzeit auf einigen Sequenzfamilien so lang, dass es nicht möglich war in angemessener Zeit Alignments zu berechnen. Deshalb sah man sich dazu gezwungen zusätzlich eine Grenze $T$ für das minimale Gewicht eines Fragments einzuführen, wodurch die Anzahl der Kanten im Inzidenzgraphen verringert wird. Diese Grenze $T$ sorgt im Gegenzug zur verbesserten Laufzeit aber auch für etwas schlechtere Ergebnisse \cite{cpm10}.

Insgesamt stellt der neue Ansatz auf global verwandten Sequenzfamilien eine deutliche Verbesserung zu DIALIGN 2.2 dar. So waren bei allen sechs getesteten Referenzmengen, die jeweils eine Vielzahl an Sequenzfamilien enthalten, die Ergebnisse des min-cut-Verfahrens besser. Das gilt sowohl für die SP-, als auch für die TC-Scores. Mit den besten globalen Alignierer kann das Verfahren zwar nicht konkurrieren, der verbreitete progressive Algorithmus CLUSTALW 2.0 wurde aber in allen Tests geschlagen. 

\subsubsection{Betrachtung der Scores im Vergleich}

Um festzustellen wie ihr Ansatz numerisch abschneidet, haben \cite{cpm10} die Scores der Gewichtsfunktionen von den Ergebnissen ihres Algorithmus mit denen von DIALIGN auf den sechs getesteten Referenzfamilien von BAliBASE verglichen. Man hat festgestellt, dass die Scores des min-cut-Algorithmus auf vier der sechs Referenzfamilien geringer sind, als mit DIALIGN 2.2. Weil gleichzeitig die TC- und SP-Scores ihres Algorithmus besser waren, werten die Autoren dies als Indiz dafür, dass die Gütefunktion letztendlich nur eingeschränkt funktioniert. Wenn diese ein schlechtes Maß für die Güte eines Alignments ist und gleichzeitig mit Hilfe von Heuristiken versucht wird die Gütefunktion zu maximieren, dann ist es logisch, dass bessere Heuristiken mit höheren Scores nicht automatisch auch biologisch bessere Ergebnisse liefern. \cite{cpm10} folgern aufgrund der Ergebnisse mit biologisch besseren Alignments bei gleichzeitig numerisch schlechteren Ergebnissen in der Gütefunktion, dass unter Weiterverwendung dieser keine größeren Verbesserungen der DIALIGN-Verfahren mehr möglich sind. Mögliche Auswege daraus werden im Abschnitt Future Works \ref{sec:fut_work} des letzten Kapitels beschrieben.

Leider gehen die Autoren nicht darauf ein wie für die Alignments des Min-Cut-Ansatzes nachträglich die numerischen Scores berechnet wurden. Durch die Berechnung der minimalen Schnitt und den Algorithmus von Pitschi werden mitunter auch einzelne Stelle aus den Zuweisungen gelöscht, sodass die Ergebnisse nicht immer längere, zusammenhängende Fragmente sind. Interpretiert man einfach ohne Lücke aufeinanderfolgenden, einander zugewiesene Stelle als Fragmente, dann erscheint es logisch, dass die Scores geringer sind. Das liegt an der Gewichtsfunktion von DIALIGN, die bei jedem Fragment einen Korrekturterm $K$ abzieht, der von der Länge der Sequenzen abhängt. Viele kurze Fragmente, wie sie beim Min-Cut-Algorithmus auftreten, haben im Allgemeinen immer ein geringeres Gesamtgewicht als weniger kurze mit der selben Gesamtlänge. Im Kontext dieser Erkenntnis stimme ich mit der Schlussfolgerung von Corel also nicht uneingeschränkt überein, solange nicht klar ist wie die Scores genau berechnet wurden.